{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da547f4d-47f8-4169-81e5-4ef3f40292bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from common_functions import google_sheets, task_fail_slack_alert, get_secret, snowflake_query\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def initialize_env():\n",
    "    snowflake_sg_secret = json.loads(get_secret(\"Snowflake-sagemaker\"))\n",
    "    slack_secret = json.loads(get_secret(\"prod/slack/reports\"))\n",
    "    fintech_service_account = json.loads(get_secret(\"prod/fintechServiceEmail/credentials\"))\n",
    "    dwh_writer_secret = json.loads(get_secret(\"prod/db/datawarehouse/sagemaker\"))\n",
    "\n",
    "    os.environ[\"SNOWFLAKE_USERNAME\"] = snowflake_sg_secret[\"username\"]\n",
    "    os.environ[\"SNOWFLAKE_PASSWORD\"] = snowflake_sg_secret[\"password\"]\n",
    "    os.environ[\"SNOWFLAKE_ACCOUNT\"] = snowflake_sg_secret[\"account\"]\n",
    "    os.environ[\"SNOWFLAKE_DATABASE\"] = snowflake_sg_secret[\"database\"]\n",
    "\n",
    "    os.environ[\"SLACK_TOKEN\"] = slack_secret[\"token\"]\n",
    "\n",
    "    os.environ[\"FINTECH_EMONEY_EMAIL\"] = fintech_service_account[\"email_name\"]\n",
    "    os.environ[\"FINTECH_EMONEY_PASSWORD\"] = fintech_service_account[\"email_password\"]\n",
    "\n",
    "    metabase_secret = json.loads(get_secret(\"prod/metabase/maxab_config\"))\n",
    "    os.environ[\"EGYPT_METABASE_USERNAME\"] = metabase_secret[\"metabase_user\"]\n",
    "    os.environ[\"EGYPT_METABASE_PASSWORD\"] = metabase_secret[\"metabase_password\"]\n",
    "\n",
    "    os.environ[\"DWH_WRITER_HOST_NEW\"] = dwh_writer_secret[\"host\"]\n",
    "    os.environ[\"DWH_WRITER_NAME_NEW\"] = dwh_writer_secret[\"dbname\"]\n",
    "    os.environ[\"DWH_WRITER_USER_NAME_NEW\"] = dwh_writer_secret[\"username\"]\n",
    "    os.environ[\"DWH_WRITER_PASSWORD_NEW\"] = dwh_writer_secret[\"password\"] \n",
    "\n",
    "    json_path_sheets = str(Path.home()) + \"/service_account_key_sheets.json\"\n",
    "    sheets_key = get_secret(\"prod/maxab-sheets\")\n",
    "    f = open(json_path_sheets, \"w\")\n",
    "    f.write(sheets_key)\n",
    "    f.close()\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"] = json_path_sheets\n",
    "\n",
    "def ret_metabase(country, question, filters={}, initialized = False):\n",
    "\n",
    "    \n",
    "    if not initialized: \n",
    "        initialize_env()\n",
    "    \n",
    "    question_id = str(question)\n",
    "    \n",
    "    if country.lower() == 'egypt':\n",
    "        base_url = 'https://bi.maxab.info/api'\n",
    "        username = str(os.environ[\"EGYPT_METABASE_USERNAME\"])\n",
    "        password = str(os.environ[\"EGYPT_METABASE_PASSWORD\"])\n",
    "    else:\n",
    "        base_url = 'https://bi.maxabma.com/api'\n",
    "        username = str(os.environ[\"AFRICA_METABASE_USERNAME\"])\n",
    "        password = str(os.environ[\"AFRICA_METABASE_PASSWORD\"])\n",
    "\n",
    "    base_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    try:\n",
    "        s_response = requests.post(\n",
    "            base_url + '/session',\n",
    "            data=json.dumps({\n",
    "                'username': username,\n",
    "                'password': password\n",
    "            }),\n",
    "            headers=base_headers)\n",
    "        \n",
    "        s_response.raise_for_status()\n",
    "\n",
    "        session_token = s_response.json()['id']\n",
    "        base_headers['X-Metabase-Session'] = session_token\n",
    "        \n",
    "        params = []\n",
    "        \n",
    "        for name, value in filters.items():\n",
    "            filter_type, filter_value = value\n",
    "            param = {'target': ['variable', ['template-tag', name]], 'value': filter_value}\n",
    "            \n",
    "            if filter_type.lower() == 'date':\n",
    "                param['type'] = 'date/range' if isinstance(filter_value, list) else 'date/single'\n",
    "            elif filter_type.lower() == 'category':\n",
    "                param['type'] = 'category'\n",
    "            elif filter_type.lower() == 'text':\n",
    "                param['type'] = 'text'\n",
    "            elif filter_type.lower() == 'number':\n",
    "                param['type'] = 'number'\n",
    "            elif filter_type.lower() == 'field list':\n",
    "                param['type'] = 'id'\n",
    "                param['target'] = ['dimension', ['template-tag', name]]\n",
    "            \n",
    "            params.append(param)\n",
    "\n",
    "        p_response = requests.post(base_url + '/card/' + question_id + '/query/csv', \n",
    "                                   json={'parameters': params}, \n",
    "                                   headers=base_headers)\n",
    "        p_response.raise_for_status()\n",
    "\n",
    "        my_dict = p_response.content\n",
    "        s = str(my_dict, 'utf-8')\n",
    "        my_dict = StringIO(s)\n",
    "        df = pd.read_csv(my_dict)\n",
    "        return(df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def check_distribution(df, agent_list):\n",
    "    \"\"\"\n",
    "    Checks if the distribution of project types across agents is even.\n",
    "    Returns a dictionary with the count of rows per agent for each project type.\n",
    "    \"\"\"\n",
    "    distribution = {}\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        distribution[project] = project_df['agent_assigned'].value_counts().reindex(agent_list, fill_value=0)\n",
    "    \n",
    "    return distribution\n",
    "\n",
    "def redistribute_rows(df, agent_list):\n",
    "    \"\"\"\n",
    "    Redistributes rows among agents if the distribution is uneven.\n",
    "    \"\"\"\n",
    "    project_types = df['project_name'].unique()\n",
    "    redistributed_data = pd.DataFrame()\n",
    "    print(\"Redistributing rows...\")\n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.sample(frac=1).reset_index(drop=True)  # Shuffle data\n",
    "        rows_per_agent = len(project_df) // len(agent_list)\n",
    "        remainder = len(project_df) % len(agent_list)\n",
    "        \n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, agent in enumerate(agent_list):\n",
    "            end_idx = start_idx + rows_per_agent + (1 if i < remainder else 0)\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            redistributed_data = pd.concat([redistributed_data, agent_data])\n",
    "            \n",
    "            start_idx = end_idx\n",
    "    \n",
    "    redistributed_data = redistributed_data.reset_index(drop=True)\n",
    "    \n",
    "    return redistributed_data\n",
    "\n",
    "def ensure_correct_dispatching(df, agent_list, final_old_assign):\n",
    "    \"\"\"\n",
    "    Ensures the dispatching is done correctly by checking and redistributing rows if necessary.\n",
    "    `final_old_assign` rows are excluded from redistribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Exclude final_old_assign from the distribution check, but keep rows with main_system_id == 1\n",
    "    new_assignments = df[~df['main_system_id'].isin(final_old_assign['main_system_id']) | (df['main_system_id'] == 1)]\n",
    "    \n",
    "    distribution = check_distribution(new_assignments, agent_list)\n",
    "    \n",
    "    # Check if any project has an uneven distribution across agents\n",
    "    uneven_distribution = any(distribution[project].nunique() > 1 for project in distribution)\n",
    "    \n",
    "    if uneven_distribution:\n",
    "        print(\"Uneven distribution detected.\")\n",
    "        new_assignments = redistribute_rows(new_assignments, agent_list)\n",
    "    else:\n",
    "        print(\"Distribution is even. No redistribution needed.\")\n",
    "    \n",
    "    # Combine the redistributed new assignments with the final_old_assign\n",
    "    final_data = pd.concat([final_old_assign, new_assignments], ignore_index=True)\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def check_distribution_df(df, agent_list):\n",
    "    \"\"\"\n",
    "    Checks if the distribution of project types across agents is even.\n",
    "    Returns a DataFrame with the count of rows per agent for each project type.\n",
    "    \"\"\"\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    distribution_data = []\n",
    "    current_time = datetime.now()  # Get the current datetime\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        agent_counts = project_df['agent_assigned'].value_counts().reindex(agent_list, fill_value=0)\n",
    "        \n",
    "        for agent, count in agent_counts.items():\n",
    "            distribution_data.append({\n",
    "                'project_type': project,\n",
    "                'agent_assigned': agent,\n",
    "                'count': count,\n",
    "                'datetime': dt.datetime.now()  + timedelta(hours=3)\n",
    "            })\n",
    "    \n",
    "    distribution_df = pd.DataFrame(distribution_data)\n",
    "    \n",
    "    return distribution_df\n",
    "\n",
    "def clean_column_id(df, column_name):\n",
    "    # Ensure the column is treated as a string\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    \n",
    "    # Replace commas in the string\n",
    "    df[column_name] = df[column_name].str.replace(',', '')\n",
    "    \n",
    "    # Convert back to an integer, if appropriate\n",
    "    df[column_name] = df[column_name].astype('Int64', errors='ignore')\n",
    "    \n",
    "    return df\n",
    "def remove_assign(previously_assigned, df, assigns, print_it = False):\n",
    "    print(\"Starting remove_assign function...\")\n",
    "\n",
    "    try:\n",
    "        if previously_assigned.shape[0] != 1:            \n",
    "            previously_assigned[0] = previously_assigned[0].fillna('').astype(str).str.replace(\" \", \"\", regex=False)\n",
    "            previously_assigned = previously_assigned.dropna()\n",
    "            previously_assigned[0] = previously_assigned[0].astype('float').astype('int')\n",
    "            df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "            if 'agent_assigned' in df.columns:\n",
    "                if df['agent_assigned'].dtype == 'object':\n",
    "                    df.loc[df['agent_assigned'].notna(), 'agent_assigned'] = df.loc[df['agent_assigned'].notna(), 'agent_assigned'].astype(int)\n",
    "\n",
    "                    print(\"Converted agent_assigned column to int.\")\n",
    "\n",
    "            filtered_ids = previously_assigned[0].astype(int).values\n",
    "            main_data_to_assign = df.loc[~df['main_system_id'].isin(filtered_ids) | (df['main_system_id'] == 1)\n",
    "            ]\n",
    "            if print_it:\n",
    "                print(f\"Available tasks for this batch after filtering. Rows remaining: {len(main_data_to_assign)}\")\n",
    "                print(f\" Retailers that have no agents to assign to {main_data_to_assign['agent_assigned'].isna().sum()}\")\n",
    "\n",
    "            main_data_to_assign['main_system_id'] = main_data_to_assign['main_system_id'].astype('int')\n",
    "            main_data_to_assign = main_data_to_assign.groupby('agent_assigned').head(assigns)\n",
    "\n",
    "            return main_data_to_assign\n",
    "\n",
    "        else:\n",
    "            print(\"Only one previously assigned entry, skipping filter.\")\n",
    "            df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "            main_data_to_assign = df.groupby('agent_assigned').head(assigns)\n",
    "            return main_data_to_assign\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] remove_assign failed: {e}\")\n",
    "# ----------------------------------------\n",
    "# Random distribution (Equal assigning)   \n",
    "# ----------------------------------------\n",
    "def assign_data_equal_projects(df, list):\n",
    "    df = df.sample(frac=1)  # Shuffle the data\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    assigned_data = pd.DataFrame()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.reset_index(drop=True)\n",
    "        rows_per_agent = len(project_df) // len(list)\n",
    "        remainder = len(project_df) % len(list)\n",
    "        \n",
    "        # Distribute rows equally\n",
    "        for i, agent in enumerate(list):\n",
    "            start_idx = i * rows_per_agent\n",
    "            end_idx = start_idx + rows_per_agent\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            # Handle remainder\n",
    "            if i < remainder:\n",
    "                extra_row = project_df.iloc[end_idx:end_idx+1].copy()\n",
    "                extra_row['agent_assigned'] = agent\n",
    "                agent_data = pd.concat([agent_data, extra_row])\n",
    "            \n",
    "            assigned_data = pd.concat([assigned_data, agent_data])\n",
    "    \n",
    "    assigned_data = assigned_data.reset_index(drop=True)\n",
    "    \n",
    "    return assigned_data\n",
    "\n",
    "# ----------------------------------------\n",
    "# Mapping distribution (Segment-based)\n",
    "# ----------------------------------------\n",
    "def assign_data_by_mapping(df, mapping_df):\n",
    "    # store retail-agent mapping in a dictionary \n",
    "    mapping_dict = dict(zip(mapping_df['MAIN_SYSTEM_ID'], mapping_df['AGENT_ID']))\n",
    "        \n",
    "    assigned_agents = []\n",
    "\n",
    "    for retailer_id in df['main_system_id']:\n",
    "        if retailer_id in mapping_dict:\n",
    "            assigned_agents.append(mapping_dict[retailer_id])\n",
    "        else:\n",
    "            assigned_agents.append(None)\n",
    "   \n",
    "    df['agent_assigned'] = assigned_agents\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_available_agents(attendance_df, current_hour):\n",
    "    \"\"\"Get list of available task-based agents for the current hour.\"\"\"\n",
    "    attendance_copy = attendance_df.copy()\n",
    "    \n",
    "    attendance_copy['start_time'] = attendance_copy['start_time'].astype(int)\n",
    "    attendance_copy['end_time'] = attendance_copy['end_time'].astype(int)\n",
    "    \n",
    "    attendance_copy['assignment_start_time'] = attendance_copy['start_time'] - 1\n",
    "    attendance_copy['assignment_end_time'] = attendance_copy['end_time'] - 1\n",
    "    \n",
    "    attendance_copy['assign_data'] = np.where(\n",
    "        (current_hour >= attendance_copy['assignment_start_time']) & \n",
    "        (current_hour <= attendance_copy['assignment_end_time']),\n",
    "        'yes', 'no')\n",
    "    \n",
    "    task_based_agents = attendance_copy.loc[\n",
    "        (attendance_copy['project'] == 'task_based') & \n",
    "        (attendance_copy['assign_data'] == 'yes')]\n",
    "    \n",
    "    task_based_list = task_based_agents['agent_id'].values.tolist()\n",
    "    print(f\"Number of available agents: {len(task_based_list)}\")\n",
    "    return task_based_list\n",
    "\n",
    "def fetch_and_process_queries(query_ids, blacklisted_retailers):\n",
    "    \"\"\"Fetch and process data from queries, removing blacklisted retailers.\"\"\"\n",
    "    queries_for_random = query_ids['Query_id_random'].dropna().astype(int).tolist()\n",
    "    queries_for_mapped = query_ids['Query_id_segment_based'].dropna().astype(int).tolist()\n",
    "    print(f\"Fetching data from {len(queries_for_random)+len(queries_for_mapped)} queries...\")\n",
    "    \n",
    "    # Process random queries\n",
    "    dataframes_R = [ret_metabase(\"EGYPT\", query, initialized=True) for query in queries_for_random]\n",
    "    empty_queries = []\n",
    "    for i, df in enumerate(dataframes_R):\n",
    "        if df.empty:\n",
    "            empty_queries.append(queries_for_random[i])\n",
    "        else:\n",
    "            print(f\"Query {queries_for_random[i]} returned {len(df)} records\")\n",
    "        df.columns = map(str.lower, df.columns)\n",
    "    if empty_queries:\n",
    "        print(f\"WARNING: Queries {empty_queries} returned empty dataframe!\")\n",
    "    # ----------------------------------------\n",
    "    # write in google sheet available data\n",
    "    # ----------------------------------------\n",
    "    # check for empty queries in Random tasks\n",
    "    for idx, row in query_ids.iterrows():\n",
    "        query_id = row['Query_id_random']\n",
    "        if pd.notna(query_id):\n",
    "            df = ret_metabase(\"EGYPT\", int(query_id), initialized=True)\n",
    "            query_ids.at[idx, 'Available_data'] = 'Empty' if df.empty else str(len(df))\n",
    "\n",
    "    # Step 3: Overwrite the sheet with updated full data (including manually-entered columns like F)\n",
    "    google_sheets('Query ID Assigning', 'Sheet1', 'overwrite', df=query_ids)\n",
    "    \n",
    "    # check for empty queries in Segment-based\n",
    "    for idx, row in query_ids.iterrows():\n",
    "        query_id = row['Query_id_segment_based']\n",
    "        if pd.notna(query_id):\n",
    "            df = ret_metabase(\"EGYPT\", int(query_id), initialized=True)\n",
    "            query_ids.at[idx, 'Available_data'] = 'Empty' if df.empty else str(len(df))\n",
    "\n",
    "    # Step 3: Overwrite the sheet with updated full data (including manually-entered columns like F)\n",
    "    google_sheets('Query ID Assigning', 'Sheet1', 'overwrite', df=query_ids)\n",
    "    \n",
    "    # Process mapped queries\n",
    "    dataframes_M = [ret_metabase(\"EGYPT\", query, initialized=True) for query in queries_for_mapped]\n",
    "    for i, df in enumerate(dataframes_M):\n",
    "        if df.empty:\n",
    "            print(f\"WARNING: Query {queries_for_mapped[i]} returned empty dataframe!\")\n",
    "        else:\n",
    "            print(f\"Query {queries_for_mapped[i]} returned {len(df)} records\")\n",
    "        df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "    # Combine and clean dataframes\n",
    "    df_unfiltered_R = pd.concat(dataframes_R, ignore_index=True)\n",
    "    df_unfiltered_M = pd.concat(dataframes_M, ignore_index=True)\n",
    "    print(f\"Mapping tasks available: {df_unfiltered_M.shape[0]}\")\n",
    "    print(f\"Random tasks available: {df_unfiltered_R.shape[0]}\")\n",
    "    \n",
    "    # Remove blacklisted retailers\n",
    "    df_raw_R = df_unfiltered_R[~df_unfiltered_R['main_system_id'].isin(blacklisted_retailers)]\n",
    "    df_raw_M = df_unfiltered_M[~df_unfiltered_M['main_system_id'].isin(blacklisted_retailers)]\n",
    "    \n",
    "    df_raw_R = clean_column_id(df_raw_R, 'main_system_id')\n",
    "    df_raw_M = clean_column_id(df_raw_M, 'main_system_id')\n",
    "    print(f\"Removed {(len(df_unfiltered_R)+len(df_unfiltered_M)) - (len(df_raw_R)+len(df_raw_M))} blacklisted retailers\")\n",
    "    \n",
    "    return df_raw_R, df_raw_M\n",
    "\n",
    "def process_previous_assignments(df_raw_R, previous_calls):\n",
    "    \"\"\"Process previous assignments and separate new and old assignments.\"\"\"\n",
    "    exclude_same_assigns = clean_column_id(previous_calls, 'main_system_id')\n",
    "    \n",
    "    # Get new assignments\n",
    "    task_based = pd.DataFrame(df_raw_R.loc[~df_raw_R['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "    \n",
    "    # Get old assignments\n",
    "    old_assign = pd.DataFrame(df_raw_R.loc[df_raw_R['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "    df_1 = old_assign.merge(previous_calls, on='main_system_id', how='left')\n",
    "    final_old_assign = df_1[[\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"project_name\"]]\n",
    "    \n",
    "    return task_based, final_old_assign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "059f6273-2082-4161-a78c-33ea4c4d5880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process at hour: 13\n",
      "Total agents: 8\n",
      "Number of available agents: 8\n",
      "/home/ec2-user/service_account_key.json\n",
      "Fetching data from 20 queries...\n",
      "Query 60605 returned 245 records\n",
      "Query 59703 returned 1241 records\n",
      "Query 36299 returned 15 records\n",
      "Query 35981 returned 69 records\n",
      "Query 38188 returned 54 records\n",
      "Query 60553 returned 250 records\n",
      "WARNING: Queries [49423, 49557, 49556, 39940, 49566, 55124, 55045, 56316, 59170, 59170, 59188, 59874, 60326] returned empty dataframe!\n",
      "/home/ec2-user/service_account_key.json\n",
      "/home/ec2-user/service_account_key.json\n",
      "Query 59585 returned 350 records\n",
      "Mapping tasks available: 350\n",
      "Random tasks available: 1874\n",
      "Removed 1 blacklisted retailers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16994/1167670908.py:221: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(str)\n",
      "/tmp/ipykernel_16994/1167670908.py:224: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].str.replace(',', '')\n",
      "/tmp/ipykernel_16994/1167670908.py:227: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype('Int64', errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 350 mapping records\n",
      "Successfully mapped 350 records to agents\n",
      "Retailers unassigned: 0\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# Initialize and get current time\n",
    "# ----------------------------------------\n",
    "initialize_env()\n",
    "now = datetime.now() + timedelta(hours=3)\n",
    "hour = int(str(now.time())[0:2])\n",
    "print(f\"Starting process at hour: {hour}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Get available agents\n",
    "# ----------------------------------------\n",
    "attendance = ret_metabase(\"EGYPT\", 13502, initialized=True)\n",
    "print(f\"Total agents: {len(attendance)}\")\n",
    "task_based_list = get_available_agents(attendance, hour)\n",
    "time.sleep(15)\n",
    "\n",
    "if not task_based_list:\n",
    "    print(\"No agents available for current hour!\")\n",
    "    \n",
    "\n",
    "# ----------------------------------------\n",
    "# Get query IDs and blacklisted retailers\n",
    "# ----------------------------------------\n",
    "query_ids = google_sheets('Query ID Assigning', 'Sheet1', 'get')\n",
    "blacklisted_retailers = query_ids['Blacklisted_retailers'].dropna().astype(int).tolist()\n",
    "\n",
    "# ----------------------------------------\n",
    "# Fetch and process query data\n",
    "# ----------------------------------------\n",
    "df_raw_R, df_raw_M = fetch_and_process_queries(query_ids, blacklisted_retailers)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Process previous assignments / For follow up calls\n",
    "# ----------------------------------------\n",
    "previous_calls = ret_metabase(\"EGYPT\", 35299, initialized=True)\n",
    "previous_calls = clean_column_id(previous_calls, 'main_system_id')\n",
    "task_based, final_old_assign = process_previous_assignments(df_raw_R, previous_calls)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Assign agents to tasks\n",
    "# ----------------------------------------\n",
    "main_data_list = []\n",
    "\n",
    "# Get and process mapping data\n",
    "mapping_df = ret_metabase('Egypt',59587)\n",
    "mapping_df = clean_column_id(mapping_df, 'MAIN_SYSTEM_ID')\n",
    "mapping_df = clean_column_id(mapping_df, 'AGENT_ID')\n",
    "\n",
    "if mapping_df.empty:\n",
    "    print(\"WARNING: No mapping data found!\")\n",
    "else:\n",
    "    print(f\"Found {len(mapping_df)} mapping records\")\n",
    "    mapping_df = mapping_df[mapping_df['AGENT_ID'].isin(task_based_list)]\n",
    "    mapped_data = assign_data_by_mapping(df_raw_M, mapping_df)\n",
    "\n",
    "    if mapped_data.empty:\n",
    "        print(\"WARNING: No data mapped to agents!\")\n",
    "    else:\n",
    "        notna = mapped_data['agent_assigned'].notna().sum()\n",
    "        na = mapped_data['agent_assigned'].isna().sum()\n",
    "        print(f\"Successfully mapped {notna} records to agents\")\n",
    "        print(f\"Retailers unassigned: {na}\")\n",
    "    main_data_list.append(mapped_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da205ada-7895-4629-84e1-eebb2d09d8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_data = assign_data_equal_projects(priority_df, task_based_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "038ab4a0-aff7-4b3f-af43-4b703c0f133f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
