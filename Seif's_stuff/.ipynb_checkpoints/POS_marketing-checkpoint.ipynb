{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1756470-e1ff-43d6-bf96-5c33246b904d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from common_functions import ret_metabase, google_sheets, upload_dataframe_to_snowflake, snowflake_query, upload_dataframe_to_pg, dwh_query, initialize_env\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "# import psycopg2\n",
    "import numpy as np\n",
    "import gspread\n",
    "import sqlalchemy\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dc9db94-fb94-43ef-ae01-134972a2d05b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'DWH_WRITER_HOST_NEW'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m initialize_env()\n\u001b[0;32m----> 3\u001b[0m host \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDWH_WRITER_HOST_NEW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m database \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDWH_WRITER_NAME_NEW\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m user \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDWH_WRITER_USER_NAME_NEW\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DWH_WRITER_HOST_NEW'"
     ]
    }
   ],
   "source": [
    "    initialize_env()\n",
    "\n",
    "    host = os.environ[\"DWH_WRITER_HOST_NEW\"]\n",
    "    database = os.environ[\"DWH_WRITER_NAME_NEW\"]\n",
    "    user = os.environ[\"DWH_WRITER_USER_NAME_NEW\"]\n",
    "    password = os.environ[\"DWH_WRITER_PASSWORD_NEW\"]\n",
    "\n",
    "    conn = psycopg2.connect(host=host, database=database, user=user, password=password)\n",
    "    print(\"Successfully connected to DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1186b-2a74-46d9-8ff5-37011286c1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    engine = sqlalchemy.create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}/{database}\")\n",
    "    print(bool(engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cafca5d8-ac32-4837-99f2-a6ab0cf615aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Benchmark',\n",
       " 'Somaya Alrab',\n",
       " 'Esraa Mohamed',\n",
       " 'Ebthal Saber',\n",
       " 'Raneen Ali',\n",
       " 'Abdelrahman Merghany',\n",
       " 'Israa Abdelhamid',\n",
       " 'Nada Saber',\n",
       " 'Marina Riyad']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_group = google_sheets(\"Agents - Retailers\", \"Agent-to-group\", \"get\")\n",
    "col_list = agent_group.columns.tolist()\n",
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e358a59b-6d92-4c88-bf54-d29e8261c6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_available_agents(attendance_df, current_hour):\n",
    "    \"\"\"\n",
    "    Get a list of available task-based agents for the current hour based on attendance DataFrame.\n",
    "    Args:\n",
    "        attendance_df (pd.DataFrame): DataFrame with agent attendance info\n",
    "        current_hour (int): Current hour (24-hour format)\n",
    "    Returns:\n",
    "        list: List of available agent IDs\n",
    "    \"\"\"\n",
    "    attendance_copy = attendance_df.copy()\n",
    "    \n",
    "    attendance_copy['start_time'] = attendance_copy['start_time'].astype(int)\n",
    "    attendance_copy['end_time'] = attendance_copy['end_time'].astype(int)\n",
    "    \n",
    "    attendance_copy['assignment_start_time'] = attendance_copy['start_time'] - 1\n",
    "    attendance_copy['assignment_end_time'] = attendance_copy['end_time'] - 1\n",
    "    \n",
    "    attendance_copy['assign_data'] = np.where(\n",
    "        (current_hour >= attendance_copy['assignment_start_time']) & \n",
    "        (current_hour <= attendance_copy['assignment_end_time']),\n",
    "        'yes', 'no')\n",
    "    \n",
    "    task_based_agents = attendance_copy.loc[\n",
    "        (attendance_copy['project'] == 'task_based') & \n",
    "        (attendance_copy['assign_data'] == 'yes')]\n",
    "    \n",
    "    task_based_list = task_based_agents['agent_id'].values.tolist()\n",
    "    print(f\"Number of available agents: {len(task_based_list)}\")\n",
    "    return task_based_list\n",
    "\n",
    "\n",
    "def assign_data_equal_projects(df, list):\n",
    "    \"\"\"\n",
    "    Distribute rows of a DataFrame equally among a list of agents, grouped by project name.\n",
    "    Special handling for credit projects: only agents 9191 and 5565 can be assigned to projects with 'credit' in the name.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'project_name'\n",
    "        list (list): List of agent IDs or names\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'agent_assigned' column\n",
    "    \"\"\"\n",
    "    df = df.sample(frac=1)  # Shuffle the data\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    # Define credit agents and other agents\n",
    "    credit_agents = [9191, 5565]\n",
    "    other_agents = [agent for agent in list if agent not in credit_agents]\n",
    "    \n",
    "    assigned_data = pd.DataFrame()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.reset_index(drop=True)\n",
    "        \n",
    "        # Check if project contains 'credit' in the name (case insensitive)\n",
    "        is_credit_project = 'credit' in project.lower()\n",
    "        \n",
    "        if is_credit_project:\n",
    "            # For credit projects, only assign to credit agents\n",
    "            if len(credit_agents) > 0:\n",
    "                rows_per_agent = len(project_df) // len(credit_agents)\n",
    "                remainder = len(project_df) % len(credit_agents)\n",
    "                \n",
    "                # Distribute rows equally among credit agents\n",
    "                for i, agent in enumerate(credit_agents):\n",
    "                    start_idx = i * rows_per_agent\n",
    "                    end_idx = start_idx + rows_per_agent\n",
    "                    agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "                    agent_data['agent_assigned'] = agent\n",
    "                    \n",
    "                    # Handle remainder\n",
    "                    if i < remainder:\n",
    "                        extra_row = project_df.iloc[end_idx:end_idx+1].copy()\n",
    "                        extra_row['agent_assigned'] = agent\n",
    "                        agent_data = pd.concat([agent_data, extra_row])\n",
    "                    \n",
    "                    assigned_data = pd.concat([assigned_data, agent_data])\n",
    "            else:\n",
    "                print(f\"Warning: No credit agents available for credit project '{project}'\")\n",
    "        else:\n",
    "            # For non-credit projects, assign to other agents only\n",
    "            if len(other_agents) > 0:\n",
    "                rows_per_agent = len(project_df) // len(other_agents)\n",
    "                remainder = len(project_df) % len(other_agents)\n",
    "                \n",
    "                # Distribute rows equally among other agents\n",
    "                for i, agent in enumerate(other_agents):\n",
    "                    start_idx = i * rows_per_agent\n",
    "                    end_idx = start_idx + rows_per_agent\n",
    "                    agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "                    agent_data['agent_assigned'] = agent\n",
    "                    \n",
    "                    # Handle remainder\n",
    "                    if i < remainder:\n",
    "                        extra_row = project_df.iloc[end_idx:end_idx+1].copy()\n",
    "                        extra_row['agent_assigned'] = agent\n",
    "                        agent_data = pd.concat([agent_data, extra_row])\n",
    "                    \n",
    "                    assigned_data = pd.concat([assigned_data, agent_data])\n",
    "            else:\n",
    "                print(f\"Warning: No other agents available for non-credit project '{project}'\")\n",
    "    \n",
    "    assigned_data = assigned_data.reset_index(drop=True)\n",
    "    print(\"Assignment by project complete with credit project restrictions.\")\n",
    "    return assigned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de7f27-b39f-4265-a380-bc700136eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Data = google_sheets(\"POS Campaign | July & August 2025\", \"Main\", \"Get\")\n",
    "\n",
    "        task_based_list = get_available_agents(attendance, hour)\n",
    "        # Convert Date column\n",
    "        Data[\"Date\"] = pd.to_datetime(Data[\"Date\"], format=\"%m/%d/%Y\")\n",
    "        # Get today's date (normalized to midnight)\n",
    "        today = pd.Timestamp.today().normalize()\n",
    "        # Filter for today's rows\n",
    "        Data_filtered = Data[Data[\"Date\"] == today]\n",
    "\n",
    "        sheet_Data = Data_filtered.copy()\n",
    "        sheet_Data.rename(columns={'mobile': 'main_system_id','name': 'retailer_name'},inplace=True)\n",
    "\n",
    "        sheet_Data[\"main_system_id\"] = sheet_Data[\"main_system_id\"].astype('Int64').astype(str)\n",
    "        # Remove leading '2' if present\n",
    "        sheet_Data[\"main_system_id\"] = sheet_Data[\"main_system_id\"].str.replace(r\"^2\", \"\", regex=True)\n",
    "\n",
    "        # Drop duplicate main_system_id values (keeping first occurrence)\n",
    "        sheet_Data = sheet_Data.drop_duplicates(subset=\"main_system_id\", keep=\"first\")\n",
    "\n",
    "        sheet_Data[\"description\"] = (\n",
    "            \"POS Acquisition Marketing - location: \"\n",
    "            + sheet_Data[\"location\"].astype(str)\n",
    "            + \" - campaign:\"\n",
    "            + sheet_Data[\"campaign\"].astype(str)\n",
    "        )\n",
    "\n",
    "        sheet_Data.drop(columns=[\"location\", \"campaign\", \"Date\"], inplace=True)\n",
    "        sheet_Data['Added_at'] = now\n",
    "        sheet_Data['project_name'] = \"POS Leads\"\n",
    "        name_list = ['Somaya Alrab','Esraa Mohamed','Marina Riyad','Raneen Ali','Abdelrahman Merghany','Israa Abdelhamid','Nada Saber','Ebthal Saber']\n",
    "        Pos_leads = assign_data_equal_projects(sheet_Data , name_list)\n",
    "        data_for_sql = assign_data_equal_projects(sheet_Data, task_based_list)\n",
    "        data_for_sql.rename(columns={'main_system_id': 'offer'},inplace=True)\n",
    "        data_for_sql['main_system_id'] = 11111\n",
    "        # Check if description column exists before dropping it\n",
    "        if \"description\" in data_for_sql.columns:\n",
    "            data_for_sql.drop(columns=[\"description\"], inplace=True)\n",
    "        data_for_sql[\"description\"] = \"POS Acquisition Marketing - campaign: \" + Data[\"campaign\"]\n",
    "        data_for_sql = data_for_sql[['main_system_id', 'description','offer','agent_assigned']]\n",
    "        data_for_sql['dispatched_at'] = now\n",
    "        data_for_sql['agent_assigned'] = data_for_sql['agent_assigned'].astype(int) \n",
    "        google_sheets(\"Agents - Retailers\", \"Task_based\", \"append\", df=Pos_leads)\n",
    "\n",
    "        for i in col_list:\n",
    "            if i == \"Benchmark\":\n",
    "                continue \n",
    "            filtered_df = Pos_leads[Pos_leads[\"agent_assigned\"] == i]\n",
    "            filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n",
    "            print(f\"Assigned to {i},{len(filtered_df)} Task-based Tasks\")\n",
    "            google_sheets(i, \"Task_based\", \"append\", df=filtered_df)\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            data_for_sql.to_sql('task_based_am_projects', schema='fintech', con=engine, if_exists='append', chunksize=1000, method='multi', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
