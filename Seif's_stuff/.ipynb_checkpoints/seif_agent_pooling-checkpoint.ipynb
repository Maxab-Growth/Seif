{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce50c506-56e8-4213-8487-8faee7e6c838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gspread\n",
      "  Downloading gspread-6.2.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting google-auth>=1.12.0 (from gspread)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-auth-oauthlib>=0.4.1 (from gspread)\n",
      "  Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.12.0->gspread)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.12.0->gspread)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth>=1.12.0->gspread) (4.7.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth>=1.12.0->gspread) (0.6.1)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib>=0.4.1->gspread)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.4.26)\n",
      "Downloading gspread-6.2.1-py3-none-any.whl (59 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, google-auth-oauthlib, gspread\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [gspread]m4/7\u001b[0m [google-auth]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cachetools-5.5.2 google-auth-2.40.3 google-auth-oauthlib-1.2.2 gspread-6.2.1 oauthlib-3.3.1 pyasn1-modules-0.4.2 requests-oauthlib-2.0.0\n",
      "Collecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting httplib2>=0.9.1 (from oauth2client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client) (0.4.2)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client) (4.7.2)\n",
      "Requirement already satisfied: six>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client) (1.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httplib2>=0.9.1->oauth2client) (3.2.3)\n",
      "Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Installing collected packages: httplib2, oauth2client\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [oauth2client]\n",
      "\u001b[1A\u001b[2KSuccessfully installed httplib2-0.22.0 oauth2client-4.1.3\n",
      "Collecting gspread_dataframe\n",
      "  Downloading gspread_dataframe-4.0.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: gspread>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gspread_dataframe) (6.2.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gspread_dataframe) (2.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gspread_dataframe) (1.17.0)\n",
      "Requirement already satisfied: google-auth>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gspread>=3.0.0->gspread_dataframe) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gspread>=3.0.0->gspread_dataframe) (1.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (4.7.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth>=1.12.0->gspread>=3.0.0->gspread_dataframe) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=0.24.0->gspread_dataframe) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=0.24.0->gspread_dataframe) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=0.24.0->gspread_dataframe) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=0.24.0->gspread_dataframe) (2025.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.3.1)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=3.0.0->gspread_dataframe) (2025.4.26)\n",
      "Downloading gspread_dataframe-4.0.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Installing collected packages: gspread_dataframe\n",
      "Successfully installed gspread_dataframe-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gspread\n",
    "!pip install oauth2client\n",
    "!pip install gspread_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10f1f6c6-0c84-4e08-9d0c-f62b44abbc5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from common_functions import google_sheets, task_fail_slack_alert, get_secret, snowflake_query\n",
    "from datetime import datetime, timedelta\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0de61e0a-0123-4fdd-b857-31ae0d6a5088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_env():\n",
    "    snowflake_sg_secret = json.loads(get_secret(\"Snowflake-sagemaker\"))\n",
    "    slack_secret = json.loads(get_secret(\"prod/slack/reports\"))\n",
    "    fintech_service_account = json.loads(get_secret(\"prod/fintechServiceEmail/credentials\"))\n",
    "    dwh_writer_secret = json.loads(get_secret(\"prod/db/datawarehouse/sagemaker\"))\n",
    "\n",
    "    os.environ[\"SNOWFLAKE_USERNAME\"] = snowflake_sg_secret[\"username\"]\n",
    "    os.environ[\"SNOWFLAKE_PASSWORD\"] = snowflake_sg_secret[\"password\"]\n",
    "    os.environ[\"SNOWFLAKE_ACCOUNT\"] = snowflake_sg_secret[\"account\"]\n",
    "    os.environ[\"SNOWFLAKE_DATABASE\"] = snowflake_sg_secret[\"database\"]\n",
    "\n",
    "    os.environ[\"SLACK_TOKEN\"] = slack_secret[\"token\"]\n",
    "\n",
    "    os.environ[\"FINTECH_EMONEY_EMAIL\"] = fintech_service_account[\"email_name\"]\n",
    "    os.environ[\"FINTECH_EMONEY_PASSWORD\"] = fintech_service_account[\"email_password\"]\n",
    "\n",
    "    metabase_secret = json.loads(get_secret(\"prod/metabase/maxab_config\"))\n",
    "    os.environ[\"EGYPT_METABASE_USERNAME\"] = metabase_secret[\"metabase_user\"]\n",
    "    os.environ[\"EGYPT_METABASE_PASSWORD\"] = metabase_secret[\"metabase_password\"]\n",
    "\n",
    "    os.environ[\"DWH_WRITER_HOST_NEW\"] = dwh_writer_secret[\"host\"]\n",
    "    os.environ[\"DWH_WRITER_NAME_NEW\"] = dwh_writer_secret[\"dbname\"]\n",
    "    os.environ[\"DWH_WRITER_USER_NAME_NEW\"] = dwh_writer_secret[\"username\"]\n",
    "    os.environ[\"DWH_WRITER_PASSWORD_NEW\"] = dwh_writer_secret[\"password\"] \n",
    "\n",
    "    json_path_sheets = str(Path.home()) + \"/service_account_key_sheets.json\"\n",
    "    sheets_key = get_secret(\"prod/maxab-sheets\")\n",
    "    f = open(json_path_sheets, \"w\")\n",
    "    f.write(sheets_key)\n",
    "    f.close()\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"] = json_path_sheets\n",
    "\n",
    "def ret_metabase(country, question, filters={}, initialized = False):\n",
    "\n",
    "    \n",
    "    if not initialized: \n",
    "        initialize_env()\n",
    "    \n",
    "    question_id = str(question)\n",
    "    \n",
    "    if country.lower() == 'egypt':\n",
    "        base_url = 'https://bi.maxab.info/api'\n",
    "        username = str(os.environ[\"EGYPT_METABASE_USERNAME\"])\n",
    "        password = str(os.environ[\"EGYPT_METABASE_PASSWORD\"])\n",
    "    else:\n",
    "        base_url = 'https://bi.maxabma.com/api'\n",
    "        username = str(os.environ[\"AFRICA_METABASE_USERNAME\"])\n",
    "        password = str(os.environ[\"AFRICA_METABASE_PASSWORD\"])\n",
    "\n",
    "    base_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    try:\n",
    "        s_response = requests.post(\n",
    "            base_url + '/session',\n",
    "            data=json.dumps({\n",
    "                'username': username,\n",
    "                'password': password\n",
    "            }),\n",
    "            headers=base_headers)\n",
    "        \n",
    "        s_response.raise_for_status()\n",
    "\n",
    "        session_token = s_response.json()['id']\n",
    "        base_headers['X-Metabase-Session'] = session_token\n",
    "        \n",
    "        params = []\n",
    "        \n",
    "        for name, value in filters.items():\n",
    "            filter_type, filter_value = value\n",
    "            param = {'target': ['variable', ['template-tag', name]], 'value': filter_value}\n",
    "            \n",
    "            if filter_type.lower() == 'date':\n",
    "                param['type'] = 'date/range' if isinstance(filter_value, list) else 'date/single'\n",
    "            elif filter_type.lower() == 'category':\n",
    "                param['type'] = 'category'\n",
    "            elif filter_type.lower() == 'text':\n",
    "                param['type'] = 'text'\n",
    "            elif filter_type.lower() == 'number':\n",
    "                param['type'] = 'number'\n",
    "            elif filter_type.lower() == 'field list':\n",
    "                param['type'] = 'id'\n",
    "                param['target'] = ['dimension', ['template-tag', name]]\n",
    "            \n",
    "            params.append(param)\n",
    "\n",
    "        p_response = requests.post(base_url + '/card/' + question_id + '/query/csv', \n",
    "                                   json={'parameters': params}, \n",
    "                                   headers=base_headers)\n",
    "        p_response.raise_for_status()\n",
    "\n",
    "        my_dict = p_response.content\n",
    "        s = str(my_dict, 'utf-8')\n",
    "        my_dict = StringIO(s)\n",
    "        df = pd.read_csv(my_dict)\n",
    "        return(df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "743dfa72-a509-4cd7-8adf-8f4e799fea80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_column_id(df, column_name):\n",
    "    # Ensure the column is treated as a string\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    \n",
    "    # Replace commas in the string\n",
    "    df[column_name] = df[column_name].str.replace(',', '')\n",
    "    \n",
    "    # Convert back to an integer, if appropriate\n",
    "    df[column_name] = df[column_name].astype('Int64', errors='ignore')\n",
    "    \n",
    "    return df\n",
    "# ----------------------------------------\n",
    "# Mapping distribution (Segment-based)\n",
    "# ----------------------------------------\n",
    "def assign_data_by_mapping(df, mapping_df):\n",
    "    # store retail-agent mapping in a dictionary \n",
    "    mapping_dict = dict(zip(mapping_df['MAIN_SYSTEM_ID'], mapping_df['AGENT_ID']))\n",
    "        \n",
    "    assigned_agents = []\n",
    "\n",
    "    for retailer_id in df['main_system_id']:\n",
    "        if retailer_id in mapping_dict:\n",
    "            assigned_agents.append(mapping_dict[retailer_id])\n",
    "        else:\n",
    "            assigned_agents.append(None)\n",
    "   \n",
    "    df['agent_assigned'] = assigned_agents\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "feabd2ba-2b39-4f58-8cf9-ea01c2476c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_data_equal_projects(df, list):\n",
    "    df = df.sample(frac=1)  # Shuffle the data\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    assigned_data = pd.DataFrame()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.reset_index(drop=True)\n",
    "        rows_per_agent = len(project_df) // len(list)\n",
    "        remainder = len(project_df) % len(list)\n",
    "        \n",
    "        # Distribute rows equally\n",
    "        for i, agent in enumerate(list):\n",
    "            start_idx = i * rows_per_agent\n",
    "            end_idx = start_idx + rows_per_agent\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            # Handle remainder\n",
    "            if i < remainder:\n",
    "                extra_row = project_df.iloc[end_idx:end_idx+1].copy()\n",
    "                extra_row['agent_assigned'] = agent\n",
    "                agent_data = pd.concat([agent_data, extra_row])\n",
    "            \n",
    "            assigned_data = pd.concat([assigned_data, agent_data])\n",
    "    \n",
    "    assigned_data = assigned_data.reset_index(drop=True)\n",
    "    \n",
    "    return assigned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2348a69a-ad98-41f7-9635-98852debcb78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_available_agents(attendance_df, current_hour):\n",
    "    \"\"\"Get list of available task-based agents for the current hour.\"\"\"\n",
    "    attendance_copy = attendance_df.copy()\n",
    "    \n",
    "    attendance_copy['start_time'] = attendance_copy['start_time'].astype(int)\n",
    "    attendance_copy['end_time'] = attendance_copy['end_time'].astype(int)\n",
    "    \n",
    "    attendance_copy['assignment_start_time'] = attendance_copy['start_time'] - 1\n",
    "    attendance_copy['assignment_end_time'] = attendance_copy['end_time'] - 1\n",
    "    \n",
    "    attendance_copy['assign_data'] = np.where(\n",
    "        (current_hour >= attendance_copy['assignment_start_time']) & \n",
    "        (current_hour <= attendance_copy['assignment_end_time']),\n",
    "        'yes', 'no')\n",
    "    \n",
    "    task_based_agents = attendance_copy.loc[\n",
    "        (attendance_copy['project'] == 'task_based') & \n",
    "        (attendance_copy['assign_data'] == 'yes')]\n",
    "    \n",
    "    task_based_list = task_based_agents['agent_id'].values.tolist()\n",
    "    print(f\"Number of available agents: {len(task_based_list)}\")\n",
    "    return task_based_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3707a7d-300f-439d-a61a-4150733766c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def assign_offers(query_1, query_2, query_3):\n",
    "    \"\"\"\n",
    "    Assign offers from query_2 and query_3 to query_1 based on specific rules.\n",
    "    If a MAIN_SYSTEM_ID appears in both query_2 and query_3, assign one to OFFER_1 and one to OFFER_2.\n",
    "    \n",
    "    Args:\n",
    "        query_1 (pd.DataFrame): Main dataframe containing MAIN_SYSTEM_ID\n",
    "        query_2 (pd.DataFrame): Dataframe containing OFFER to be mapped as OFFER_1\n",
    "        query_3 (pd.DataFrame): Dataframe containing OFFER to be mapped as OFFER_2 or OFFER_1\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated query_1 with assigned offers\n",
    "    \"\"\"\n",
    "    # Print input columns for debugging\n",
    "    print(\"Input columns:\")\n",
    "    print(\"query_1:\", query_1.columns.tolist())\n",
    "    print(\"query_2:\", query_2.columns.tolist())\n",
    "    print(\"query_3:\", query_3.columns.tolist())\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = query_1.copy()\n",
    "    \n",
    "    # Ensure required columns exist in input dataframes\n",
    "    if 'MAIN_SYSTEM_ID' not in result.columns:\n",
    "        raise ValueError(\"query_1 must contain 'MAIN_SYSTEM_ID' column\")\n",
    "    if 'MAIN_SYSTEM_ID' not in query_2.columns or 'OFFER' not in query_2.columns:\n",
    "        raise ValueError(\"query_2 must contain 'MAIN_SYSTEM_ID' and 'OFFER' columns\")\n",
    "    if 'MAIN_SYSTEM_ID' not in query_3.columns or 'OFFER' not in query_3.columns:\n",
    "        raise ValueError(\"query_3 must contain 'MAIN_SYSTEM_ID' and 'OFFER' columns\")\n",
    "    \n",
    "    # Convert MAIN_SYSTEM_ID to string type in all dataframes\n",
    "    result['MAIN_SYSTEM_ID'] = result['MAIN_SYSTEM_ID'].astype(str)\n",
    "    query_2['MAIN_SYSTEM_ID'] = query_2['MAIN_SYSTEM_ID'].astype(str)\n",
    "    query_3['MAIN_SYSTEM_ID'] = query_3['MAIN_SYSTEM_ID'].astype(str)\n",
    "    \n",
    "    # Step 1: Map query_2 offers to OFFER_1\n",
    "    # Create a new DataFrame with just the columns we need\n",
    "    offer1_df = pd.DataFrame({\n",
    "        'MAIN_SYSTEM_ID': query_2['MAIN_SYSTEM_ID'],\n",
    "        'OFFER_1_new': query_2['OFFER']  # Use a different name to avoid conflicts\n",
    "    })\n",
    "    \n",
    "    # Merge with result\n",
    "    result = pd.merge(result, offer1_df, on='MAIN_SYSTEM_ID', how='left')\n",
    "    print(\"\\nAfter first merge, columns:\", result.columns.tolist())\n",
    "    \n",
    "    # Step 2: Map query_3 offers\n",
    "    # Create a new DataFrame with just the columns we need\n",
    "    offer_temp_df = pd.DataFrame({\n",
    "        'MAIN_SYSTEM_ID': query_3['MAIN_SYSTEM_ID'],\n",
    "        'OFFER_temp': query_3['OFFER']\n",
    "    })\n",
    "    \n",
    "    # Merge with result\n",
    "    result = pd.merge(result, offer_temp_df, on='MAIN_SYSTEM_ID', how='left')\n",
    "    print(\"After second merge, columns:\", result.columns.tolist())\n",
    "    \n",
    "    # Step 3: Assign offers based on conditions\n",
    "    # Create masks for the conditions\n",
    "    has_offer1 = result['OFFER_1'].notna()\n",
    "    has_offer1_new = result['OFFER_1_new'].notna()\n",
    "    has_temp = result['OFFER_temp'].notna()\n",
    "    \n",
    "    # First, handle cases where we have both OFFER_1_new and OFFER_temp\n",
    "    both_offers = has_offer1_new & has_temp\n",
    "    result.loc[both_offers, 'OFFER_1'] = result.loc[both_offers, 'OFFER_1_new']\n",
    "    result.loc[both_offers, 'OFFER_2'] = result.loc[both_offers, 'OFFER_temp']\n",
    "    \n",
    "    # Then handle remaining cases\n",
    "    # Where OFFER_1 is null but OFFER_1_new exists, use OFFER_1_new\n",
    "    result.loc[~has_offer1 & has_offer1_new & ~both_offers, 'OFFER_1'] = result.loc[~has_offer1 & has_offer1_new & ~both_offers, 'OFFER_1_new']\n",
    "    \n",
    "    # Where OFFER_1 is null but OFFER_temp exists, use OFFER_temp\n",
    "    result.loc[~has_offer1 & has_temp & ~both_offers, 'OFFER_1'] = result.loc[~has_offer1 & has_temp & ~both_offers, 'OFFER_temp']\n",
    "    \n",
    "    # Where OFFER_1 exists but OFFER_2 is null and OFFER_temp exists, use OFFER_temp for OFFER_2\n",
    "    result.loc[has_offer1 & ~has_offer1_new & has_temp, 'OFFER_2'] = result.loc[has_offer1 & ~has_offer1_new & has_temp, 'OFFER_temp']\n",
    "    \n",
    "    # Drop the temporary columns\n",
    "    result.drop(columns=['OFFER_1_new', 'OFFER_temp'], inplace=True)\n",
    "    \n",
    "    print(\"Final columns:\", result.columns.tolist())\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    # query_1 = pd.DataFrame({'MAIN_SYSTEM_ID': [1, 2, 3]})\n",
    "    # query_2 = pd.DataFrame({'MAIN_SYSTEM_ID': [1, 2], 'OFFER': ['A', 'B']})\n",
    "    # query_3 = pd.DataFrame({'MAIN_SYSTEM_ID': [2, 3], 'OFFER': ['C', 'D']})\n",
    "    # result = assign_offers(query_1, query_2, query_3)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "88042762-0923-49d6-80d4-bebd8857d2ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_agents(new_data):\n",
    "    Agents = google_sheets('Marketing test', 'Sheet2', 'get')\n",
    "    av_agents = Agents[Agents['Available'] == 'yes'].copy()\n",
    "    av_agents = av_agents[[\"Agent_assigned\"]]\n",
    "    agent_names = av_agents['Agent_assigned'].values\n",
    "    num_retailers = len(new_data)\n",
    "    repeated_agents = np.tile(agent_names, int(np.ceil(num_retailers / len(agent_names))))[:num_retailers]\n",
    "    np.random.shuffle(repeated_agents)\n",
    "    return repeated_agents\n",
    "def task_based():\n",
    "    now = datetime.now() + timedelta(hours=3)\n",
    "    hour = int(str(now.time())[0:2])\n",
    "    print(f\"Starting process at hour: {hour}\")\n",
    "    \n",
    "    attendance = ret_metabase(\"EGYPT\", 13502, initialized=True)\n",
    "    print(f\"Total agents: {len(attendance)}\")\n",
    "    task_based_list = get_available_agents(attendance, hour)\n",
    "    \n",
    "    query_ids = google_sheets('Agents - Retailers', 'Query ID', 'get')\n",
    "    blacklisted_retailers = query_ids['Blacklisted_retailers'].dropna().astype(int).tolist()\n",
    "    data = fetch_and_process_queries(query_ids, blacklisted_retailers)\n",
    "    main_data = assign_data_equal_projects(data , task_based_list)\n",
    "\n",
    "    agents = google_sheets('Agents - Retailers', 'Data', 'get')\n",
    "\n",
    "    # Merge on agent ID\n",
    "    main_data = main_data.merge(agents, left_on='agent_assigned', right_on='Agent_id', how='left')\n",
    "\n",
    "    # Replace the agent_assigned column with the Agent name\n",
    "    main_data['agent_assigned'] = main_data['Agent']\n",
    "\n",
    "    # Optionally drop the now-redundant columns\n",
    "    main_data.drop(columns=['Agent', 'Agent_id'], inplace=True)\n",
    "    \n",
    "    main_data.drop(columns=[\"retailer_mobile_number\"], inplace=True)\n",
    "    main_data = main_data[['agent_assigned','main_system_id','retailer_name', 'project_name','description']]\n",
    "    \n",
    "    google_sheets(\"Agents - Retailers\", \"Task_based\", \"append\", df=main_data)\n",
    "    \n",
    "    print(\"tasks added to Task_based sheet\")\n",
    "    return main_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "96db72bd-d477-4ca8-8e3b-398f3b74af51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_and_process_queries(query_ids, blacklisted_retailers):\n",
    "    \"\"\"Fetch and process data from queries, removing blacklisted retailers.\"\"\"\n",
    "    queries = query_ids['Task_based'].dropna().astype(int).tolist()\n",
    "    print(f\"Fetching data from {len(queries)} queries...\")\n",
    "    \n",
    "    # Process queries\n",
    "    dataframes = [ret_metabase(\"EGYPT\", query, initialized=True) for query in queries]\n",
    "    # print(dataframes)\n",
    "    empty_queries = []\n",
    "    for i, df in enumerate(dataframes):\n",
    "        if df.empty:\n",
    "            empty_queries.append(queries[i])\n",
    "        else:\n",
    "            print(f\"Query {queries[i]} returned {len(df)} records\")\n",
    "        df.columns = map(str.lower, df.columns)\n",
    "    if empty_queries:\n",
    "        print(f\"WARNING: Queries {empty_queries} returned empty dataframe!\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # write in google sheet available data\n",
    "    # ----------------------------------------\n",
    "    # check for empty queries and get project names\n",
    "    for idx, row in query_ids.iterrows():\n",
    "        query_id = row['Task_based']\n",
    "        if pd.notna(query_id):\n",
    "            df = ret_metabase(\"EGYPT\", int(query_id), initialized=True)\n",
    "            query_ids.at[idx, 'Available_data'] = 'Empty' if df.empty else str(len(df))\n",
    "            # Add project name if available\n",
    "            if not df.empty and 'PROJECT_NAME' in df.columns:\n",
    "                query_ids.at[idx, 'Project_Name'] = df['PROJECT_NAME'].iloc[0]\n",
    "\n",
    "    # Overwrite the sheet with updated full data\n",
    "    google_sheets('Agents - Retailers', 'Query ID', 'overwrite', df=query_ids)\n",
    "    \n",
    "    # Combine and clean dataframes\n",
    "    df_unfiltered = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"Total tasks available: {df_unfiltered.shape[0]}\")\n",
    "    \n",
    "    # Remove blacklisted retailers\n",
    "    df_raw = df_unfiltered[~df_unfiltered['main_system_id'].isin(blacklisted_retailers)]\n",
    "    df_raw = clean_column_id(df_raw, 'main_system_id')\n",
    "    print(f\"Removed {len(df_unfiltered) - len(df_raw)} blacklisted retailers\")\n",
    "    \n",
    "    return df_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1ae92c0e-deba-4b19-bef0-10699a50b783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ranking_system(query):\n",
    "    # Convert the LAST_REACHABLE_CALL column to datetime\n",
    "    query['LAST_REACHABLE_CALL'] = pd.to_datetime(query['LAST_REACHABLE_CALL'])\n",
    "\n",
    "    # Calculate the cutoff date for recent calls\n",
    "    now = datetime.now()\n",
    "    seven_days_ago = now - timedelta(days=7)\n",
    "\n",
    "    # Separate into recent and older calls\n",
    "    recent_calls = query[query['LAST_REACHABLE_CALL'] >= seven_days_ago].copy()\n",
    "    older_calls = query[(query['LAST_REACHABLE_CALL'] < seven_days_ago) | (query[\"LAST_REACHABLE_CALL\"].isnull())].copy()\n",
    "    \n",
    "    offer = older_calls[older_calls['OFFER_1'].notnull()].copy()\n",
    "    no_offer = older_calls[older_calls['OFFER_1'].isnull()].copy()\n",
    "    # Sort both by RANK\n",
    "    offer.sort_values(by=\"RANK\", ascending=True, inplace=True)\n",
    "    no_offer.sort_values(by=\"RANK\", ascending=True, inplace=True)\n",
    "    recent_calls.sort_values(by=\"RANK\", ascending=True, inplace=True)\n",
    "\n",
    "    # Combine: older calls first, then recent ones\n",
    "    reordered_df = pd.concat([offer, no_offer, recent_calls], ignore_index=True)\n",
    "\n",
    "\n",
    "    \n",
    "    return reordered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6e51a185-d09b-4867-9001-2b07283dea08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "query_1_o = ret_metabase(\"Egypt\", 60727)\n",
    "query_2_o = ret_metabase(\"Egypt\", 60731)\n",
    "query_3_o = ret_metabase(\"Egypt\", 60732)\n",
    "sheet_data_o = google_sheets(\"Agents - Retailers\", \"Main_sheet\", \"get\")\n",
    "agent_group = google_sheets(\"Agents - Retailers\", \"Agent-to-group\", \"get\")\n",
    "col_list = agent_group.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd9e808-ec64-4608-9ee7-8a5f04120fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_2_o' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery_2_o\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_2_o' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ff0dde56-34bc-4466-a124-3f9948aa760d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_1 = query_1_o.copy()\n",
    "query_2 = query_2_o.copy()\n",
    "query_3 = query_3_o.copy()\n",
    "sheet_data = sheet_data_o.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e0e1c4d8-f99b-46b2-a735-544f2a14680f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sheet isnt cleared and has 13500\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:54: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data_to_add = pd.concat([arranged_data, called_retailers], ignore_index=True)\n",
      "/tmp/ipykernel_17216/4215029290.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"GROUP_TYPE\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somaya Alrab T1\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"GROUP_TYPE\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esraa Mohamed T2\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"GROUP_TYPE\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marina Riyad T3\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"GROUP_TYPE\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raneen Ali T4\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"GROUP_TYPE\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abdelrahman Merghany T5\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"GROUP_TYPE\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Israa Abdelhamid T6\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"GROUP_TYPE\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nada Saber T7\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"GROUP_TYPE\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ebthal Saber T8\n",
      "/home/ec2-user/service_account_key.json\n",
      "/home/ec2-user/service_account_key.json\n",
      "Starting process at hour: 17\n",
      "Total agents: 8\n",
      "Number of available agents: 8\n",
      "/home/ec2-user/service_account_key.json\n",
      "Fetching data from 5 queries...\n",
      "Query 60605 returned 246 records\n",
      "Query 59703 returned 1251 records\n",
      "Query 36299 returned 17 records\n",
      "Query 35981 returned 74 records\n",
      "Query 38188 returned 54 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/3303086478.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '246' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  query_ids.at[idx, 'Available_data'] = 'Empty' if df.empty else str(len(df))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n",
      "Total tasks available: 1642\n",
      "Removed 1 blacklisted retailers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/3280234467.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(str)\n",
      "/tmp/ipykernel_17216/3280234467.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].str.replace(',', '')\n",
      "/tmp/ipykernel_17216/3280234467.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype('Int64', errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n",
      "/home/ec2-user/service_account_key.json\n",
      "tasks added to Task_based sheet\n",
      "Assigned to Somaya Alrab,207 Task-based Tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned to Esraa Mohamed,204 Task-based Tasks\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned to Marina Riyad,206 Task-based Tasks\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned to Raneen Ali,205 Task-based Tasks\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned to Abdelrahman Merghany,208 Task-based Tasks\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned to Israa Abdelhamid,203 Task-based Tasks\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned to Nada Saber,203 Task-based Tasks\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/4215029290.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned to Ebthal Saber,205 Task-based Tasks\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "# query_1 = ret_metabase(\"Egypt\", 60727)\n",
    "# query_2 = ret_metabase(\"Egypt\", 60731)\n",
    "# query_3 = ret_metabase(\"Egypt\", 60732)\n",
    "# sheet_data = google_sheets(\"Agents - Retailers\", \"Main sheet\", \"get\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Assign group project and update ranking \n",
    "# ----------------------------------------\n",
    "if sheet_data.empty:\n",
    "\n",
    "    query_1 = ranking_system(query_1)\n",
    "    query_1.drop(columns=[\"R1\"], inplace=True)\n",
    "\n",
    "    data_to_add = assign_offers(query_1, query_2, query_3)\n",
    "    \n",
    "    data_to_add[\"Call Status\"] = \"\"\n",
    "    data_to_add[\"Comment\"] = \"\"\n",
    "\n",
    "    data_to_add = data_to_add[['Call Status', 'Comment', 'MAIN_SYSTEM_ID',\n",
    "                               'GROUP_TYPE', 'OBJECTIVE', 'OFFER_1', 'OFFER_2',\n",
    "                               'OFFER_3','BALANCE','CURRENT_THP', 'THP_TARGET','LAST_REACHABLE_CALL', 'RANK']]\n",
    "\n",
    "    for i in col_list:\n",
    "        if i == \"Benchmark\":\n",
    "            continue \n",
    "        group_type = agent_group[i].iloc[0]\n",
    "        filtered_df = data_to_add[data_to_add[\"GROUP_TYPE\"] == group_type]\n",
    "        filtered_df.drop(columns=[\"GROUP_TYPE\"], inplace=True)\n",
    "        print(f\"{i}\",group_type)\n",
    "        google_sheets(i, \"Agent_group\", \"append\", df=filtered_df)\n",
    "        \n",
    "    google_sheets(\"Agents - Retailers\", \"Main_sheet\", \"append\", df=data_to_add)\n",
    "    \n",
    "else:\n",
    "# ----------------------------------------\n",
    "# update reanking in all sheets\n",
    "# ----------------------------------------\n",
    "    print(f\"sheet isnt cleared and has {len(sheet_data)}\")\n",
    "    # Merge only the 'RANK' column based on MAIN_SYSTEM_ID\n",
    "    sheet_data = google_sheets(\"Agents - Retailers\", \"Main_sheet\", \"get\")\n",
    "    sheet_data.drop(columns=[\"RANK\", 'LAST_REACHABLE_CALL'], inplace=True)\n",
    "    sheet_data = sheet_data.merge(\n",
    "        query_1[['MAIN_SYSTEM_ID', 'RANK', 'LAST_REACHABLE_CALL']],\n",
    "        on='MAIN_SYSTEM_ID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    uncalled_retailers = sheet_data[sheet_data[\"Call Status\"].isnull()].copy()\n",
    "    called_retailers = sheet_data[sheet_data[\"Call Status\"].notnull()].copy()\n",
    "    \n",
    "    arranged_data = ranking_system(uncalled_retailers)\n",
    "\n",
    "    \n",
    "    data_to_add = pd.concat([arranged_data, called_retailers], ignore_index=True)\n",
    "    data_to_add = data_to_add[['Call Status', 'Comment', 'MAIN_SYSTEM_ID',\n",
    "                               'GROUP_TYPE', 'OBJECTIVE', 'OFFER_1', 'OFFER_2',\n",
    "                               'OFFER_3','BALANCE','CURRENT_THP', 'THP_TARGET','LAST_REACHABLE_CALL', 'RANK']]\n",
    "    for i in col_list:\n",
    "        if i == \"Benchmark\":\n",
    "            continue \n",
    "        group_type = agent_group[i].iloc[0]\n",
    "        filtered_df = data_to_add[data_to_add[\"GROUP_TYPE\"] == group_type]\n",
    "        filtered_df.drop(columns=[\"GROUP_TYPE\"], inplace=True)\n",
    "        print(f\"{i}\",group_type)\n",
    "\n",
    "        google_sheets(i, \"Agent_group\", \"overwrite\", df=filtered_df)\n",
    "    \n",
    "    google_sheets(\"Agents - Retailers\", \"Main_sheet\", \"overwrite\", df=data_to_add)\n",
    "# ----------------------------------------\n",
    "# Assign Task based projects\n",
    "# ----------------------------------------\n",
    "if task_based_list:\n",
    "    task_data = task_based()\n",
    "    for i in col_list:\n",
    "        if i == \"Benchmark\":\n",
    "            continue \n",
    "        filtered_df = task_data[task_data[\"agent_assigned\"] == i]\n",
    "        filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n",
    "        print(f\"Assigned to {i},{len(filtered_df)} Task-based Tasks\")\n",
    "        google_sheets(i, \"Task_based\", \"overwrite\", df=filtered_df)\n",
    "else:\n",
    "    print(\"No agents available for current hour!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0f90b854-c8a7-4d7c-9727-2875f7496a96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Call Status', 'Comment', 'MAIN_SYSTEM_ID', 'GROUP_TYPE', 'OBJECTIVE', 'OFFER_1', 'OFFER_2', 'OFFER_3', 'BALANCE', 'CURRENT_THP', 'THP_TARGET', 'LAST_REACHABLE_CALL_x', 'RANK', 'LAST_REACHABLE_CALL_y']\n"
     ]
    }
   ],
   "source": [
    "print(uncalled_retailers.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "edeb903d-5fae-4640-87b2-d78848fe2147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somaya Alrab\n",
      "Esraa Mohamed\n",
      "Marina Riyad\n",
      "Raneen Ali\n",
      "Abdelrahman Merghany\n",
      "Israa Abdelhamid\n",
      "Nada Saber\n",
      "Ebthal Saber\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17216/2401609255.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n",
      "/tmp/ipykernel_17216/2401609255.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n",
      "/tmp/ipykernel_17216/2401609255.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n",
      "/tmp/ipykernel_17216/2401609255.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n",
      "/tmp/ipykernel_17216/2401609255.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n",
      "/tmp/ipykernel_17216/2401609255.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n",
      "/tmp/ipykernel_17216/2401609255.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n",
      "/tmp/ipykernel_17216/2401609255.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "    for i in col_list:\n",
    "        if i == \"Benchmark\":\n",
    "            continue \n",
    "        filtered_df = main_data[main_data[\"agent_assigned\"] == i]\n",
    "        filtered_df.drop(columns=[\"agent_assigned\"], inplace=True)\n",
    "        print(i)\n",
    "        google_sheets(i, \"Task_based\", \"overwrite\", df=filtered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
