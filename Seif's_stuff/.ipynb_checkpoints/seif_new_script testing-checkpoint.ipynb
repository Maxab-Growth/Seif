{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e5332-1427-43c0-b835-9157c17c6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.common_functions import google_sheets, task_fail_slack_alert, get_secret, snowflake_query\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import slack\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def initialize_env():\n",
    "    snowflake_sg_secret = json.loads(get_secret(\"Snowflake-sagemaker\"))\n",
    "    slack_secret = json.loads(get_secret(\"prod/slack/reports\"))\n",
    "    fintech_service_account = json.loads(get_secret(\"prod/fintechServiceEmail/credentials\"))\n",
    "    dwh_writer_secret = json.loads(get_secret(\"prod/db/datawarehouse/sagemaker\"))\n",
    "\n",
    "    os.environ[\"SNOWFLAKE_USERNAME\"] = snowflake_sg_secret[\"username\"]\n",
    "    os.environ[\"SNOWFLAKE_PASSWORD\"] = snowflake_sg_secret[\"password\"]\n",
    "    os.environ[\"SNOWFLAKE_ACCOUNT\"] = snowflake_sg_secret[\"account\"]\n",
    "    os.environ[\"SNOWFLAKE_DATABASE\"] = snowflake_sg_secret[\"database\"]\n",
    "\n",
    "    os.environ[\"SLACK_TOKEN\"] = slack_secret[\"token\"]\n",
    "\n",
    "    os.environ[\"FINTECH_EMONEY_EMAIL\"] = fintech_service_account[\"email_name\"]\n",
    "    os.environ[\"FINTECH_EMONEY_PASSWORD\"] = fintech_service_account[\"email_password\"]\n",
    "\n",
    "    metabase_secret = json.loads(get_secret(\"prod/metabase/maxab_config\"))\n",
    "    os.environ[\"EGYPT_METABASE_USERNAME\"] = metabase_secret[\"metabase_user\"]\n",
    "    os.environ[\"EGYPT_METABASE_PASSWORD\"] = metabase_secret[\"metabase_password\"]\n",
    "\n",
    "    os.environ[\"DWH_WRITER_HOST_NEW\"] = dwh_writer_secret[\"host\"]\n",
    "    os.environ[\"DWH_WRITER_NAME_NEW\"] = dwh_writer_secret[\"dbname\"]\n",
    "    os.environ[\"DWH_WRITER_USER_NAME_NEW\"] = dwh_writer_secret[\"username\"]\n",
    "    os.environ[\"DWH_WRITER_PASSWORD_NEW\"] = dwh_writer_secret[\"password\"] \n",
    "\n",
    "    json_path_sheets = str(Path.home()) + \"/service_account_key_sheets.json\"\n",
    "    sheets_key = get_secret(\"prod/maxab-sheets\")\n",
    "    f = open(json_path_sheets, \"w\")\n",
    "    f.write(sheets_key)\n",
    "    f.close()\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"] = json_path_sheets\n",
    "\n",
    "def ret_metabase(country, question, filters={}, initialized = False):\n",
    "\n",
    "    \n",
    "    if not initialized: \n",
    "        initialize_env()\n",
    "    \n",
    "    question_id = str(question)\n",
    "    \n",
    "    if country.lower() == 'egypt':\n",
    "        base_url = 'https://bi.maxab.info/api'\n",
    "        username = str(os.environ[\"EGYPT_METABASE_USERNAME\"])\n",
    "        password = str(os.environ[\"EGYPT_METABASE_PASSWORD\"])\n",
    "    else:\n",
    "        base_url = 'https://bi.maxabma.com/api'\n",
    "        username = str(os.environ[\"AFRICA_METABASE_USERNAME\"])\n",
    "        password = str(os.environ[\"AFRICA_METABASE_PASSWORD\"])\n",
    "\n",
    "    base_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    try:\n",
    "        s_response = requests.post(\n",
    "            base_url + '/session',\n",
    "            data=json.dumps({\n",
    "                'username': username,\n",
    "                'password': password\n",
    "            }),\n",
    "            headers=base_headers)\n",
    "        \n",
    "        s_response.raise_for_status()\n",
    "\n",
    "        session_token = s_response.json()['id']\n",
    "        base_headers['X-Metabase-Session'] = session_token\n",
    "        \n",
    "        params = []\n",
    "        \n",
    "        for name, value in filters.items():\n",
    "            filter_type, filter_value = value\n",
    "            param = {'target': ['variable', ['template-tag', name]], 'value': filter_value}\n",
    "            \n",
    "            if filter_type.lower() == 'date':\n",
    "                param['type'] = 'date/range' if isinstance(filter_value, list) else 'date/single'\n",
    "            elif filter_type.lower() == 'category':\n",
    "                param['type'] = 'category'\n",
    "            elif filter_type.lower() == 'text':\n",
    "                param['type'] = 'text'\n",
    "            elif filter_type.lower() == 'number':\n",
    "                param['type'] = 'number'\n",
    "            elif filter_type.lower() == 'field list':\n",
    "                param['type'] = 'id'\n",
    "                param['target'] = ['dimension', ['template-tag', name]]\n",
    "            \n",
    "            params.append(param)\n",
    "\n",
    "        p_response = requests.post(base_url + '/card/' + question_id + '/query/csv', \n",
    "                                   json={'parameters': params}, \n",
    "                                   headers=base_headers)\n",
    "        p_response.raise_for_status()\n",
    "\n",
    "        my_dict = p_response.content\n",
    "        s = str(my_dict, 'utf-8')\n",
    "        my_dict = StringIO(s)\n",
    "        df = pd.read_csv(my_dict)\n",
    "        return(df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def check_distribution(df, agent_list):\n",
    "    \"\"\"\n",
    "    Checks if the distribution of project types across agents is even.\n",
    "    Returns a dictionary with the count of rows per agent for each project type.\n",
    "    \"\"\"\n",
    "    distribution = {}\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        distribution[project] = project_df['agent_assigned'].value_counts().reindex(agent_list, fill_value=0)\n",
    "    \n",
    "    return distribution\n",
    "\n",
    "def redistribute_rows(df, agent_list):\n",
    "    \"\"\"\n",
    "    Redistributes rows among agents if the distribution is uneven.\n",
    "    \"\"\"\n",
    "    project_types = df['project_name'].unique()\n",
    "    redistributed_data = pd.DataFrame()\n",
    "    print(\"Redistributing rows...\")\n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.sample(frac=1).reset_index(drop=True)  # Shuffle data\n",
    "        rows_per_agent = len(project_df) // len(agent_list)\n",
    "        remainder = len(project_df) % len(agent_list)\n",
    "        \n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, agent in enumerate(agent_list):\n",
    "            end_idx = start_idx + rows_per_agent + (1 if i < remainder else 0)\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            redistributed_data = pd.concat([redistributed_data, agent_data])\n",
    "            \n",
    "            start_idx = end_idx\n",
    "    \n",
    "    redistributed_data = redistributed_data.reset_index(drop=True)\n",
    "    \n",
    "    return redistributed_data\n",
    "\n",
    "def ensure_correct_dispatching(df, agent_list, final_old_assign):\n",
    "    \"\"\"\n",
    "    Ensures the dispatching is done correctly by checking and redistributing rows if necessary.\n",
    "    `final_old_assign` rows are excluded from redistribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Exclude final_old_assign from the distribution check, but keep rows with main_system_id == 1\n",
    "    new_assignments = df[~df['main_system_id'].isin(final_old_assign['main_system_id']) | (df['main_system_id'] == 1)]\n",
    "    \n",
    "    distribution = check_distribution(new_assignments, agent_list)\n",
    "    \n",
    "    # Check if any project has an uneven distribution across agents\n",
    "    uneven_distribution = any(distribution[project].nunique() > 1 for project in distribution)\n",
    "    \n",
    "    if uneven_distribution:\n",
    "        print(\"Uneven distribution detected.\")\n",
    "        new_assignments = redistribute_rows(new_assignments, agent_list)\n",
    "    else:\n",
    "        print(\"Distribution is even. No redistribution needed.\")\n",
    "    \n",
    "    # Combine the redistributed new assignments with the final_old_assign\n",
    "    final_data = pd.concat([final_old_assign, new_assignments], ignore_index=True)\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def check_distribution_df(df, agent_list):\n",
    "    \"\"\"\n",
    "    Checks if the distribution of project types across agents is even.\n",
    "    Returns a DataFrame with the count of rows per agent for each project type.\n",
    "    \"\"\"\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    distribution_data = []\n",
    "    current_time = datetime.now()  # Get the current datetime\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        agent_counts = project_df['agent_assigned'].value_counts().reindex(agent_list, fill_value=0)\n",
    "        \n",
    "        for agent, count in agent_counts.items():\n",
    "            distribution_data.append({\n",
    "                'project_type': project,\n",
    "                'agent_assigned': agent,\n",
    "                'count': count,\n",
    "                'datetime': dt.datetime.now()  + timedelta(hours=3)\n",
    "            })\n",
    "    \n",
    "    distribution_df = pd.DataFrame(distribution_data)\n",
    "    \n",
    "    return distribution_df\n",
    "\n",
    "def clean_column_id(df, column_name):\n",
    "    # Ensure the column is treated as a string\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    \n",
    "    # Replace commas in the string\n",
    "    df[column_name] = df[column_name].str.replace(',', '')\n",
    "    \n",
    "    # Convert back to an integer, if appropriate\n",
    "    df[column_name] = df[column_name].astype('Int64', errors='ignore')\n",
    "    \n",
    "    return df\n",
    "def remove_assign(previously_assigned, df, assigns, print_it = False):\n",
    "    print(\"Starting remove_assign function...\")\n",
    "\n",
    "    try:\n",
    "        if previously_assigned.shape[0] != 1:            \n",
    "            previously_assigned[0] = previously_assigned[0].fillna('').astype(str).str.replace(\" \", \"\", regex=False)\n",
    "            previously_assigned = previously_assigned.dropna()\n",
    "            previously_assigned[0] = previously_assigned[0].astype('float').astype('int')\n",
    "            df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "            if 'agent_assigned' in df.columns:\n",
    "                if df['agent_assigned'].dtype == 'object':\n",
    "                    df.loc[df['agent_assigned'].notna(), 'agent_assigned'] = df.loc[df['agent_assigned'].notna(), 'agent_assigned'].astype(int)\n",
    "\n",
    "                    print(\"Converted agent_assigned column to int.\")\n",
    "\n",
    "            filtered_ids = previously_assigned[0].astype(int).values\n",
    "            main_data_to_assign = df.loc[~df['main_system_id'].isin(filtered_ids) | (df['main_system_id'] == 1)\n",
    "            ]\n",
    "            if print_it:\n",
    "                print(f\"Available tasks for this batch after filtering. Rows remaining: {len(main_data_to_assign)}\")\n",
    "                print(f\" Retailers that have no agents to assign to {main_data_to_assign['agent_assigned'].isna().sum()}\")\n",
    "\n",
    "            main_data_to_assign['main_system_id'] = main_data_to_assign['main_system_id'].astype('int')\n",
    "            main_data_to_assign = main_data_to_assign.groupby('agent_assigned').head(assigns)\n",
    "\n",
    "            return main_data_to_assign\n",
    "\n",
    "        else:\n",
    "            print(\"Only one previously assigned entry, skipping filter.\")\n",
    "            df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "            main_data_to_assign = df.groupby('agent_assigned').head(assigns)\n",
    "            return main_data_to_assign\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] remove_assign failed: {e}\")\n",
    "# ----------------------------------------\n",
    "# Random distribution (Equal assigning)   \n",
    "# ----------------------------------------\n",
    "def assign_data_equal_projects(df, list):\n",
    "    df = df.sample(frac=1)  # Shuffle the data\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    assigned_data = pd.DataFrame()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.reset_index(drop=True)\n",
    "        rows_per_agent = len(project_df) // len(list)\n",
    "        remainder = len(project_df) % len(list)\n",
    "        \n",
    "        # Distribute rows equally\n",
    "        for i, agent in enumerate(list):\n",
    "            start_idx = i * rows_per_agent\n",
    "            end_idx = start_idx + rows_per_agent\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            # Handle remainder\n",
    "            if i < remainder:\n",
    "                extra_row = project_df.iloc[end_idx:end_idx+1].copy()\n",
    "                extra_row['agent_assigned'] = agent\n",
    "                agent_data = pd.concat([agent_data, extra_row])\n",
    "            \n",
    "            assigned_data = pd.concat([assigned_data, agent_data])\n",
    "    \n",
    "    assigned_data = assigned_data.reset_index(drop=True)\n",
    "    \n",
    "    return assigned_data\n",
    "\n",
    "# ----------------------------------------\n",
    "# Mapping distribution (Segment-based)\n",
    "# ----------------------------------------\n",
    "def assign_data_by_mapping(df, mapping_df):\n",
    "    # store retail-agent mapping in a dictionary \n",
    "    mapping_dict = dict(zip(mapping_df['MAIN_SYSTEM_ID'], mapping_df['AGENT_ID']))\n",
    "        \n",
    "    assigned_agents = []\n",
    "\n",
    "    for retailer_id in df['main_system_id']:\n",
    "        if retailer_id in mapping_dict:\n",
    "            assigned_agents.append(mapping_dict[retailer_id])\n",
    "        else:\n",
    "            assigned_agents.append(None)\n",
    "   \n",
    "    df['agent_assigned'] = assigned_agents\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_available_agents(attendance_df, current_hour):\n",
    "    \"\"\"Get list of available task-based agents for the current hour.\"\"\"\n",
    "    attendance_copy = attendance_df.copy()\n",
    "    \n",
    "    attendance_copy['start_time'] = attendance_copy['start_time'].astype(int)\n",
    "    attendance_copy['end_time'] = attendance_copy['end_time'].astype(int)\n",
    "    \n",
    "    attendance_copy['assignment_start_time'] = attendance_copy['start_time'] - 1\n",
    "    attendance_copy['assignment_end_time'] = attendance_copy['end_time'] - 1\n",
    "    \n",
    "    attendance_copy['assign_data'] = np.where(\n",
    "        (current_hour >= attendance_copy['assignment_start_time']) & \n",
    "        (current_hour <= attendance_copy['assignment_end_time']),\n",
    "        'yes', 'no')\n",
    "    \n",
    "    task_based_agents = attendance_copy.loc[\n",
    "        (attendance_copy['project'] == 'task_based') & \n",
    "        (attendance_copy['assign_data'] == 'yes')]\n",
    "    \n",
    "    task_based_list = task_based_agents['agent_id'].values.tolist()\n",
    "    print(f\"Number of available agents: {len(task_based_list)}\")\n",
    "    return task_based_list\n",
    "\n",
    "def fetch_and_process_queries(query_ids, blacklisted_retailers):\n",
    "    \"\"\"Fetch and process data from queries, removing blacklisted retailers.\"\"\"\n",
    "    queries_for_random = query_ids['Query_id_random'].dropna().astype(int).tolist()\n",
    "    queries_for_mapped = query_ids['Query_id_segment_based'].dropna().astype(int).tolist()\n",
    "    print(f\"Fetching data from {len(queries_for_random)+len(queries_for_mapped)} queries...\")\n",
    "    \n",
    "    # Process random queries\n",
    "    dataframes_R = [ret_metabase(\"EGYPT\", query, initialized=True) for query in queries_for_random]\n",
    "    empty_queries = []\n",
    "    for i, df in enumerate(dataframes_R):\n",
    "        if df.empty:\n",
    "            empty_queries.append(queries_for_random[i])\n",
    "        else:\n",
    "            print(f\"Query {queries_for_random[i]} returned {len(df)} records\")\n",
    "        df.columns = map(str.lower, df.columns)\n",
    "    if empty_queries:\n",
    "        print(f\"WARNING: Queries {empty_queries} returned empty dataframe!\")\n",
    "    # ----------------------------------------\n",
    "    # write in google sheet available data\n",
    "    # ----------------------------------------\n",
    "    # check for empty queries in Random tasks\n",
    "    for idx, row in query_ids.iterrows():\n",
    "        query_id = row['Query_id_random']\n",
    "        if pd.notna(query_id):\n",
    "            df = ret_metabase(\"EGYPT\", int(query_id), initialized=True)\n",
    "            query_ids.at[idx, 'Available_data'] = 'Empty' if df.empty else str(len(df))\n",
    "\n",
    "    # Step 3: Overwrite the sheet with updated full data (including manually-entered columns like F)\n",
    "    google_sheets('Query ID Assigning', 'Sheet1', 'overwrite', df=query_ids)\n",
    "    \n",
    "    # check for empty queries in Segment-based\n",
    "    for idx, row in query_ids.iterrows():\n",
    "        query_id = row['Query_id_segment_based']\n",
    "        if pd.notna(query_id):\n",
    "            df = ret_metabase(\"EGYPT\", int(query_id), initialized=True)\n",
    "            query_ids.at[idx, 'Available_data'] = 'Empty' if df.empty else str(len(df))\n",
    "\n",
    "    # Step 3: Overwrite the sheet with updated full data (including manually-entered columns like F)\n",
    "    google_sheets('Query ID Assigning', 'Sheet1', 'overwrite', df=query_ids)\n",
    "    \n",
    "    # Process mapped queries\n",
    "    dataframes_M = [ret_metabase(\"EGYPT\", query, initialized=True) for query in queries_for_mapped]\n",
    "    for i, df in enumerate(dataframes_M):\n",
    "        if df.empty:\n",
    "            print(f\"WARNING: Query {queries_for_mapped[i]} returned empty dataframe!\")\n",
    "        else:\n",
    "            print(f\"Query {queries_for_mapped[i]} returned {len(df)} records\")\n",
    "        df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "    # Combine and clean dataframes\n",
    "    df_unfiltered_R = pd.concat(dataframes_R, ignore_index=True)\n",
    "    df_unfiltered_M = pd.concat(dataframes_M, ignore_index=True)\n",
    "    print(f\"Mapping tasks available: {df_unfiltered_M.shape[0]}\")\n",
    "    print(f\"Random tasks available: {df_unfiltered_R.shape[0]}\")\n",
    "    \n",
    "    # Remove blacklisted retailers\n",
    "    df_raw_R = df_unfiltered_R[~df_unfiltered_R['main_system_id'].isin(blacklisted_retailers)]\n",
    "    df_raw_M = df_unfiltered_M[~df_unfiltered_M['main_system_id'].isin(blacklisted_retailers)]\n",
    "    \n",
    "    df_raw_R = clean_column_id(df_raw_R, 'main_system_id')\n",
    "    df_raw_M = clean_column_id(df_raw_M, 'main_system_id')\n",
    "    print(f\"Removed {(len(df_unfiltered_R)+len(df_unfiltered_M)) - (len(df_raw_R)+len(df_raw_M))} blacklisted retailers\")\n",
    "    \n",
    "    return df_raw_R, df_raw_M\n",
    "\n",
    "def process_previous_assignments(df_raw_R, previous_calls):\n",
    "    \"\"\"Process previous assignments and separate new and old assignments.\"\"\"\n",
    "    exclude_same_assigns = clean_column_id(previous_calls, 'main_system_id')\n",
    "    \n",
    "    # Get new assignments\n",
    "    task_based = pd.DataFrame(df_raw_R.loc[~df_raw_R['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "    \n",
    "    # Get old assignments\n",
    "    old_assign = pd.DataFrame(df_raw_R.loc[df_raw_R['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "    df_1 = old_assign.merge(previous_calls, on='main_system_id', how='left')\n",
    "    final_old_assign = df_1[[\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"project_name\"]]\n",
    "    \n",
    "    return task_based, final_old_assign\n",
    "\n",
    "def task_based_assignment():\n",
    "    # ----------------------------------------\n",
    "    # Initialize and get current time\n",
    "    # ----------------------------------------\n",
    "    initialize_env()\n",
    "    now = datetime.now() + timedelta(hours=3)\n",
    "    hour = int(str(now.time())[0:2])\n",
    "    print(f\"Starting process at hour: {hour}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Get available agents\n",
    "    # ----------------------------------------\n",
    "    attendance = ret_metabase(\"EGYPT\", 13502, initialized=True)\n",
    "    print(f\"Total agents: {len(attendance)}\")\n",
    "    task_based_list = get_available_agents(attendance, hour)\n",
    "    time.sleep(15)\n",
    "    \n",
    "    if not task_based_list:\n",
    "        print(\"No agents available for current hour!\")\n",
    "        return\n",
    "        \n",
    "    # ----------------------------------------\n",
    "    # Get query IDs and blacklisted retailers\n",
    "    # ----------------------------------------\n",
    "    query_ids = google_sheets('Query ID Assigning', 'Sheet1', 'get')\n",
    "    blacklisted_retailers = query_ids['Blacklisted_retailers'].dropna().astype(int).tolist()\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Fetch and process query data\n",
    "    # ----------------------------------------\n",
    "    df_raw_R, df_raw_M = fetch_and_process_queries(query_ids, blacklisted_retailers)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Process previous assignments / For follow up calls\n",
    "    # ----------------------------------------\n",
    "    previous_calls = ret_metabase(\"EGYPT\", 35299, initialized=True)\n",
    "    previous_calls = clean_column_id(previous_calls, 'main_system_id')\n",
    "    task_based, final_old_assign = process_previous_assignments(df_raw_R, previous_calls)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Assign agents to tasks\n",
    "    # ----------------------------------------\n",
    "    main_data_list = []\n",
    "    \n",
    "    # Get and process mapping data\n",
    "    mapping_df = ret_metabase('Egypt',59587)\n",
    "    mapping_df = clean_column_id(mapping_df, 'MAIN_SYSTEM_ID')\n",
    "    mapping_df = clean_column_id(mapping_df, 'AGENT_ID')\n",
    "    \n",
    "    if mapping_df.empty:\n",
    "        print(\"WARNING: No mapping data found!\")\n",
    "    else:\n",
    "        print(f\"Found {len(mapping_df)} mapping records\")\n",
    "        mapping_df = mapping_df[mapping_df['AGENT_ID'].isin(task_based_list)]\n",
    "        mapped_data = assign_data_by_mapping(df_raw_M, mapping_df)\n",
    "        \n",
    "        if mapped_data.empty:\n",
    "            print(\"WARNING: No data mapped to agents!\")\n",
    "        else:\n",
    "            notna = mapped_data['agent_assigned'].notna().sum()\n",
    "            na = mapped_data['agent_assigned'].isna().sum()\n",
    "            print(f\"Successfully mapped {notna} records to agents\")\n",
    "            print(f\"Retailers unassigned: {na}\")\n",
    "        main_data_list.append(mapped_data)\n",
    "    \n",
    "    # Process tasks by priority\n",
    "    priority_range = range(1, 16)  # 1 to 15 inclusive\n",
    "    for priority in priority_range:\n",
    "        priority_df = task_based[task_based[\"priority\"] == priority].reset_index(drop=True)\n",
    "        main_data = assign_data_equal_projects(priority_df, task_based_list)\n",
    "        main_data_list.append(main_data)\n",
    "    \n",
    "    main_data_total = pd.concat(main_data_list, ignore_index=True)\n",
    "    print(f\"Total assignments available (including ones already assigned): {main_data_total.shape[0]}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Clean and filter data\n",
    "    # ----------------------------------------\n",
    "    # Select required columns\n",
    "    main_data = main_data_total[\n",
    "        [\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"project_name\"]]\n",
    "    \n",
    "    # Handle special case for main_system_id == 1\n",
    "    main_system_id_1 = main_data[main_data['main_system_id'] == 1]\n",
    "    other_main_system_ids = main_data[main_data['main_system_id'] != 1]\n",
    "    other_main_system_ids = other_main_system_ids.drop_duplicates(subset=['main_system_id'])\n",
    "    main_data = pd.concat([main_system_id_1, other_main_system_ids], ignore_index=True)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Process previous assignments\n",
    "    # ----------------------------------------\n",
    "    sheet = google_sheets('[HOURLY] TASK-BASED Data', 'Data', 'get', [6])\n",
    "    already_assigned = [[str(val)] for val in sheet.iloc[1:, 0].dropna()]\n",
    "    already_assigned_df = pd.DataFrame.from_dict(already_assigned)\n",
    "    \n",
    "    df = main_data.copy()\n",
    "    \n",
    "    # Remove previous assignments and limit retailers per agent\n",
    "    if not already_assigned_df.empty:\n",
    "        main_data_to_assign = remove_assign(already_assigned_df, df, 40, True)\n",
    "        final_old_assign_new = remove_assign(already_assigned_df, final_old_assign, 5)\n",
    "    else:\n",
    "        main_data_to_assign = df.groupby('agent_assigned').head(40)\n",
    "        final_old_assign_new = final_old_assign.groupby('agent_assigned').head(5)\n",
    "    \n",
    "    # Filter for available agents\n",
    "    filtered_df = main_data_to_assign[main_data_to_assign['agent_assigned'].isin(task_based_list)]\n",
    "    filtered_df_old = final_old_assign_new[final_old_assign_new['agent_assigned'].isin(task_based_list)]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        print(\"WARNING: No assignments after filtering!\")\n",
    "    else:\n",
    "        print(f\"Number of filtered assignments: {len(filtered_df)}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Prepare final data\n",
    "    # ----------------------------------------\n",
    "    final_data_to_assign = filtered_df.drop_duplicates(subset='main_system_id', keep='first').copy()\n",
    "    final_old_assign_new = filtered_df_old.drop_duplicates(subset='main_system_id', keep='first').copy()\n",
    "    \n",
    "    final_data_to_assign['added_at'] = now\n",
    "    final_old_assign_new['added_at'] = now\n",
    "    \n",
    "    final_data_to_assign = final_data_to_assign.drop(columns='index', errors='ignore')\n",
    "    final_data_to_assign = ensure_correct_dispatching(final_data_to_assign, task_based_list, final_old_assign_new)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Save data to various destinations\n",
    "    # ----------------------------------------\n",
    "    print(\"Starting data export process...\")\n",
    "    \n",
    "    # Save to parquet\n",
    "    final_data_to_assign = final_data_to_assign.astype(str)\n",
    "    sheet_df = final_data_to_assign[[\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"added_at\"]]\n",
    "    \n",
    "\n",
    "\n",
    "    google_sh = google_sheets('[HOURLY] TASK-BASED Data', 'raw_data', 'append', df=sheet_df)\n",
    "\n",
    "\n",
    "    # importing the distribution count in a sheet to track the dispatching \n",
    "    distribution_df = check_distribution_df(final_data_to_assign, task_based_list)\n",
    "    distribution_df = distribution_df.astype(str)\n",
    "\n",
    "    google_sh = google_sheets('Tracking Project Distribution', 'data', 'append', df=distribution_df)\n",
    "\n",
    "def update_am_attendance_table():\n",
    "    # setup the environment:\n",
    "    initialize_env()\n",
    "\n",
    "    # get secrets:\n",
    "    host = os.environ[\"DWH_WRITER_HOST_NEW\"]\n",
    "    database = os.environ[\"DWH_WRITER_NAME_NEW\"]\n",
    "    user = os.environ[\"DWH_WRITER_USER_NAME_NEW\"]\n",
    "    password = os.environ[\"DWH_WRITER_PASSWORD_NEW\"]\n",
    "\n",
    "    conn = psycopg2.connect(host=host, database=database, user=user, password=password)\n",
    "\n",
    "    # get the df:\n",
    "    df_3 = google_sheets(\"daily_automatic_assignment\", \"attendance_table\", \"get\")\n",
    "\n",
    "    # db connection:\n",
    "    engine = sqlalchemy.create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}/{database}\")\n",
    "    print(bool(engine))\n",
    "\n",
    "    # data cleaning in am_attendance\n",
    "    for col in ['id', 'agent_id']:\n",
    "        df_3.loc[df_3[col] != '', col] = df_3.loc[df_3[col] != '', col].astype(\"int\")\n",
    "\n",
    "    for col in ['date']:\n",
    "        df_3.loc[df_3[col] != '', col] = df_3.loc[df_3[col] != '', col].astype(\"datetime64[ns]\")\n",
    "\n",
    "    # add last_updated_at:\n",
    "    df_3[\"last_updated_at\"] = datetime.now()\n",
    "\n",
    "    # update the table on db:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"start\")\n",
    "        df_3.to_sql(name='am_attendance', schema='fintech', con=engine, if_exists='replace', chunksize=1000, method='multi')\n",
    "        print(\"end\")\n",
    "        conn.close()\n",
    "\n",
    "    time.sleep(10)\n",
    "def inserting_to_autodialer():\n",
    "    import numpy as np \n",
    "    from sshtunnel import SSHTunnelForwarder\n",
    "    import psycopg2\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "\n",
    "    three_cx_secret = json.loads(get_secret('prod/3cx/postgres'))\n",
    "    dbname = three_cx_secret[\"csat_dbname\"]\n",
    "    host = three_cx_secret[\"host\"]\n",
    "    password = three_cx_secret[\"csat_password\"]\n",
    "    user = three_cx_secret[\"username\"]\n",
    "    port = three_cx_secret[\"csat_port\"]\n",
    "    \n",
    "    df = pd.read_parquet(\"s3://airflow-temp-files-maxab/fintech_acc_mgmt/hourly_sheet.parquet\")\n",
    "    df['agent_assigned'] = df['agent_assigned'].astype(\"int\")\n",
    "    df['three_cx_id'] = 0\n",
    "    \n",
    "    print(\"mapping the agents_ids to three_cx_ids\")\n",
    "    agents_3cx_ids_mapping = {2484:109, 2648:415, 4078:289, 5280:150, 8302:137, 9191:147, 9785:469}\n",
    "    for agent_id, three_cx_id in agents_3cx_ids_mapping.items():    \n",
    "        df['three_cx_id'] = np.where(df['agent_assigned'] == agent_id, three_cx_id,df['three_cx_id'])\n",
    "\n",
    "    print(\"Querying the list retailers' decrypted mobile\")\n",
    "    list_retailers = df['main_system_id'].astype(int).unique().tolist()\n",
    "    list_retailers = f\"{list_retailers}\".replace('[','').replace(']','')\n",
    "    retailers_mobile = snowflake_query(country='egypt', \n",
    "                                       query =f\"\"\"SELECT id, mobile from datawarehouse.public.retailers where id in ({list_retailers})\"\"\")\n",
    "    df = df.merge(retailers_mobile, how='left', left_on = 'main_system_id', right_on = 'id')\n",
    "\n",
    "    print(\"converting the rows to list of tuples to then insert to it database\")\n",
    "    final_df = df[['main_system_id', 'mobile', 'retailer_name', 'three_cx_id', 'description', 'offer', 'reward']]\n",
    "    final_values = [tuple(x) for x in final_df.to_numpy()]\n",
    "\n",
    "    with psycopg2.connect(\n",
    "        dbname=dbname,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port\n",
    "    ) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.executemany(f\"\"\"INSERT INTO persons(customer_id, number, name, queues, offer_description, offer, reward) \n",
    "                            values (%s, %s, %s, %s, %s, %s, %s);\"\"\", final_values)\n",
    "            conn.commit()\n",
    "    \n",
    "    print(\"The data is inserted successfully\")\n",
    "            \n",
    "default_args = {\n",
    "    'owner': 'Rowaa',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 12, 4),\n",
    "    'on_failure_callback': task_fail_slack_alert,\n",
    "    'schedule_interval': '30 6-19 * * *',\n",
    "    'catchup': False\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='fintech_account_management_dispatching',\n",
    "    schedule_interval='30 6-19 * * *',\n",
    "    # schedule_interval='30 8,9,10,11,12,13,17,18,19 * * *',\n",
    "    catchup=False,\n",
    "    default_args=default_args,\n",
    "    tags=['fintech_account_management_hourly_dispatching'],\n",
    "    dagrun_timeout=timedelta(minutes=40),\n",
    ") as dag:\n",
    "    update_am_attendance_table_task = PythonOperator(\n",
    "        task_id='update_am_attendance_table_task',\n",
    "        python_callable=update_am_attendance_table\n",
    "    )\n",
    "    task_based_assignment_task = PythonOperator(\n",
    "        task_id='task_based_assignment_task',\n",
    "        python_callable=task_based_assignment\n",
    "    )\n",
    "\n",
    "update_am_attendance_table_task>>task_based_assignment_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
