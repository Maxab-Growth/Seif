{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5530c8f6-e376-4bff-8174-c477a9ca4d96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from common_functions import get_secret, ret_metabase, google_sheets\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sqlalchemy\n",
    "import requests\n",
    "import psycopg2\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "import requests\n",
    "def get_jwt(country='EG'):\n",
    "    import jwt\n",
    "    fintech_service_account = json.loads(get_secret(\"prod/fintechServiceEmail/credentials\"))\n",
    "    fintech_service_account_emailname= fintech_service_account[\"email_name\"]\n",
    "    fintech_service_account_emailpass= fintech_service_account[\"email_password\"]\n",
    "    payload = {\n",
    "            \"client_id\":\"admin-portal\",\n",
    "            \"grant_type\":\"password\",\n",
    "            \"username\":fintech_service_account_emailname,\n",
    "            \"password\":fintech_service_account_emailpass \n",
    "        }\n",
    " \n",
    "    r = requests.post(\"https://sso.maxab.info/auth/realms/maxab/protocol/openid-connect/token\",\n",
    "        headers={\"Content-Type\":\"application/x-www-form-urlencoded\"},\n",
    "        data=payload\n",
    "        )\n",
    "\n",
    "    fullJwtResponse = r.json()\n",
    "    jwt_access_token = fullJwtResponse['access_token']\n",
    "        \n",
    "    return jwt_access_token\n",
    "\n",
    "def initialize_env():\n",
    "    \"\"\"Initialize environment variables if needed\"\"\"\n",
    "    # Add any environment initialization logic here\n",
    "    pass\n",
    "\n",
    "def send_text_slack(channel, text):\n",
    "    \"\"\"\n",
    "    Send a text message to a Slack channel.\n",
    "    \n",
    "    Args:\n",
    "        channel (str): The Slack channel name to send the message to\n",
    "        text (str): The message text to send\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If the message fails to send\n",
    "    \"\"\"\n",
    "    print(f\"Sending Slack message to channel '{channel}'...\")\n",
    "    \n",
    "    import slack\n",
    "    import os\n",
    "\n",
    "    initialize_env()\n",
    "\n",
    "    client = slack.WebClient(token=os.environ[\"SLACK_TOKEN\"])\n",
    "    try:\n",
    "        client.chat_postMessage(\n",
    "            channel=channel,\n",
    "            text=text\n",
    "        )\n",
    "        print('Slack message sent successfully')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to send Slack message: {str(e)}')\n",
    "        raise e\n",
    "\n",
    "def send_file_to_slack(channel, file_path, title=None):\n",
    "    \"\"\"\n",
    "    Send a file to a Slack channel.\n",
    "    \n",
    "    Args:\n",
    "        channel (str): The Slack channel name to send the file to\n",
    "        file_path (str): The path to the file to upload\n",
    "        title (str): Optional title for the file\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If the file upload fails\n",
    "    \"\"\"\n",
    "    print(f\"Sending file to Slack channel '{channel}': {file_path}\")\n",
    "    \n",
    "    import slack\n",
    "    import os\n",
    "\n",
    "    initialize_env()\n",
    "\n",
    "    client = slack.WebClient(token=os.environ[\"SLACK_TOKEN\"])\n",
    "    try:\n",
    "        # Upload file to Slack\n",
    "        response = client.files_upload(\n",
    "            channels=channel,\n",
    "            file=file_path,\n",
    "            title=title or os.path.basename(file_path)\n",
    "        )\n",
    "        print(f'File uploaded successfully: {response[\"file\"][\"name\"]}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to upload file to Slack: {str(e)}')\n",
    "        raise e\n",
    "\n",
    "def send_daily_report_slack(cancellation_stats, creation_stats, dispatch_stats, creation_df, execution_date):\n",
    "    \"\"\"\n",
    "    Send a daily summary report to Slack with all task statistics and creation data as Excel file.\n",
    "    \n",
    "    Args:\n",
    "        cancellation_stats (dict): Statistics from cancellation batch processing\n",
    "        creation_stats (dict): Statistics from creation batch processing  \n",
    "        dispatch_stats (dict): Statistics from dispatch batch processing\n",
    "        creation_df (DataFrame): The creation data DataFrame\n",
    "        execution_date (datetime): The execution date\n",
    "    \"\"\"\n",
    "    print(\"Preparing daily Slack report...\")\n",
    "    \n",
    "    # Format the execution date\n",
    "    exec_date_str = execution_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Create summary statistics\n",
    "    total_cancellation_batches = cancellation_stats.get('total_batches', 0)\n",
    "    successful_cancellation_batches = cancellation_stats.get('successful_batches', 0)\n",
    "    cancellation_success_rate = cancellation_stats.get('success_rate', 0)\n",
    "    \n",
    "    total_creation_batches = creation_stats.get('total_batches', 0)\n",
    "    successful_creation_batches = creation_stats.get('successful_batches', 0)\n",
    "    creation_success_rate = creation_stats.get('success_rate', 0)\n",
    "    total_rows_created = creation_stats.get('total_rows_created', 0)\n",
    "    \n",
    "    total_dispatch_batches = dispatch_stats.get('total_batches', 0)\n",
    "    successful_dispatch_batches = dispatch_stats.get('successful_batches', 0)\n",
    "    dispatch_success_rate = dispatch_stats.get('success_rate', 0)\n",
    "    total_rows_dispatched = dispatch_stats.get('total_rows_dispatched', 0)\n",
    "    \n",
    "    # Get creation data summary\n",
    "    total_creation_rows = len(creation_df) if creation_df is not None else 0\n",
    "    unique_workflows = creation_df['Workflow ID'].nunique() if creation_df is not None and 'Workflow ID' in creation_df.columns else 0\n",
    "    \n",
    "    # Create the Slack message\n",
    "    slack_msg = f\"\"\"\n",
    ":bar_chart: *Daily Ecom Dispatching Report - {exec_date_str}*\n",
    "\n",
    "*📊 Task Processing Summary:*\n",
    "\n",
    "*🔄 Cancellation Tasks:*\n",
    "• Total Batches: {total_cancellation_batches}\n",
    "• Successful: {successful_cancellation_batches}\n",
    "• Success Rate: {cancellation_success_rate:.1f}%\n",
    "\n",
    "*📝 Creation Tasks:*\n",
    "• Total Batches: {total_creation_batches}\n",
    "• Successful: {successful_creation_batches}\n",
    "• Success Rate: {creation_success_rate:.1f}%\n",
    "• Rows Created: {total_rows_created:,}\n",
    "\n",
    "*📤 Dispatch Tasks:*\n",
    "• Total Batches: {total_dispatch_batches}\n",
    "• Successful: {successful_dispatch_batches}\n",
    "• Success Rate: {dispatch_success_rate:.1f}%\n",
    "• Rows Dispatched: {total_rows_dispatched:,}\n",
    "\n",
    "*📈 Creation Data Summary:*\n",
    "• Total Rows Processed: {total_creation_rows:,}\n",
    "• Unique Workflows: {unique_workflows}\n",
    "\n",
    "*🔍 Data Integrity Check:*\n",
    "• Rows Created: {total_rows_created:,}\n",
    "• Rows Dispatched: {total_rows_dispatched:,}\n",
    "• Match Status: {'✅ MATCH' if total_rows_created == total_rows_dispatched else '🚨 MISMATCH'}\n",
    "{f'• Difference: {abs(total_rows_created - total_rows_dispatched):,} rows' if total_rows_created != total_rows_dispatched else ''}\n",
    "\n",
    "*🎯 Overall Performance:*\n",
    "• Total Batches: {total_cancellation_batches + total_creation_batches + total_dispatch_batches}\n",
    "• Total Successful: {successful_cancellation_batches + successful_creation_batches + successful_dispatch_batches}\n",
    "• Overall Success Rate: {((successful_cancellation_batches + successful_creation_batches + successful_dispatch_batches) / max(1, total_cancellation_batches + total_creation_batches + total_dispatch_batches) * 100):.1f}%\n",
    "\n",
    ":white_check_mark: Daily dispatching process completed!\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send the text message first\n",
    "    send_text_slack(channel='seif_error_logs', text=slack_msg)\n",
    "    \n",
    "    # Now send the Excel file with creation data\n",
    "    if creation_df is not None and len(creation_df) > 0:\n",
    "        try:\n",
    "            # Create Excel file with creation data\n",
    "            excel_filename = f\"creation_data_{execution_date.strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "            \n",
    "            # Save DataFrame to Excel file\n",
    "            creation_df.to_excel(excel_filename, index=False)\n",
    "            \n",
    "            # Send file to Slack\n",
    "            send_file_to_slack(channel='seif_error_logs', file_path=excel_filename, \n",
    "                              title=f\"Creation Data - {execution_date.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            # Clean up the temporary file\n",
    "            if os.path.exists(excel_filename):\n",
    "                os.remove(excel_filename)\n",
    "                \n",
    "            print(f\"Excel file sent to Slack: {excel_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to send Excel file to Slack: {str(e)}\")\n",
    "            # Don't fail the entire process if file upload fails\n",
    "    else:\n",
    "        print(\"No creation data to send as Excel file\")\n",
    "\n",
    "def task_fail_slack_alert(context):\n",
    "    \"\"\"\n",
    "    Send a Slack alert when an Airflow task fails.\n",
    "    \n",
    "    This function is called by Airflow's on_failure_callback and formats\n",
    "    the error information into a readable Slack message.\n",
    "    \n",
    "    Args:\n",
    "        context (dict): Airflow context containing task failure information\n",
    "    \"\"\"\n",
    "    print(\"Preparing Slack failure alert...\")\n",
    "    \n",
    "    slack_msg = \"\"\"\n",
    "        :red_circle: Task Failed.\n",
    "        *Task*: {task}  \n",
    "        *Dag*: {dag} \n",
    "        *Execution Time*: {exec_date}  \n",
    "        *Reason*: {exception}\n",
    "    \"\"\".format(\n",
    "        task=context.get('task_instance').task_id,\n",
    "        dag=context.get('task_instance').dag_id,\n",
    "        exec_date=context.get('execution_date'),\n",
    "        exception=context.get('exception')\n",
    "    )\n",
    "\n",
    "    send_text_slack(channel='seif_error_logs', text=slack_msg)\n",
    "\n",
    "def EG_Ecom_on_ground_dispatching():\n",
    "    \"\"\"\n",
    "    Main function for Egypt Ecom on ground dispatching process\n",
    "    \"\"\"\n",
    "    print(\"Starting Egypt Ecom on Ground Dispatching process...\")\n",
    "    \n",
    "    # Get data from Metabase\n",
    "    df = ret_metabase(\"Egypt\", 63605)  \n",
    "    creation_df = df.copy()\n",
    "    creation_df = creation_df[['RETAILER_ID', 'Workflow ID', 'Date', 'Description']]\n",
    "    # google_sheets(\"On_Ground_Dispatch_LOGS\", \"fetched_data\", \"overwrite\", df=df)\n",
    "\n",
    "    # Separate DataFrame for cancellation with different query ID\n",
    "    # df_cancellation = ret_metabase(\"Egypt\", 62832)\n",
    "    # cancellation_df = df_cancellation.copy()\n",
    "\n",
    "    AT = get_jwt()\n",
    "\n",
    "    # ------------------------\n",
    "    # Bulk Cancellation with Batch Processing (2000 batch limit)\n",
    "    # ------------------------\n",
    "\n",
    "#     def process_cancellation_batch(batch_df, batch_number, total_batches):\n",
    "#         \"\"\"\n",
    "#         Process a single batch of data for bulk task cancellation\n",
    "#         \"\"\"\n",
    "#         # Create a temporary file for this batch\n",
    "#         batch_file_path = f\"data_for_cancellation_batch_{batch_number}.xlsx\"\n",
    "        \n",
    "#         try:\n",
    "#             # Save batch DataFrame to an Excel file\n",
    "#             batch_df.to_excel(batch_file_path, index=False)\n",
    "            \n",
    "#             # Prepare the request\n",
    "#             url = \"https://api.maxab.info/logistics/task-based/api/portal/v1/tasks/sheets/cancellation\"\n",
    "            \n",
    "#             headers = {\n",
    "#                 \"accept\": \"application/json, text/plain, */*\",\n",
    "#                 \"authorization\": f\"Bearer {AT}\",\n",
    "#                 \"country_identifier\": \"EG\",\n",
    "#                 \"language\": \"AR\",\n",
    "#                 \"origin\": \"https://logistics.maxab.info\",\n",
    "#                 \"referer\": \"https://logistics.maxab.info/\",\n",
    "#                 \"user-agent\": \"Mozilla/5.0\",\n",
    "#             }\n",
    "            \n",
    "#             files = {\n",
    "#                 \"file\": (batch_file_path, open(batch_file_path, \"rb\"), \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "#             }\n",
    "            \n",
    "#             # Send POST request with file upload\n",
    "#             response = requests.post(url, headers=headers, files=files)\n",
    "            \n",
    "#             print(f\"Cancellation Batch {batch_number}/{total_batches} - Response: {response.status_code}\")\n",
    "            \n",
    "#             if response.status_code in [200, 201]:\n",
    "#                 print(f\"✓ Cancellation Batch {batch_number} processed successfully\")\n",
    "#             else:\n",
    "#                 print(f\"✗ Cancellation Batch {batch_number} failed with status {response.status_code}\")\n",
    "#                 print(f\"Response: {response.text}\")\n",
    "                \n",
    "#             return response.status_code in [200, 201]\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"✗ Error processing cancellation batch {batch_number}: {str(e)}\")\n",
    "#             return False\n",
    "            \n",
    "#         finally:\n",
    "#             # Clean up temporary file\n",
    "#             if os.path.exists(batch_file_path):\n",
    "#                 os.remove(batch_file_path)\n",
    "\n",
    "#     # Calculate batch size and number of batches for cancellation\n",
    "#     CANCELLATION_BATCH_SIZE = 2000\n",
    "#     total_cancellation_rows = len(cancellation_df)\n",
    "#     total_cancellation_batches = (total_cancellation_rows + CANCELLATION_BATCH_SIZE - 1) // CANCELLATION_BATCH_SIZE  # Ceiling division\n",
    "\n",
    "#     print(f\"\\n=== Bulk Cancellation Tasks ===\")\n",
    "#     print(f\"Total cancellation rows to process: {total_cancellation_rows}\")\n",
    "#     print(f\"Cancellation batch size: {CANCELLATION_BATCH_SIZE}\")\n",
    "#     print(f\"Total cancellation batches: {total_cancellation_batches}\")\n",
    "\n",
    "#     # Process cancellation data in batches\n",
    "#     successful_cancellation_batches = 0\n",
    "#     failed_cancellation_batches = 0\n",
    "\n",
    "#     for batch_num in range(1, total_cancellation_batches + 1):\n",
    "#         start_idx = (batch_num - 1) * CANCELLATION_BATCH_SIZE\n",
    "#         end_idx = min(batch_num * CANCELLATION_BATCH_SIZE, total_cancellation_rows)\n",
    "        \n",
    "#         batch_df = cancellation_df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "#         print(f\"\\nProcessing cancellation batch {batch_num}/{total_cancellation_batches} (rows {start_idx + 1}-{end_idx})\")\n",
    "        \n",
    "#         success = process_cancellation_batch(batch_df, batch_num, total_cancellation_batches)\n",
    "        \n",
    "#         if success:\n",
    "#             successful_cancellation_batches += 1\n",
    "#         else:\n",
    "#             failed_cancellation_batches += 1\n",
    "\n",
    "#     print(f\"\\n=== Cancellation Batch Processing Complete ===\")\n",
    "#     print(f\"Successful cancellation batches: {successful_cancellation_batches}/{total_cancellation_batches}\")\n",
    "#     print(f\"Failed cancellation batches: {failed_cancellation_batches}/{total_cancellation_batches}\")\n",
    "#     print(f\"Cancellation success rate: {(successful_cancellation_batches/total_cancellation_batches)*100:.1f}%\")\n",
    "    \n",
    "#     # Store cancellation statistics\n",
    "#     cancellation_stats = {\n",
    "#         'total_batches': total_cancellation_batches,\n",
    "#         'successful_batches': successful_cancellation_batches,\n",
    "#         'failed_batches': failed_cancellation_batches,\n",
    "#         'success_rate': (successful_cancellation_batches/total_cancellation_batches)*100 if total_cancellation_batches > 0 else 0\n",
    "#     }\n",
    "\n",
    "#     # ------------------------\n",
    "#     # Bulk create Task with Batch Processing\n",
    "#     # ------------------------\n",
    "\n",
    "#     def process_batch(batch_df, batch_number, total_batches):\n",
    "#         \"\"\"\n",
    "#         Process a single batch of data for bulk task creation with retry logic for ongoing task errors\n",
    "#         \"\"\"\n",
    "#         import json\n",
    "#         import re\n",
    "        \n",
    "#         current_batch = batch_df.copy()\n",
    "#         max_retries = 10  # Maximum number of retries to prevent infinite loops\n",
    "#         retry_count = 0\n",
    "        \n",
    "#         while retry_count < max_retries and len(current_batch) > 0:\n",
    "#             # Create a temporary file for this batch\n",
    "#             batch_file_path = f\"data_for_SF_batch_{batch_number}_retry_{retry_count}.xlsx\"\n",
    "            \n",
    "#             try:\n",
    "#                 # Save batch DataFrame to an Excel file\n",
    "#                 current_batch.to_excel(batch_file_path, index=False)\n",
    "                \n",
    "#                 # Prepare the request\n",
    "#                 url = \"https://api.maxab.info/logistics/task-based/api/portal/v1/tasks/sheets/creation\"\n",
    "                \n",
    "#                 headers = {\n",
    "#                     \"accept\": \"application/json, text/plain, */*\",\n",
    "#                     \"authorization\": f\"Bearer {AT}\",\n",
    "#                     \"country_identifier\": \"EG\",\n",
    "#                     \"language\": \"AR\",\n",
    "#                     \"origin\": \"https://logistics.maxab.info\",\n",
    "#                     \"referer\": \"https://logistics.maxab.info/\",\n",
    "#                     \"user-agent\": \"Mozilla/5.0\",\n",
    "#                 }\n",
    "                \n",
    "#                 files = {\n",
    "#                     \"file\": (batch_file_path, open(batch_file_path, \"rb\"), \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "#                 }\n",
    "                \n",
    "#                 # Send POST request with file upload\n",
    "#                 response = requests.post(url, headers=headers, files=files)\n",
    "                \n",
    "#                 print(f\"Batch {batch_number}/{total_batches} (retry {retry_count}) - Response: {response.status_code}\")\n",
    "                \n",
    "#                 if response.status_code in [200, 201]:\n",
    "#                     print(f\"✓ Batch {batch_number} processed successfully after {retry_count} retries\")\n",
    "#                     return True\n",
    "#                 elif response.status_code == 400:\n",
    "#                     # Try to parse the error response\n",
    "#                     try:\n",
    "#                         error_data = json.loads(response.text)\n",
    "#                         error_message = error_data.get('message', '')\n",
    "                        \n",
    "#                         # Check if it's an ongoing task error\n",
    "#                         if 'RETAILER_HAS_ONGOING_TASK' in error_data.get('error_code', ''):\n",
    "#                             # Extract retailer ID from the error message\n",
    "#                             # Pattern: \"Retailer With ID 771554 Has Ongoing Task for the same workflow\"\n",
    "#                             match = re.search(r'Retailer With ID (\\d+) Has Ongoing Task', error_message)\n",
    "#                             if match:\n",
    "#                                 problematic_retailer_id = int(match.group(1))\n",
    "#                                 print(f\"⚠️  Removing retailer ID {problematic_retailer_id} with ongoing task\")\n",
    "                                \n",
    "#                                 # Remove the problematic retailer from the batch\n",
    "#                                 current_batch = current_batch[current_batch['Retailer ID'] != problematic_retailer_id]\n",
    "                                \n",
    "#                                 if len(current_batch) == 0:\n",
    "#                                     print(f\"✗ Batch {batch_number} failed - no valid retailers remaining\")\n",
    "#                                     return False\n",
    "                                \n",
    "#                                 retry_count += 1\n",
    "#                                 print(f\"🔄 Retrying batch {batch_number} with {len(current_batch)} remaining retailers\")\n",
    "#                                 continue\n",
    "#                             else:\n",
    "#                                 print(f\"✗ Could not extract retailer ID from error message: {error_message}\")\n",
    "#                                 return False\n",
    "#                         else:\n",
    "#                             # Other 400 error, not related to ongoing tasks\n",
    "#                             print(f\"✗ Batch {batch_number} failed with 400 error: {error_message}\")\n",
    "#                             return False\n",
    "                            \n",
    "#                     except json.JSONDecodeError:\n",
    "#                         print(f\"✗ Could not parse error response: {response.text}\")\n",
    "#                         return False\n",
    "#                 else:\n",
    "#                     # Other error status codes\n",
    "#                     print(f\"✗ Batch {batch_number} failed with status {response.status_code}\")\n",
    "#                     print(f\"Response: {response.text}\")\n",
    "#                     return False\n",
    "                    \n",
    "#             except Exception as e:\n",
    "#                 print(f\"✗ Error processing batch {batch_number}: {str(e)}\")\n",
    "#                 return False\n",
    "                \n",
    "#             finally:\n",
    "#                 # Clean up temporary file\n",
    "#                 if os.path.exists(batch_file_path):\n",
    "#                     os.remove(batch_file_path)\n",
    "        \n",
    "#         if retry_count >= max_retries:\n",
    "#             print(f\"✗ Batch {batch_number} failed after {max_retries} retries\")\n",
    "#             return False\n",
    "        \n",
    "#         return True\n",
    "\n",
    "#     # Calculate batch size and number of batches\n",
    "#     BATCH_SIZE = 500\n",
    "#     total_rows = len(creation_df)\n",
    "#     total_batches = (total_rows + BATCH_SIZE - 1) // BATCH_SIZE  # Ceiling division\n",
    "\n",
    "#     print(f\"Total rows to process: {total_rows}\")\n",
    "#     print(f\"Batch size: {BATCH_SIZE}\")\n",
    "#     print(f\"Total batches: {total_batches}\")\n",
    "\n",
    "#     # Process data in batches\n",
    "#     successful_batches = 0\n",
    "#     failed_batches = 0\n",
    "#     total_rows_created = 0  # Track actual rows created\n",
    "\n",
    "#     for batch_num in range(1, total_batches + 1):\n",
    "#         start_idx = (batch_num - 1) * BATCH_SIZE\n",
    "#         end_idx = min(batch_num * BATCH_SIZE, total_rows)\n",
    "        \n",
    "#         batch_df = creation_df.iloc[start_idx:end_idx].copy()\n",
    "#         batch_row_count = len(batch_df)  # Number of rows in this batch\n",
    "        \n",
    "#         print(f\"\\nProcessing batch {batch_num}/{total_batches} (rows {start_idx + 1}-{end_idx})\")\n",
    "        \n",
    "#         success = process_batch(batch_df, batch_num, total_batches)\n",
    "        \n",
    "#         if success:\n",
    "#             successful_batches += 1\n",
    "#             total_rows_created += batch_row_count  # Add successful batch rows to total\n",
    "#         else:\n",
    "#             failed_batches += 1\n",
    "\n",
    "#     print(f\"\\n=== Batch Processing Complete ===\")\n",
    "#     print(f\"Successful batches: {successful_batches}/{total_batches}\")\n",
    "#     print(f\"Failed batches: {failed_batches}/{total_batches}\")\n",
    "#     print(f\"Success rate: {(successful_batches/total_batches)*100:.1f}%\")\n",
    "#     print(f\"Total rows created: {total_rows_created}\")\n",
    "    \n",
    "#     # Store creation statistics\n",
    "#     creation_stats = {\n",
    "#         'total_batches': total_batches,\n",
    "#         'successful_batches': successful_batches,\n",
    "#         'failed_batches': failed_batches,\n",
    "#         'success_rate': (successful_batches/total_batches)*100 if total_batches > 0 else 0,\n",
    "#         'total_rows_created': total_rows_created\n",
    "#     }\n",
    "\n",
    "    #--------------------------\n",
    "    # Export Data\n",
    "    #--------------------------\n",
    "\n",
    "    url = \"https://api.maxab.info/logistics/task-based/api/portal/v1/tasks/export\"\n",
    "\n",
    "    # Get current date in YYYY-MM-DD format\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    querystring = {\"date_from\": current_date, \"date_to\": current_date, \"status_ids\": \"1\"}\n",
    "\n",
    "    payload = \"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {AT}\"}\n",
    "\n",
    "    response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring)\n",
    "\n",
    "    print(f\"2. Export Data Response: {response.status_code}\")\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "    #--------------------------\n",
    "    # Merge DataFrames\n",
    "    #--------------------------\n",
    "\n",
    "    # Load Excel directly into pandas\n",
    "    export_df = pd.read_excel(BytesIO(response.content))\n",
    "    export_df = export_df[export_df['Status'] == 'CREATED']\n",
    "\n",
    "    # Merge on common columns\n",
    "    merged_df = export_df.merge(\n",
    "        df[['RETAILER_ID', 'AGENT_ID']],   # only keep necessary columns\n",
    "        how='left',                        # keep all rows from export_df\n",
    "        left_on='Retailer Id',             # column name in export_df\n",
    "        right_on='RETAILER_ID'             # column name in df\n",
    "    )\n",
    "\n",
    "    # Drop duplicate 'Retailer ID' (optional, since export_df already has 'Retailer Id')\n",
    "    merged_df = merged_df.drop_duplicates(subset=['RETAILER_ID', 'Task Id'])\n",
    "    merged_df = merged_df.drop(columns=['RETAILER_ID'])\n",
    "\n",
    "    merged_df = merged_df[['Task Id', 'AGENT_ID']]\n",
    "\n",
    "    #--------------------------\n",
    "    # Bulk Dispatch Tasks with Batch Processing\n",
    "    #--------------------------\n",
    "\n",
    "    def process_dispatch_batch(batch_df, batch_number, total_batches):\n",
    "        \"\"\"\n",
    "        Process a single batch of data for bulk task dispatching with retry logic for agent not found errors\n",
    "        \"\"\"\n",
    "        import json\n",
    "        import re\n",
    "        \n",
    "        current_batch = batch_df.copy()\n",
    "        max_retries = 10  # Maximum number of retries to prevent infinite loops\n",
    "        retry_count = 0\n",
    "        \n",
    "        while retry_count < max_retries and len(current_batch) > 0:\n",
    "            # Create a temporary file for this batch\n",
    "            batch_file_path = f\"data_for_BD_batch_{batch_number}_retry_{retry_count}.xlsx\"\n",
    "            \n",
    "            try:\n",
    "                # Save batch DataFrame to an Excel file\n",
    "                current_batch.to_excel(batch_file_path, index=False)\n",
    "                \n",
    "                # Prepare the request\n",
    "                url = \"https://api.maxab.info/logistics/task-based/api/portal/v1/tasks/sheets/dispatching\"\n",
    "                \n",
    "                headers = {\n",
    "                    \"accept\": \"application/json, text/plain, */*\",\n",
    "                    \"authorization\": f\"Bearer {AT}\",\n",
    "                    \"country_identifier\": \"EG\",\n",
    "                    \"language\": \"AR\",\n",
    "                    \"origin\": \"https://logistics.maxab.info\",\n",
    "                    \"referer\": \"https://logistics.maxab.info/\",\n",
    "                    \"user-agent\": \"Mozilla/5.0\",\n",
    "                }\n",
    "                \n",
    "                files = {\n",
    "                    \"file\": (batch_file_path, open(batch_file_path, \"rb\"), \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "                }\n",
    "                \n",
    "                # Send POST request with file upload\n",
    "                response = requests.post(url, headers=headers, files=files)\n",
    "                \n",
    "                print(f\"Dispatch Batch {batch_number}/{total_batches} (retry {retry_count}) - Response: {response.status_code}\")\n",
    "                \n",
    "                if response.status_code in [200, 201]:\n",
    "                    print(f\"✓ Dispatch Batch {batch_number} processed successfully after {retry_count} retries\")\n",
    "                    return True\n",
    "                elif response.status_code == 400:\n",
    "                    # Try to parse the error response\n",
    "                    try:\n",
    "                        error_data = json.loads(response.text)\n",
    "                        error_message = error_data.get('message', '')\n",
    "                        error_code = error_data.get('error_code', '')\n",
    "                        \n",
    "                        # Check if it's an agent skill mismatch error\n",
    "                        if 'AGENT_SKILL_MISMATCHING' in error_code:\n",
    "                            # Extract task ID from the error message\n",
    "                            match = re.search(r'Row: \\d+, Task (\\d+): Agent skill is not matching with task skill', error_message)\n",
    "                            if match:\n",
    "                                problematic_task_id = int(match.group(1))\n",
    "                                print(f\"⚠️  Found AGENT_SKILL_MISMATCHING error for Task ID {problematic_task_id}\")\n",
    "                                \n",
    "                                # Find the agent_id for this task in the original merged_df\n",
    "                                task_row = merged_df[merged_df['Task Id'] == problematic_task_id]\n",
    "                                if not task_row.empty:\n",
    "                                    problematic_agent_id = task_row['AGENT_ID'].iloc[0]\n",
    "                                    print(f\"⚠️  Removing agent ID {problematic_agent_id} (associated with Task ID {problematic_task_id}) - skill mismatch\")\n",
    "                                    \n",
    "                                    # Remove all rows with this agent_id from the current batch\n",
    "                                    current_batch = current_batch[current_batch['AGENT_ID'] != problematic_agent_id]\n",
    "                                    \n",
    "                                    if len(current_batch) == 0:\n",
    "                                        print(f\"✗ Dispatch Batch {batch_number} failed - no valid agents remaining\")\n",
    "                                        return False\n",
    "                                    \n",
    "                                    retry_count += 1\n",
    "                                    print(f\"🔄 Retrying dispatch batch {batch_number} with {len(current_batch)} remaining agents\")\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    print(f\"✗ Could not find Task ID {problematic_task_id} in merged_df\")\n",
    "                                    return False\n",
    "                            else:\n",
    "                                print(f\"✗ Could not extract task ID from error message: {error_message}\")\n",
    "                                return False\n",
    "                        else:\n",
    "                            # Other 400 error, not related to agent skill mismatch\n",
    "                            print(f\"✗ Dispatch Batch {batch_number} failed with 400 error: {error_message}\")\n",
    "                            return False\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"✗ Could not parse error response: {response.text}\")\n",
    "                        return False\n",
    "                elif response.status_code == 404:\n",
    "                    # Try to parse the error response\n",
    "                    try:\n",
    "                        error_data = json.loads(response.text)\n",
    "                        error_message = error_data.get('message', '')\n",
    "                        \n",
    "                        # Check if it's an agent not found error\n",
    "                        if 'agent.NotFound' in error_data.get('error_code', ''):\n",
    "                            # Extract agent ID and row number from the error message\n",
    "                            # Pattern: \"Row: 141, Agent with ID 485 is not found\"\n",
    "                            match = re.search(r'Row: (\\d+), Agent with ID (\\d+) is not found', error_message)\n",
    "                            if match:\n",
    "                                problematic_row = int(match.group(1))\n",
    "                                problematic_agent_id = int(match.group(2))\n",
    "                                print(f\"⚠️  Removing agent ID {problematic_agent_id} (row {problematic_row}) - agent not found\")\n",
    "                                \n",
    "                                # Remove the problematic agent from the batch\n",
    "                                # Note: We need to remove by agent ID since that's what's causing the issue\n",
    "                                current_batch = current_batch[current_batch['AGENT_ID'] != problematic_agent_id]\n",
    "                                \n",
    "                                if len(current_batch) == 0:\n",
    "                                    print(f\"✗ Dispatch Batch {batch_number} failed - no valid agents remaining\")\n",
    "                                    return False\n",
    "                                \n",
    "                                retry_count += 1\n",
    "                                print(f\"🔄 Retrying dispatch batch {batch_number} with {len(current_batch)} remaining agents\")\n",
    "                                continue\n",
    "                            else:\n",
    "                                print(f\"✗ Could not extract agent ID from error message: {error_message}\")\n",
    "                                return False\n",
    "                        else:\n",
    "                            # Other 404 error, not related to agent not found\n",
    "                            print(f\"✗ Dispatch Batch {batch_number} failed with 404 error: {error_message}\")\n",
    "                            return False\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"✗ Could not parse error response: {response.text}\")\n",
    "                        return False\n",
    "                else:\n",
    "                    # Other error status codes\n",
    "                    print(f\"✗ Dispatch Batch {batch_number} failed with status {response.status_code}\")\n",
    "                    print(f\"Response: {response.text}\")\n",
    "                    return False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error processing dispatch batch {batch_number}: {str(e)}\")\n",
    "                return False\n",
    "                \n",
    "            finally:\n",
    "                # Clean up temporary file\n",
    "                if os.path.exists(batch_file_path):\n",
    "                    os.remove(batch_file_path)\n",
    "        \n",
    "        if retry_count >= max_retries:\n",
    "            print(f\"✗ Dispatch Batch {batch_number} failed after {max_retries} retries\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    # Calculate batch size and number of batches for dispatch\n",
    "    DISPATCH_BATCH_SIZE = 500\n",
    "    total_dispatch_rows = len(merged_df)\n",
    "    total_dispatch_batches = (total_dispatch_rows + DISPATCH_BATCH_SIZE - 1) // DISPATCH_BATCH_SIZE  # Ceiling division\n",
    "\n",
    "    print(f\"\\n=== Bulk Dispatch Tasks ===\")\n",
    "    print(f\"Total dispatch rows to process: {total_dispatch_rows}\")\n",
    "    print(f\"Dispatch batch size: {DISPATCH_BATCH_SIZE}\")\n",
    "    print(f\"Total dispatch batches: {total_dispatch_batches}\")\n",
    "\n",
    "    data = df[df[\"COUPON_VALUE\"].notnull()]\n",
    "    \n",
    "    coupons = data[[\"RETAILER_ID\", \"COUPON_VALUE\"]].rename(\n",
    "    columns={\n",
    "        \"RETAILER_ID\": \"retailer_id\",\n",
    "        \"COUPON_VALUE\": \"amount\"})\n",
    "\n",
    "    coupons[\"reason_id\"] = 69\n",
    "    coupons[\"type_id\"] = 4\n",
    "    coupons[\"redemption_method\"] =''\n",
    "\n",
    "    # Send in batches of max 500 rows per request\n",
    "\n",
    "    import time\n",
    "    JT = get_jwt()\n",
    "\n",
    "    url = \"https://api.maxab.info/commerce/api/admins/v1/wallet/transactions\"\n",
    "    data = {\n",
    "        \"compensation_coupon_rule_id\": \"343\",\n",
    "        \"incentive_coupon_rule_id\": \"344\"\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {JT}\"\n",
    "    }\n",
    "\n",
    "    BATCH_SIZE = 500\n",
    "    total_rows = len(coupons)\n",
    "    total_batches = (total_rows + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    print(f\"Uploading coupons in {total_batches} batch(es) of up to {BATCH_SIZE} rows...\")\n",
    "\n",
    "    for batch_index in range(total_batches):\n",
    "        start_idx = batch_index * BATCH_SIZE\n",
    "        end_idx = min((batch_index + 1) * BATCH_SIZE, total_rows)\n",
    "        batch_df = coupons.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "        buf = BytesIO()\n",
    "        with pd.ExcelWriter(buf, engine=\"xlsxwriter\") as writer:\n",
    "            batch_df.to_excel(writer, index=False)\n",
    "        buf.seek(0)\n",
    "\n",
    "        files = {\n",
    "            \"file\": (f\"coupons_batch_{batch_index + 1}_of_{total_batches}.xlsx\", buf, \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "        }\n",
    "\n",
    "        print(f\"Sending batch {batch_index + 1}/{total_batches} (rows {start_idx + 1}-{end_idx})\")\n",
    "\n",
    "        # Perform the POST request with simple 429 retry (sleep 3 minutes)\n",
    "        max_429_retries = 4\n",
    "        response = None\n",
    "        for attempt in range(1, max_429_retries + 1):\n",
    "            try:\n",
    "                if hasattr(files.get(\"file\", (None, None))[1], 'seek'):\n",
    "                    files[\"file\"][1].seek(0)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            response = requests.post(url, headers=headers, files=files, data=data)\n",
    "            if response.status_code == 429 and attempt < max_429_retries:\n",
    "                print(f\"HTTP 429 received for batch {batch_index + 1} (attempt {attempt}/{max_429_retries}). Sleeping 180 seconds before retry...\")\n",
    "                time.sleep(180)\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        print(f\"Batch {batch_index + 1}/{total_batches} response: {response.status_code}\")\n",
    "        try:\n",
    "            print(response.text)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(response.status_code)\n",
    "    print(response.text)\n",
    "\n",
    "\n",
    "\n",
    "    # Process dispatch data in batches\n",
    "    successful_dispatch_batches = 0\n",
    "    failed_dispatch_batches = 0\n",
    "    total_rows_dispatched = 0  # Track actual rows dispatched\n",
    "\n",
    "    for batch_num in range(1, total_dispatch_batches + 1):\n",
    "        start_idx = (batch_num - 1) * DISPATCH_BATCH_SIZE\n",
    "        end_idx = min(batch_num * DISPATCH_BATCH_SIZE, total_dispatch_rows)\n",
    "        \n",
    "        batch_df = merged_df.iloc[start_idx:end_idx].copy()\n",
    "        batch_row_count = len(batch_df)  # Number of rows in this batch\n",
    "        \n",
    "        print(f\"\\nProcessing dispatch batch {batch_num}/{total_dispatch_batches} (rows {start_idx + 1}-{end_idx})\")\n",
    "        \n",
    "        success = process_dispatch_batch(batch_df, batch_num, total_dispatch_batches)\n",
    "        \n",
    "        if success:\n",
    "            successful_dispatch_batches += 1\n",
    "            total_rows_dispatched += batch_row_count  # Add successful batch rows to total\n",
    "        else:\n",
    "            failed_dispatch_batches += 1\n",
    "\n",
    "    print(f\"\\n=== Dispatch Batch Processing Complete ===\")\n",
    "    print(f\"Successful dispatch batches: {successful_dispatch_batches}/{total_dispatch_batches}\")\n",
    "    print(f\"Failed dispatch batches: {failed_dispatch_batches}/{total_dispatch_batches}\")\n",
    "    print(f\"Dispatch success rate: {(successful_dispatch_batches/total_dispatch_batches)*100:.1f}%\")\n",
    "    print(f\"Total rows dispatched: {total_rows_dispatched}\")\n",
    "    \n",
    "    # Store dispatch statistics\n",
    "    dispatch_stats = {\n",
    "        'total_batches': total_dispatch_batches,\n",
    "        'successful_batches': successful_dispatch_batches,\n",
    "        'failed_batches': failed_dispatch_batches,\n",
    "        'success_rate': (successful_dispatch_batches/total_dispatch_batches)*100 if total_dispatch_batches > 0 else 0,\n",
    "        'total_rows_dispatched': total_rows_dispatched\n",
    "    }\n",
    "\n",
    "    # Check for row count mismatch between creation and dispatch\n",
    "    print(f\"\\n=== Row Count Verification ===\")\n",
    "    print(f\"Total rows created: {total_rows_created}\")\n",
    "    print(f\"Total rows dispatched: {total_rows_dispatched}\")\n",
    "    \n",
    "    if total_rows_created != total_rows_dispatched:\n",
    "        mismatch_alert = f\"\"\"\n",
    "🚨 *ROW COUNT MISMATCH ALERT* 🚨\n",
    "\n",
    "*Creation vs Dispatch Row Count Mismatch Detected!*\n",
    "\n",
    "📊 *Row Counts:*\n",
    "• Rows Created: {total_rows_created:,}  \n",
    "• Rows Dispatched: {total_rows_dispatched:,}\n",
    "• Difference: {abs(total_rows_created - total_rows_dispatched):,} rows\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"🚨 ROW COUNT MISMATCH DETECTED! 🚨\")\n",
    "        print(f\"Created: {total_rows_created}, Dispatched: {total_rows_dispatched}\")\n",
    "        print(\"Sending mismatch alert to Slack...\")\n",
    "        \n",
    "        try:\n",
    "            send_text_slack(channel='seif_error_logs', text=mismatch_alert)\n",
    "            print(\"Mismatch alert sent to Slack successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to send mismatch alert to Slack: {str(e)}\")\n",
    "    else:\n",
    "        print(\"✅ Row counts match - no data integrity issues detected\")\n",
    "\n",
    "    print(\"Egypt agent dispatching process completed successfully!\")\n",
    "    \n",
    "    # Send daily report to Slack\n",
    "    try:\n",
    "        from airflow.utils.context import Context\n",
    "        execution_date = datetime.now()  # Default to current time if not in Airflow context\n",
    "        send_daily_report_slack(cancellation_stats, creation_stats, dispatch_stats, creation_df, execution_date)\n",
    "        print(\"Daily report sent to Slack successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send daily report to Slack: {str(e)}\")\n",
    "        # Don't fail the entire task if Slack reporting fai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f95d375c-0626-4f99-a7f5-dec3b6fd4e66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Egypt Ecom on Ground Dispatching process...\n",
      "/home/ec2-user/service_account_key.json\n",
      "2. Export Data Response: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bulk Dispatch Tasks ===\n",
      "Total dispatch rows to process: 10000\n",
      "Dispatch batch size: 500\n",
      "Total dispatch batches: 20\n",
      "Uploading coupons in 2 batch(es) of up to 500 rows...\n",
      "Sending batch 1/2 (rows 1-500)\n",
      "Batch 1/2 response: 200\n",
      "PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0013\u0000\u0000\u0000[Content_Types].xml�S�n�0\u0010����*6�PU\u0015�C\u001f�\u0016��\u0003\\{�X�%����]\u00078�R�\n",
      "q�cfgfW�d�q�ZCB\u0013|��|�*�*h㻆},^�{Va�^K\u001b<4�\u00076�N\u0016XQ�ǆ�9�\u0007!P��$�\u0010�\u0013҆�d�c�D�j);\u0010��ѝP�g��E�M'O�ʕ����H7L�h���R���G��^�'�\u0003\u0007{\u0013�zސʮ\u001bB��3\u001c",
      "�\u000b",
      "˙��h.�h�W�жF�\u000ej娄CQՠ똈���}ιL�U:\u0012\u0014D�\u0013����%އ����,�B���[�\t��\u001e",
      " ;˱�\t�{N��~��X�p�\u001c",
      "ykOL�\u0004\u0018�kN�V��ܿBZ~����q\u0018��\u000f �a\u0019\u001fr��{O�\u0001PKz��q;\u0001\u0000\u0000\u001c",
      "\u0004\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u000b",
      "\u0000\u0000\u0000_rels/.rels���j�0\f",
      "�_���8�`�Q��2�m��\u00014[ILb��ږ���.[K\n",
      "\u001b�($}�\u0007�v?�I�Q.���uӂ�h���\u001bx>=��@\u0015��p�H\u0006\"�~�}�\t�n����*\"\u0016\u0003�H�׺؁\u0002��\u0013���8\u0007�Z�^'�#��7m{��O\u0006�3��\u0019�G�\u0006u�ܓ\u0018�'��y|a\u001e",
      "�����D�\t��\u000el_\u0003EYȾ�\u0000���vql\u001f3�ML�eh\u0016���*�\u0004��\\3�Y0���oJ׏�\u0003\t:\u0014���\u001f�}\u0002PK��z��\u0000\u0000\u0000I\u0002\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000docProps/app.xmlM��\n",
      "�0\u0010D�~EȽ��ADҔ��A? ��6�lB�J?ߜ���0���ͯ�)�@��׍\u0014H6���V>\u001f��$;�SC\n",
      "\u0017\u0007�GS�b��\u0014��l\u0017�&�e��L!y�%�\u0019�49��`_\u001e",
      "���4G���F\u001c",
      "��\u0005J��\u0018Wg\n",
      "~\u000e�\u0003PK�|wؑ\u0000\u0000\u0000�\u0000\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0011\u0000\u0000\u0000docProps/core.xmlm��J�0\u0010F_%依������(\u000b",
      "��+�w!\u0019�b�C\u0012����u��B.���\u001c",
      "&Sm\u000ez ��CoMM��Q�FZ՛��\u000f�mvNI��(1X�55�n�J:.�ǝ�\u000e}�1��1�KW�.F�\u0001��P��'¤��z-bz�\u0016����E(\u0019[��(��\u0002&a�\u0016#=*�\\���\u000f�@I�\u00015�\u0018��\u000b",
      "�a#z\u001d",
      "�m���<�~��q���̥�\n",
      "x������z3}]\"m���K�\"�\"I��K\u001b�N\u001e",
      "W�W�-mJV�f\u0005��z��xy��s\u0005��'�����\"-�C������\\��57�PKH�:�\u0005\u0001\u0000\u0000�\u0001\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000xl/sharedStrings.xmlm��\n",
      "\u0000\u0000\u0000xl/styles.xml���n� \u0010��>\u0005bop2TQ��P)U�RWb�6*\u001c",
      "\u0016����\u0017�ӤS�Nw�s���3ߍ֐\u000b",
      "��\u001d",
      "�t��(\u0001l��������ҝx�!N\u0006\u000e=@$ɀ��}��3c���ʰr\u0003`:i��2��w,\f",
      "\u001e",
      "��^+�Z��m�8���ZG\t�ĩx\u000fD_��\u0005�\u0002PK\u0018�\u0013�\u0000\u0000\u0000�\u0000\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "�T�\u0013�R#\u0015\u001c",
      "�voc �;c�iE��Û��E\u0010<|��4Iɣ���\u0019�F\u0005#��n���B�z�F���y�j3\u0015y��yҥ�jt>���2��Lژ�!6�\u000f2F�O\u0005Y��4@M�!\u0014���G��������1�t��y��p��\"\tn����u��\u0018���a�ΦDi�9�&#��%I�\u0006�9��}���cK�\u000e�T��$?������`J������7���o��f��M|\u0003PK�1X@C\u0001\u0000\u0000�\u0002\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u000f\u0000\u0000\u0000xl/workbook.xml���N�0\u0010D�|����I�8� ��8\u0014����4Vcod�m>\u001f'U�#��hf�L��� ��a���r\u0010�4�Ɲ\u0014|\u001e",
      "�7/��\u001e",
      "�\u001b�s�|\u0016)>�q'e�=Y\f",
      "\u0019��ӱ�\u0018��'\u0019FO؆�(�An��YZ4\u000e�\u000f���hzc}���\u001d",
      "�i��Ɔތ\u0001��e\u001f^�\u0018�x͟\u0014t8\u0004\u0002Y���e�\u0016~��\u0014����\u0011\u001b\u0005�������\n",
      "g����^�\f",
      "6�p�\u0003�U���r΀%�좃��\u001d",
      "��/\u0003�\u0003I�`|�Rˤ��:f����~\f",
      "���mF�\u000b",
      "v����\u001c",
      "�:���ׯ�������p9HB�Sy\u001d",
      "ݵK~\u0004�\u0018����\u000b",
      "PK�\u0003;��\u0000\u0000\u00003\u0002\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\u0000\u0000\u0000xl/worksheets/sheet1.xmle��N�0\u0010��<��;uҪ��$U������rw�Mb5�F����٤?J�mGߎf�N���\u0011G���Ke<��\u0000�ca\\������I.���C�\u000f5\u0000\t�w!�5Q�P*�5X\u001d",
      "&؂cR���X�J�փ.\u0006�m�4�\u001e",
      "����,)�\u0005�\u0007\n",
      "\u0007\u0000\u0000xl/workbook.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[�\u0003;��\u0000\u0000\u00003\u0002\u0000\u0000\u001a\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000xl/_rels/workbook.xml.relsPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[����$\u0001\u0000\u0000�\u0001\u0000\u0000\u0018\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000N\t\u0000\u0000xl/worksheets/sheet1.xmlPK\u0005\u0006\u0000\u0000\u0000\u0000\t\u0000\t\u0000?\u0002\u0000\u0000�\u0000\f",
      "SF[z��q;\u0001\u0000\u0000\u001c",
      "\u0004\u0000\u0000\u0013\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000[Content_Types].xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[��z��\u0000\u0000\u0000I\u0002\u0000\u0000\u000b",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000|\u0001\u0000\u0000_rels/.relsPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[�|wؑ\u0000\u0000\u0000�\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0002\u0000\u0000docProps/app.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[H�:�\u0005\u0001\u0000\u0000�\u0001\u0000\u0000\u0011\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000g\u0003\u0000\u0000docProps/core.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[\u0018�\u0013�\u0000\u0000\u0000�\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0004\u0000\u0000xl/sharedStrings.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[�1X@C\u0001\u0000\u0000�\u0002\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\n",
      "Sending batch 2/2 (rows 501-946)\n",
      "Batch 2/2 response: 200\n",
      "PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0013\u0000\u0000\u0000[Content_Types].xml�S�n�0\u0010����*6�PU\u0015�C\u001f�\u0016��\u0003\\{�X�%����]\u00078�R�\n",
      "q�cfgfW�d�q�ZCB\u0013|��|�*�*h㻆},^�{Va�^K\u001b<4�\u00076�N\u0016XQ�ǆ�9�\u0007!P��$�\u0010�\u0013҆�d�c�D�j);\u0010��ѝP�g��E�M'O�ʕ����H7L�h���R���G��^�'�\u0003\u0007{\u0013�zސʮ\u001bB��3\u001c",
      "�\u000b",
      "˙��h.�h�W�жF�\u000ej娄CQՠ똈���}ιL�U:\u0012\u0014D�\u0013����%އ����,�B���[�\t��\u001e",
      " ;˱�\t�{N��~��X�p�\u001c",
      "ykOL�\u0004\u0018�kN�V��ܿBZ~����q\u0018��\u000f �a\u0019\u001fr��{O�\u0001PKz��q;\u0001\u0000\u0000\u001c",
      "\u0004\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u000b",
      "\u0000\u0000\u0000_rels/.rels���j�0\f",
      "�_���8�`�Q��2�m��\u00014[ILb��ږ���.[K\n",
      "\u001b�($}�\u0007�v?�I�Q.���uӂ�h���\u001bx>=��@\u0015��p�H\u0006\"�~�}�\t�n����*\"\u0016\u0003�H�׺؁\u0002��\u0013���8\u0007�Z�^'�#��7m{��O\u0006�3��\u0019�G�\u0006u�ܓ\u0018�'��y|a\u001e",
      "�����D�\t��\u000el_\u0003EYȾ�\u0000���vql\u001f3�ML�eh\u0016���*�\u0004��\\3�Y0���oJ׏�\u0003\t:\u0014���\u001f�}\u0002PK��z��\u0000\u0000\u0000I\u0002\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000docProps/app.xmlM��\n",
      "�0\u0010D�~EȽ��ADҔ��A? ��6�lB�J?ߜ���0���ͯ�)�@��׍\u0014H6���V>\u001f��$;�SC\n",
      "\u0017\u0007�GS�b��\u0014��l\u0017�&�e��L!y�%�\u0019�49��`_\u001e",
      "���4G���F\u001c",
      "��\u0005J��\u0018Wg\n",
      "~\u000e�\u0003PK�|wؑ\u0000\u0000\u0000�\u0000\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0011\u0000\u0000\u0000docProps/core.xmlm��J�0\u0010F_%依������(\u000b",
      "��+�w!\u0019�b�C\u0012����u��B.���\u001c",
      "&Sm\u000ez ��CoMM��Q�FZ՛��\u000f�mvNI��(1X�55�n�J:.�ǝ�\u000e}�1��1�KW�.F�\u0001��P��'¤��z-bz�\u0016����E(\u0019[��(��\u0002&a�\u0016#=*�\\���\u000f�@I�\u00015�\u0018��\u000b",
      "�a#z\u001d",
      "�m���<�~��q���̥�\n",
      "x������z3}]\"m���K�\"�\"I��K\u001b�N\u001e",
      "W�W�-mJV�f\u0005��z��xy��s\u0005��'�����\"-�C������\\��57�PKH�:�\u0005\u0001\u0000\u0000�\u0001\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000xl/sharedStrings.xmlm��\n",
      "\u0000\u0000\u0000xl/styles.xml���n� \u0010��>\u0005bop2TQ��P)U�RWb�6*\u001c",
      "\u0016����\u0017�ӤS�Nw�s���3ߍ֐\u000b",
      "��\u001d",
      "�t��(\u0001l��������ҝx�!N\u0006\u000e=@$ɀ��}��3c���ʰr\u0003`:i��2��w,\f",
      "\u001e",
      "��^+�Z��m�8���ZG\t�ĩx\u000fD_��\u0005�\u0002PK\u0018�\u0013�\u0000\u0000\u0000�\u0000\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "�T�\u0013�R#\u0015\u001c",
      "�voc �;c�iE��Û��E\u0010<|��4Iɣ���\u0019�F\u0005#��n���B�z�F���y�j3\u0015y��yҥ�jt>���2��Lژ�!6�\u000f2F�O\u0005Y��4@M�!\u0014���G��������1�t��y��p��\"\tn����u��\u0018���a�ΦDi�9�&#��%I�\u0006�9��}���cK�\u000e�T��$?������`J������7���o��f��M|\u0003PK�1X@C\u0001\u0000\u0000�\u0002\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u000f\u0000\u0000\u0000xl/workbook.xml���n�0\u0010D��\n",
      "g����^�\f",
      "6�p�\u0003�U���r΀%�좃��\u001d",
      "��/\u0003�\u0003I�`|�Rˤ��:f����~\f",
      "���mF�\u000b",
      "v����\u001c",
      "�:���ׯ�������p9HB�Sy\u001d",
      "ݵK~\u0004�\u0018����\u000b",
      "PK�\u0003;��\u0000\u0000\u00003\u0002\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\u0000\u0000\u0000xl/worksheets/sheet1.xmle��N�0\u0010��<��;uҪ��$U������rw�Mb5�F����٤?J�mGߎf�N���\u0011G���Ke<��\u0000�ca\\������I.���C�\u000f5\u0000\t�w!�5Q�P*�5X\u001d",
      "&؂cR���X�J�փ.\u0006�m�4�\u001e",
      "����,)�\u0005�\u0007\n",
      "\u0007\u0000\u0000xl/workbook.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[�\u0003;��\u0000\u0000\u00003\u0002\u0000\u0000\u001a\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000xl/_rels/workbook.xml.relsPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[����$\u0001\u0000\u0000�\u0001\u0000\u0000\u0018\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000N\t\u0000\u0000xl/worksheets/sheet1.xmlPK\u0005\u0006\u0000\u0000\u0000\u0000\t\u0000\t\u0000?\u0002\u0000\u0000�\u0000\f",
      "SF[z��q;\u0001\u0000\u0000\u001c",
      "\u0004\u0000\u0000\u0013\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000[Content_Types].xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[��z��\u0000\u0000\u0000I\u0002\u0000\u0000\u000b",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000|\u0001\u0000\u0000_rels/.relsPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[�|wؑ\u0000\u0000\u0000�\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0002\u0000\u0000docProps/app.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[H�:�\u0005\u0001\u0000\u0000�\u0001\u0000\u0000\u0011\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000g\u0003\u0000\u0000docProps/core.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[\u0018�\u0013�\u0000\u0000\u0000�\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0004\u0000\u0000xl/sharedStrings.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[�1X@C\u0001\u0000\u0000�\u0002\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\n",
      "200\n",
      "PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0013\u0000\u0000\u0000[Content_Types].xml�S�n�0\u0010����*6�PU\u0015�C\u001f�\u0016��\u0003\\{�X�%����]\u00078�R�\n",
      "q�cfgfW�d�q�ZCB\u0013|��|�*�*h㻆},^�{Va�^K\u001b<4�\u00076�N\u0016XQ�ǆ�9�\u0007!P��$�\u0010�\u0013҆�d�c�D�j);\u0010��ѝP�g��E�M'O�ʕ����H7L�h���R���G��^�'�\u0003\u0007{\u0013�zސʮ\u001bB��3\u001c",
      "�\u000b",
      "˙��h.�h�W�жF�\u000ej娄CQՠ똈���}ιL�U:\u0012\u0014D�\u0013����%އ����,�B���[�\t��\u001e",
      " ;˱�\t�{N��~��X�p�\u001c",
      "ykOL�\u0004\u0018�kN�V��ܿBZ~����q\u0018��\u000f �a\u0019\u001fr��{O�\u0001PKz��q;\u0001\u0000\u0000\u001c",
      "\u0004\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u000b",
      "\u0000\u0000\u0000_rels/.rels���j�0\f",
      "�_���8�`�Q��2�m��\u00014[ILb��ږ���.[K\n",
      "\u001b�($}�\u0007�v?�I�Q.���uӂ�h���\u001bx>=��@\u0015��p�H\u0006\"�~�}�\t�n����*\"\u0016\u0003�H�׺؁\u0002��\u0013���8\u0007�Z�^'�#��7m{��O\u0006�3��\u0019�G�\u0006u�ܓ\u0018�'��y|a\u001e",
      "�����D�\t��\u000el_\u0003EYȾ�\u0000���vql\u001f3�ML�eh\u0016���*�\u0004��\\3�Y0���oJ׏�\u0003\t:\u0014���\u001f�}\u0002PK��z��\u0000\u0000\u0000I\u0002\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000docProps/app.xmlM��\n",
      "�0\u0010D�~EȽ��ADҔ��A? ��6�lB�J?ߜ���0���ͯ�)�@��׍\u0014H6���V>\u001f��$;�SC\n",
      "\u0017\u0007�GS�b��\u0014��l\u0017�&�e��L!y�%�\u0019�49��`_\u001e",
      "���4G���F\u001c",
      "��\u0005J��\u0018Wg\n",
      "~\u000e�\u0003PK�|wؑ\u0000\u0000\u0000�\u0000\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0011\u0000\u0000\u0000docProps/core.xmlm��J�0\u0010F_%依������(\u000b",
      "��+�w!\u0019�b�C\u0012����u��B.���\u001c",
      "&Sm\u000ez ��CoMM��Q�FZ՛��\u000f�mvNI��(1X�55�n�J:.�ǝ�\u000e}�1��1�KW�.F�\u0001��P��'¤��z-bz�\u0016����E(\u0019[��(��\u0002&a�\u0016#=*�\\���\u000f�@I�\u00015�\u0018��\u000b",
      "�a#z\u001d",
      "�m���<�~��q���̥�\n",
      "x������z3}]\"m���K�\"�\"I��K\u001b�N\u001e",
      "W�W�-mJV�f\u0005��z��xy��s\u0005��'�����\"-�C������\\��57�PKH�:�\u0005\u0001\u0000\u0000�\u0001\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000xl/sharedStrings.xmlm��\n",
      "\u0000\u0000\u0000xl/styles.xml���n� \u0010��>\u0005bop2TQ��P)U�RWb�6*\u001c",
      "\u0016����\u0017�ӤS�Nw�s���3ߍ֐\u000b",
      "��\u001d",
      "�t��(\u0001l��������ҝx�!N\u0006\u000e=@$ɀ��}��3c���ʰr\u0003`:i��2��w,\f",
      "\u001e",
      "��^+�Z��m�8���ZG\t�ĩx\u000fD_��\u0005�\u0002PK\u0018�\u0013�\u0000\u0000\u0000�\u0000\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "�T�\u0013�R#\u0015\u001c",
      "�voc �;c�iE��Û��E\u0010<|��4Iɣ���\u0019�F\u0005#��n���B�z�F���y�j3\u0015y��yҥ�jt>���2��Lژ�!6�\u000f2F�O\u0005Y��4@M�!\u0014���G��������1�t��y��p��\"\tn����u��\u0018���a�ΦDi�9�&#��%I�\u0006�9��}���cK�\u000e�T��$?������`J������7���o��f��M|\u0003PK�1X@C\u0001\u0000\u0000�\u0002\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u000f\u0000\u0000\u0000xl/workbook.xml���n�0\u0010D��\n",
      "g����^�\f",
      "6�p�\u0003�U���r΀%�좃��\u001d",
      "��/\u0003�\u0003I�`|�Rˤ��:f����~\f",
      "���mF�\u000b",
      "v����\u001c",
      "�:���ׯ�������p9HB�Sy\u001d",
      "ݵK~\u0004�\u0018����\u000b",
      "PK�\u0003;��\u0000\u0000\u00003\u0002\u0000\u0000PK\u0003\u0004\u0014\u0000\f",
      "SF[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\u0000\u0000\u0000xl/worksheets/sheet1.xmle��N�0\u0010��<��;uҪ��$U������rw�Mb5�F����٤?J�mGߎf�N���\u0011G���Ke<��\u0000�ca\\������I.���C�\u000f5\u0000\t�w!�5Q�P*�5X\u001d",
      "&؂cR���X�J�փ.\u0006�m�4�\u001e",
      "����,)�\u0005�\u0007\n",
      "\u0007\u0000\u0000xl/workbook.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[�\u0003;��\u0000\u0000\u00003\u0002\u0000\u0000\u001a\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000xl/_rels/workbook.xml.relsPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[����$\u0001\u0000\u0000�\u0001\u0000\u0000\u0018\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000N\t\u0000\u0000xl/worksheets/sheet1.xmlPK\u0005\u0006\u0000\u0000\u0000\u0000\t\u0000\t\u0000?\u0002\u0000\u0000�\u0000\f",
      "SF[z��q;\u0001\u0000\u0000\u001c",
      "\u0004\u0000\u0000\u0013\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000[Content_Types].xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[��z��\u0000\u0000\u0000I\u0002\u0000\u0000\u000b",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000|\u0001\u0000\u0000_rels/.relsPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[�|wؑ\u0000\u0000\u0000�\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0002\u0000\u0000docProps/app.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[H�:�\u0005\u0001\u0000\u0000�\u0001\u0000\u0000\u0011\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000g\u0003\u0000\u0000docProps/core.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[\u0018�\u0013�\u0000\u0000\u0000�\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0004\u0000\u0000xl/sharedStrings.xmlPK\u0001\u0002\u0014\u0000\u0014\u0000\f",
      "SF[�1X@C\u0001\u0000\u0000�\u0002\u0000\u0000\n",
      "\u0000\u0000\u0000\u0000\n",
      "\n",
      "Processing dispatch batch 1/20 (rows 1-500)\n",
      "Dispatch Batch 1/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 1 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 2/20 (rows 501-1000)\n",
      "Dispatch Batch 2/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 2 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 3/20 (rows 1001-1500)\n",
      "Dispatch Batch 3/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 3 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 4/20 (rows 1501-2000)\n",
      "Dispatch Batch 4/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 4 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 5/20 (rows 2001-2500)\n",
      "Dispatch Batch 5/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 5 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 6/20 (rows 2501-3000)\n",
      "Dispatch Batch 6/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 6 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 7/20 (rows 3001-3500)\n",
      "Dispatch Batch 7/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 7 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 8/20 (rows 3501-4000)\n",
      "Dispatch Batch 8/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 8 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 9/20 (rows 4001-4500)\n",
      "Dispatch Batch 9/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 9 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 10/20 (rows 4501-5000)\n",
      "Dispatch Batch 10/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 10 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 11/20 (rows 5001-5500)\n",
      "Dispatch Batch 11/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 11 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 12/20 (rows 5501-6000)\n",
      "Dispatch Batch 12/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 12 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 13/20 (rows 6001-6500)\n",
      "Dispatch Batch 13/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 13 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 14/20 (rows 6501-7000)\n",
      "Dispatch Batch 14/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 14 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 15/20 (rows 7001-7500)\n",
      "Dispatch Batch 15/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 15 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 16/20 (rows 7501-8000)\n",
      "Dispatch Batch 16/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 16 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 17/20 (rows 8001-8500)\n",
      "Dispatch Batch 17/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 17 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 18/20 (rows 8501-9000)\n",
      "Dispatch Batch 18/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 18 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 19/20 (rows 9001-9500)\n",
      "Dispatch Batch 19/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 19 processed successfully after 0 retries\n",
      "\n",
      "Processing dispatch batch 20/20 (rows 9501-10000)\n",
      "Dispatch Batch 20/20 (retry 0) - Response: 200\n",
      "✓ Dispatch Batch 20 processed successfully after 0 retries\n",
      "\n",
      "=== Dispatch Batch Processing Complete ===\n",
      "Successful dispatch batches: 20/20\n",
      "Failed dispatch batches: 0/20\n",
      "Dispatch success rate: 100.0%\n",
      "Total rows dispatched: 10000\n",
      "\n",
      "=== Row Count Verification ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'total_rows_created' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mEG_Ecom_on_ground_dispatching\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 818\u001b[0m, in \u001b[0;36mEG_Ecom_on_ground_dispatching\u001b[0;34m()\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Check for row count mismatch between creation and dispatch\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Row Count Verification ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 818\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal rows created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtotal_rows_created\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal rows dispatched: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_rows_dispatched\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_rows_created \u001b[38;5;241m!=\u001b[39m total_rows_dispatched:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_rows_created' is not defined"
     ]
    }
   ],
   "source": [
    "EG_Ecom_on_ground_dispatching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833bb27b-bdd1-4243-90ce-960957553559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
