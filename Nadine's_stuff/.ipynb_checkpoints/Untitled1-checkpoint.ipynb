{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2116524e-9243-4587-9216-85b9c68ce270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from common_functions import google_sheets, task_fail_slack_alert, get_secret, snowflake_query\n",
    "from datetime import datetime, timedelta\n",
    "import datetime as dt\n",
    "\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def initialize_env():\n",
    "    snowflake_sg_secret = json.loads(get_secret(\"Snowflake-sagemaker\"))\n",
    "    slack_secret = json.loads(get_secret(\"prod/slack/reports\"))\n",
    "    fintech_service_account = json.loads(get_secret(\"prod/fintechServiceEmail/credentials\"))\n",
    "    dwh_writer_secret = json.loads(get_secret(\"prod/db/datawarehouse/sagemaker\"))\n",
    "\n",
    "    os.environ[\"SNOWFLAKE_USERNAME\"] = snowflake_sg_secret[\"username\"]\n",
    "    os.environ[\"SNOWFLAKE_PASSWORD\"] = snowflake_sg_secret[\"password\"]\n",
    "    os.environ[\"SNOWFLAKE_ACCOUNT\"] = snowflake_sg_secret[\"account\"]\n",
    "    os.environ[\"SNOWFLAKE_DATABASE\"] = snowflake_sg_secret[\"database\"]\n",
    "\n",
    "    os.environ[\"SLACK_TOKEN\"] = slack_secret[\"token\"]\n",
    "\n",
    "    os.environ[\"FINTECH_EMONEY_EMAIL\"] = fintech_service_account[\"email_name\"]\n",
    "    os.environ[\"FINTECH_EMONEY_PASSWORD\"] = fintech_service_account[\"email_password\"]\n",
    "\n",
    "    metabase_secret = json.loads(get_secret(\"prod/metabase/maxab_config\"))\n",
    "    os.environ[\"EGYPT_METABASE_USERNAME\"] = metabase_secret[\"metabase_user\"]\n",
    "    os.environ[\"EGYPT_METABASE_PASSWORD\"] = metabase_secret[\"metabase_password\"]\n",
    "\n",
    "    os.environ[\"DWH_WRITER_HOST_NEW\"] = dwh_writer_secret[\"host\"]\n",
    "    os.environ[\"DWH_WRITER_NAME_NEW\"] = dwh_writer_secret[\"dbname\"]\n",
    "    os.environ[\"DWH_WRITER_USER_NAME_NEW\"] = dwh_writer_secret[\"username\"]\n",
    "    os.environ[\"DWH_WRITER_PASSWORD_NEW\"] = dwh_writer_secret[\"password\"] \n",
    "\n",
    "    json_path_sheets = str(Path.home()) + \"/service_account_key_sheets.json\"\n",
    "    sheets_key = get_secret(\"prod/maxab-sheets\")\n",
    "    f = open(json_path_sheets, \"w\")\n",
    "    f.write(sheets_key)\n",
    "    f.close()\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"] = json_path_sheets\n",
    "\n",
    "def ret_metabase(country, question, filters={}, initialized = False):\n",
    "\n",
    "    \n",
    "    if not initialized: \n",
    "        initialize_env()\n",
    "    \n",
    "    question_id = str(question)\n",
    "    \n",
    "    if country.lower() == 'egypt':\n",
    "        base_url = 'https://bi.maxab.info/api'\n",
    "        username = str(os.environ[\"EGYPT_METABASE_USERNAME\"])\n",
    "        password = str(os.environ[\"EGYPT_METABASE_PASSWORD\"])\n",
    "    else:\n",
    "        base_url = 'https://bi.maxabma.com/api'\n",
    "        username = str(os.environ[\"AFRICA_METABASE_USERNAME\"])\n",
    "        password = str(os.environ[\"AFRICA_METABASE_PASSWORD\"])\n",
    "\n",
    "    base_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    try:\n",
    "        s_response = requests.post(\n",
    "            base_url + '/session',\n",
    "            data=json.dumps({\n",
    "                'username': username,\n",
    "                'password': password\n",
    "            }),\n",
    "            headers=base_headers)\n",
    "        \n",
    "        s_response.raise_for_status()\n",
    "\n",
    "        session_token = s_response.json()['id']\n",
    "        base_headers['X-Metabase-Session'] = session_token\n",
    "        \n",
    "        params = []\n",
    "        \n",
    "        for name, value in filters.items():\n",
    "            filter_type, filter_value = value\n",
    "            param = {'target': ['variable', ['template-tag', name]], 'value': filter_value}\n",
    "            \n",
    "            if filter_type.lower() == 'date':\n",
    "                param['type'] = 'date/range' if isinstance(filter_value, list) else 'date/single'\n",
    "            elif filter_type.lower() == 'category':\n",
    "                param['type'] = 'category'\n",
    "            elif filter_type.lower() == 'text':\n",
    "                param['type'] = 'text'\n",
    "            elif filter_type.lower() == 'number':\n",
    "                param['type'] = 'number'\n",
    "            elif filter_type.lower() == 'field list':\n",
    "                param['type'] = 'id'\n",
    "                param['target'] = ['dimension', ['template-tag', name]]\n",
    "            \n",
    "            params.append(param)\n",
    "\n",
    "        p_response = requests.post(base_url + '/card/' + question_id + '/query/csv', \n",
    "                                   json={'parameters': params}, \n",
    "                                   headers=base_headers)\n",
    "        p_response.raise_for_status()\n",
    "\n",
    "        my_dict = p_response.content\n",
    "        s = str(my_dict, 'utf-8')\n",
    "        my_dict = StringIO(s)\n",
    "        df = pd.read_csv(my_dict)\n",
    "        return(df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def check_distribution(df, agent_list):\n",
    "    \"\"\"\n",
    "    Checks if the distribution of project types across agents is even.\n",
    "    Returns a dictionary with the count of rows per agent for each project type.\n",
    "    \"\"\"\n",
    "    distribution = {}\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        distribution[project] = project_df['agent_assigned'].value_counts().reindex(agent_list, fill_value=0)\n",
    "    \n",
    "    return distribution\n",
    "\n",
    "def redistribute_rows(df, agent_list):\n",
    "    \"\"\"\n",
    "    Redistributes rows among agents if the distribution is uneven.\n",
    "    \"\"\"\n",
    "    project_types = df['project_name'].unique()\n",
    "    redistributed_data = pd.DataFrame()\n",
    "    print(\"Redistributing rows...\")\n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.sample(frac=1).reset_index(drop=True)  # Shuffle data\n",
    "        rows_per_agent = len(project_df) // len(agent_list)\n",
    "        remainder = len(project_df) % len(agent_list)\n",
    "        \n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, agent in enumerate(agent_list):\n",
    "            end_idx = start_idx + rows_per_agent + (1 if i < remainder else 0)\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            redistributed_data = pd.concat([redistributed_data, agent_data])\n",
    "            \n",
    "            start_idx = end_idx\n",
    "    \n",
    "    redistributed_data = redistributed_data.reset_index(drop=True)\n",
    "    \n",
    "    return redistributed_data\n",
    "\n",
    "def ensure_correct_dispatching(df, agent_list, final_old_assign):\n",
    "    \"\"\"\n",
    "    Ensures the dispatching is done correctly by checking and redistributing rows if necessary.\n",
    "    `final_old_assign` rows are excluded from redistribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Exclude final_old_assign from the distribution check, but keep rows with main_system_id == 1\n",
    "    new_assignments = df[~df['main_system_id'].isin(final_old_assign['main_system_id']) | (df['main_system_id'] == 1)]\n",
    "    \n",
    "    distribution = check_distribution(new_assignments, agent_list)\n",
    "    \n",
    "    # Check if any project has an uneven distribution across agents\n",
    "    uneven_distribution = any(distribution[project].nunique() > 1 for project in distribution)\n",
    "    \n",
    "    if uneven_distribution:\n",
    "        print(\"Uneven distribution detected.\")\n",
    "        new_assignments = redistribute_rows(new_assignments, agent_list)\n",
    "    else:\n",
    "        print(\"Distribution is even. No redistribution needed.\")\n",
    "    \n",
    "    # Combine the redistributed new assignments with the final_old_assign\n",
    "    final_data = pd.concat([final_old_assign, new_assignments], ignore_index=True)\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def check_distribution_df(df, agent_list):\n",
    "    \"\"\"\n",
    "    Checks if the distribution of project types across agents is even.\n",
    "    Returns a DataFrame with the count of rows per agent for each project type.\n",
    "    \"\"\"\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    distribution_data = []\n",
    "    current_time = datetime.now()  # Get the current datetime\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        agent_counts = project_df['agent_assigned'].value_counts().reindex(agent_list, fill_value=0)\n",
    "        \n",
    "        for agent, count in agent_counts.items():\n",
    "            distribution_data.append({\n",
    "                'project_type': project,\n",
    "                'agent_assigned': agent,\n",
    "                'count': count,\n",
    "                'datetime': dt.datetime.now()  + timedelta(hours=3)\n",
    "            })\n",
    "    \n",
    "    distribution_df = pd.DataFrame(distribution_data)\n",
    "    \n",
    "    return distribution_df\n",
    "\n",
    "def clean_column_id(df, column_name):\n",
    "    # Ensure the column is treated as a string\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    \n",
    "    # Replace commas in the string\n",
    "    df[column_name] = df[column_name].str.replace(',', '')\n",
    "    \n",
    "    # Convert back to an integer, if appropriate\n",
    "    df[column_name] = df[column_name].astype('Int64', errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ----------------------------------------\n",
    "# Random distribution (Equal assigning)   \n",
    "# ----------------------------------------\n",
    "def assign_data_equal_projects(df, list):\n",
    "    df = df.sample(frac=1)  # Shuffle the data\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    assigned_data = pd.DataFrame()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.reset_index(drop=True)\n",
    "        rows_per_agent = len(project_df) // len(list)\n",
    "        remainder = len(project_df) % len(list)\n",
    "        \n",
    "        # Distribute rows equally\n",
    "        for i, agent in enumerate(list):\n",
    "            start_idx = i * rows_per_agent\n",
    "            end_idx = start_idx + rows_per_agent\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            # Handle remainder\n",
    "            if i < remainder:\n",
    "                extra_row = project_df.iloc[end_idx:end_idx+1].copy()\n",
    "                extra_row['agent_assigned'] = agent\n",
    "                agent_data = pd.concat([agent_data, extra_row])\n",
    "            \n",
    "            assigned_data = pd.concat([assigned_data, agent_data])\n",
    "    \n",
    "    assigned_data = assigned_data.reset_index(drop=True)\n",
    "    \n",
    "    return assigned_data\n",
    "\n",
    "# ----------------------------------------\n",
    "# Mapping distribution (Segment-based)\n",
    "# ----------------------------------------\n",
    "def assign_data_by_mapping(df, mapping_df):\n",
    "    # store retail-agent mapping in a dictionary \n",
    "    mapping_dict = dict(zip(mapping_df['main_system_id'], mapping_df['agent_assigned']))\n",
    "        \n",
    "    assigned_agents = []\n",
    "\n",
    "    for retailer_id in df['main_system_id']:\n",
    "        if retailer_id in mapping_dict:\n",
    "            assigned_agents.append(mapping_dict[retailer_id])\n",
    "        else:\n",
    "            assigned_agents.append(None)\n",
    "   \n",
    "    df['agent_assigned'] = assigned_agents\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_available_agents(attendance_df, current_hour):\n",
    "    \"\"\"Get list of available task-based agents for the current hour.\"\"\"\n",
    "    attendance_copy = attendance_df.copy()\n",
    "    \n",
    "    attendance_copy['start_time'] = attendance_copy['start_time'].astype(int)\n",
    "    attendance_copy['end_time'] = attendance_copy['end_time'].astype(int)\n",
    "    \n",
    "    attendance_copy['assignment_start_time'] = attendance_copy['start_time'] - 1\n",
    "    attendance_copy['assignment_end_time'] = attendance_copy['end_time'] - 1\n",
    "    \n",
    "    attendance_copy['assign_data'] = np.where(\n",
    "        (current_hour >= attendance_copy['assignment_start_time']) & \n",
    "        (current_hour <= attendance_copy['assignment_end_time']),\n",
    "        'yes', 'no')\n",
    "    \n",
    "    task_based_agents = attendance_copy.loc[\n",
    "        (attendance_copy['project'] == 'task_based') & \n",
    "        (attendance_copy['assign_data'] == 'yes')]\n",
    "    \n",
    "    task_based_list = task_based_agents['agent_id'].values.tolist()\n",
    "    print(f\"Number of available agents: {len(task_based_list)}\")\n",
    "    return task_based_list\n",
    "\n",
    "def fetch_and_process_queries(query_ids, blacklisted_retailers):\n",
    "    \"\"\"Fetch and process data from queries, removing blacklisted retailers.\"\"\"\n",
    "    queries_for_random = query_ids['Query_id_random'].dropna().astype(int).tolist()\n",
    "    queries_for_mapped = query_ids['Query_id_segment_based'].dropna().astype(int).tolist()\n",
    "    print(f\"Fetching data from {len(queries_for_random)+len(queries_for_mapped)} queries...\")\n",
    "    \n",
    "    # Process random queries\n",
    "    dataframes_R = [ret_metabase(\"EGYPT\", query, initialized=True) for query in queries_for_random]\n",
    "    empty_queries = []\n",
    "    for i, df in enumerate(dataframes_R):\n",
    "        if df.empty:\n",
    "            empty_queries.append(queries_for_random[i])\n",
    "        else:\n",
    "            print(f\"Query {queries_for_random[i]} returned {len(df)} records\")\n",
    "        df.columns = map(str.lower, df.columns)\n",
    "    if empty_queries:\n",
    "        print(f\"WARNING: Queries {empty_queries} returned empty dataframe!\")\n",
    "    # ----------------------------------------\n",
    "    # write in google sheet available data\n",
    "    # ----------------------------------------\n",
    "    # check for empty queries in Random tasks\n",
    "    for idx, row in query_ids.iterrows():\n",
    "        query_id = row['Query_id_random']\n",
    "        if pd.notna(query_id):\n",
    "            df = ret_metabase(\"EGYPT\", int(query_id), initialized=True)\n",
    "            query_ids.at[idx, 'Available_data'] = 'Empty' if df.empty else str(len(df))\n",
    "\n",
    "    # Step 3: Overwrite the sheet with updated full data (including manually-entered columns like F)\n",
    "    google_sheets('Query ID Assigning', 'Sheet1', 'overwrite', df=query_ids)\n",
    "    \n",
    "    # check for empty queries in Segment-based\n",
    "    for idx, row in query_ids.iterrows():\n",
    "        query_id = row['Query_id_segment_based']\n",
    "        if pd.notna(query_id):\n",
    "            df = ret_metabase(\"EGYPT\", int(query_id), initialized=True)\n",
    "            query_ids.at[idx, 'Available_data'] = 'Empty' if df.empty else str(len(df))\n",
    "\n",
    "    # Step 3: Overwrite the sheet with updated full data (including manually-entered columns like F)\n",
    "    google_sheets('Query ID Assigning', 'Sheet1', 'overwrite', df=query_ids)\n",
    "    \n",
    "    # Process mapped queries\n",
    "    dataframes_M = [ret_metabase(\"EGYPT\", query, initialized=True) for query in queries_for_mapped]\n",
    "    for i, df in enumerate(dataframes_M):\n",
    "        if df.empty:\n",
    "            print(f\"WARNING: Query {queries_for_mapped[i]} returned empty dataframe!\")\n",
    "        else:\n",
    "            print(f\"Query {queries_for_mapped[i]} returned {len(df)} records\")\n",
    "        df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "    # Combine and clean dataframes\n",
    "    df_unfiltered_R = pd.concat(dataframes_R, ignore_index=True)\n",
    "    df_unfiltered_M = pd.concat(dataframes_M, ignore_index=True)\n",
    "    print(f\"Mapping tasks available: {df_unfiltered_M.shape[0]}\")\n",
    "    print(f\"Random tasks available: {df_unfiltered_R.shape[0]}\")\n",
    "    \n",
    "    # Remove blacklisted retailers\n",
    "    df_raw_R = df_unfiltered_R[~df_unfiltered_R['main_system_id'].isin(blacklisted_retailers)]\n",
    "    df_raw_M = df_unfiltered_M[~df_unfiltered_M['main_system_id'].isin(blacklisted_retailers)]\n",
    "    \n",
    "    df_raw_R = clean_column_id(df_raw_R, 'main_system_id')\n",
    "    df_raw_M = clean_column_id(df_raw_M, 'main_system_id')\n",
    "    print(f\"Removed {(len(df_unfiltered_R)+len(df_unfiltered_M)) - (len(df_raw_R)+len(df_raw_M))} blacklisted retailers\")\n",
    "    \n",
    "    return df_raw_R, df_raw_M\n",
    "\n",
    "def process_previous_assignments(df_raw_R, previous_calls):\n",
    "    \"\"\"Process previous assignments and separate new and old assignments.\"\"\"\n",
    "    exclude_same_assigns = clean_column_id(previous_calls, 'main_system_id')\n",
    "    \n",
    "    # Get new assignments\n",
    "    task_based = pd.DataFrame(df_raw_R.loc[~df_raw_R['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "    \n",
    "    # Get old assignments\n",
    "    old_assgns = pd.DataFrame(df_raw_R.loc[df_raw_R['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "    df_1 = old_assgns.merge(previous_calls, on='main_system_id', how='left')\n",
    "    final_old_assign = df_1[[\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"project_name\"]]\n",
    "    \n",
    "    return task_based, final_old_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a27cc36-6803-4ebc-9da1-214d5765bfed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_assign(previously_assigned, df, assigns, print_it = False):\n",
    "    print(\"Starting remove_assign function...\")\n",
    "\n",
    "    try:\n",
    "        if not previously_assigned.empty:\n",
    "            previously_assigned[0] = previously_assigned[0].fillna('').astype(str).str.replace(\" \", \"\", regex=False)\n",
    "            previously_assigned = previously_assigned.dropna()\n",
    "            previously_assigned[0] = previously_assigned[0].astype('float').astype('int')\n",
    "            df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "            if 'agent_assigned' in df.columns:\n",
    "                if df['agent_assigned'].dtype == 'object':\n",
    "                    df.loc[df['agent_assigned'].notna(), 'agent_assigned'] = df.loc[df['agent_assigned'].notna(), 'agent_assigned'].astype(int)\n",
    "\n",
    "                    print(\"Converted agent_assigned column to int.\")\n",
    "\n",
    "            filtered_ids = previously_assigned[0].astype(int).values\n",
    "            main_data_to_assign = df.loc[~df['main_system_id'].isin(filtered_ids) | (df['main_system_id'] == 1)\n",
    "            ]\n",
    "            if print_it:\n",
    "                print(f\"Available tasks for this batch after filtering. Rows remaining: {len(main_data_to_assign)}\")\n",
    "                print(f\" Retailers that have no aggents to assign to {main_data_to_assign['agent_assigned'].isna().sum()}\")\n",
    "\n",
    "            main_data_to_assign['main_system_id'] = main_data_to_assign['main_system_id'].astype('int')\n",
    "            main_data_to_assign = main_data_to_assign.groupby('agent_assigned').head(assigns)\n",
    "\n",
    "            return main_data_to_assign\n",
    "\n",
    "        else:\n",
    "            print(\"Only one previously assigned entry, skipping filter.\")\n",
    "            df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "            main_data_to_assign = df.groupby('agent_assigned').head(assigns)\n",
    "            return main_data_to_assign\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] remove_assign failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7677631-809f-4a3b-8502-6e68c0cb9c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process at hour: 17\n",
      "Totale agents: 6\n",
      "Number of available agents: 6\n",
      "/home/ec2-user/service_account_key.json\n",
      "Fetching data from 18 queries...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 133\u001b[0m\n\u001b[1;32m    131\u001b[0m     sheet_df \u001b[38;5;241m=\u001b[39m final_data_to_assign[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_system_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretailer_mobile_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretailer_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_assigned\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded_at\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    132\u001b[0m     sheet_df\n\u001b[0;32m--> 133\u001b[0m \u001b[43mtask_based_assignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mtask_based_assignment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m blacklisted_retailers \u001b[38;5;241m=\u001b[39m query_ids[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlacklisted_retailers\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Fetch and process query data\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m df_raw_R, df_raw_M \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_and_process_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblacklisted_retailers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Process previous assignments / For follow up calls\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[1;32m     36\u001b[0m previous_calls \u001b[38;5;241m=\u001b[39m ret_metabase(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEGYPT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m35299\u001b[39m, initialized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 315\u001b[0m, in \u001b[0;36mfetch_and_process_queries\u001b[0;34m(query_ids, blacklisted_retailers)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(queries_for_random)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(queries_for_mapped)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m queries...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# Process random queries\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m dataframes_R \u001b[38;5;241m=\u001b[39m [ret_metabase(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEGYPT\u001b[39m\u001b[38;5;124m\"\u001b[39m, query, initialized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries_for_random]\n\u001b[1;32m    316\u001b[0m empty_queries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataframes_R):\n",
      "Cell \u001b[0;32mIn[1], line 315\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(queries_for_random)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(queries_for_mapped)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m queries...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# Process random queries\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m dataframes_R \u001b[38;5;241m=\u001b[39m [\u001b[43mret_metabase\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEGYPT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitialized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries_for_random]\n\u001b[1;32m    316\u001b[0m empty_queries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataframes_R):\n",
      "Cell \u001b[0;32mIn[1], line 109\u001b[0m, in \u001b[0;36mret_metabase\u001b[0;34m(country, question, filters, initialized)\u001b[0m\n\u001b[1;32m    105\u001b[0m         param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemplate-tag\u001b[39m\u001b[38;5;124m'\u001b[39m, name]]\n\u001b[1;32m    107\u001b[0m     params\u001b[38;5;241m.\u001b[39mappend(param)\n\u001b[0;32m--> 109\u001b[0m p_response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/card/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquestion_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/query/csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m p_response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    114\u001b[0m my_dict \u001b[38;5;241m=\u001b[39m p_response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def task_based_assignment():\n",
    "    # ----------------------------------------\n",
    "    # Initialize and get current time\n",
    "    # ----------------------------------------\n",
    "    initialize_env()\n",
    "    now = datetime.now() + timedelta(hours=3)\n",
    "    hour = int(str(now.time())[0:2])\n",
    "    print(f\"Starting process at hour: {hour}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Get available agents\n",
    "    # ----------------------------------------\n",
    "    attendance = ret_metabase(\"EGYPT\", 13502, initialized=True)\n",
    "    print(f\"Totale agents: {len(attendance)}\")\n",
    "    task_based_list = get_available_agents(attendance, hour)\n",
    "    time.sleep(15)\n",
    "    \n",
    "    if not task_based_list:\n",
    "        print(\"No agents available for current hour!\")\n",
    "        return\n",
    "        \n",
    "    # ----------------------------------------\n",
    "    # Get query IDs and blacklisted retailers\n",
    "    # ----------------------------------------\n",
    "    query_ids = google_sheets('Query ID Assigning', 'Sheet1', 'get')\n",
    "    blacklisted_retailers = query_ids['Blacklisted_retailers'].dropna().astype(int).tolist()\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Fetch and process query data\n",
    "    # ----------------------------------------\n",
    "    df_raw_R, df_raw_M = fetch_and_process_queries(query_ids, blacklisted_retailers)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Process previous assignments / For follow up calls\n",
    "    # ----------------------------------------\n",
    "    previous_calls = ret_metabase(\"EGYPT\", 35299, initialized=True)\n",
    "    previous_calls = clean_column_id(previous_calls, 'main_system_id')\n",
    "    task_based, final_old_assign = process_previous_assignments(df_raw_R, previous_calls)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Assign agents to tasks\n",
    "    # ----------------------------------------\n",
    "    main_data_list = []\n",
    "    \n",
    "    # Get and process mapping data\n",
    "    mapping_df = google_sheets('EG Telesales Cycle Assignment', 'Sheet1', 'get')\n",
    "    mapping_df = clean_column_id(mapping_df, 'main_system_id')\n",
    "    mapping_df = clean_column_id(mapping_df, 'agent_assigned')\n",
    "    \n",
    "    if mapping_df.empty:\n",
    "        print(\"WARNING: No mapping data found!\")\n",
    "    else:\n",
    "        print(f\"Found {len(mapping_df)} mapping records\")\n",
    "        mapping_df = mapping_df[mapping_df['agent_assigned'].isin(task_based_list)]\n",
    "        mapped_data = assign_data_by_mapping(df_raw_M, mapping_df)\n",
    "        \n",
    "        if mapped_data.empty:\n",
    "            print(\"WARNING: No data mapped to agents!\")\n",
    "        else:\n",
    "            notna = mapped_data['agent_assigned'].notna().sum()\n",
    "            na = mapped_data['agent_assigned'].isna().sum()\n",
    "            print(f\"Successfully mapped {notna} records to agents\")\n",
    "            print(f\"Retailers unassigned: {na}\")\n",
    "        main_data_list.append(mapped_data)\n",
    "    \n",
    "    # Process tasks by priority\n",
    "    priority_range = range(1, 16)  # 1 to 15 inclusive\n",
    "    for priority in priority_range:\n",
    "        priority_df = task_based[task_based[\"priority\"] == priority].reset_index(drop=True)\n",
    "        main_data = assign_data_equal_projects(priority_df, task_based_list)\n",
    "        main_data_list.append(main_data)\n",
    "    \n",
    "    main_data_total = pd.concat(main_data_list, ignore_index=True)\n",
    "    print(f\"Total assignments available (including ones already assigned): {main_data_total.shape[0]}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Clean and filter data\n",
    "    # ----------------------------------------\n",
    "    # Select required columns\n",
    "    main_data = main_data_total[\n",
    "        [\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"project_name\"]]\n",
    "    \n",
    "    # Handle special case for main_system_id == 1\n",
    "    main_system_id_1 = main_data[main_data['main_system_id'] == 1]\n",
    "    other_main_system_ids = main_data[main_data['main_system_id'] != 1]\n",
    "    other_main_system_ids = other_main_system_ids.drop_duplicates(subset=['main_system_id'])\n",
    "    main_data = pd.concat([main_system_id_1, other_main_system_ids], ignore_index=True)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Process previous assignments\n",
    "    # ----------------------------------------\n",
    "    sheet = google_sheets('[HOURLY] TASK-BASED Data', 'Data', 'get', [6])\n",
    "    already_assigned = [[str(val)] for val in sheet.iloc[1:, 0].dropna()]\n",
    "    already_assigned_df = pd.DataFrame.from_dict(already_assigned)\n",
    "    \n",
    "    df = main_data.copy()\n",
    "    \n",
    "    # Remove previous assignments and limit retailers per agent\n",
    "    \n",
    "    main_data_to_assign = remove_assign(already_assigned_df, df, 40)\n",
    "    final_old_assign_new = remove_assign(already_assigned_df, final_old_assign, 5)\n",
    "    \n",
    "    # Filter for available agents\n",
    "    filtered_df = main_data_to_assign[main_data_to_assign['agent_assigned'].isin(task_based_list)]\n",
    "    filtered_df_old = final_old_assign_new[final_old_assign_new['agent_assigned'].isin(task_based_list)]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        print(\"WARNING: No assignments after filtering!\")\n",
    "    else:\n",
    "        print(f\"Number of filtered assignments: {len(filtered_df)}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Prepare final data\n",
    "    # ----------------------------------------\n",
    "    final_data_to_assign = filtered_df.drop_duplicates(subset='main_system_id', keep='first').copy()\n",
    "    final_old_assign_new = filtered_df_old.drop_duplicates(subset='main_system_id', keep='first').copy()\n",
    "    \n",
    "    final_data_to_assign['added_at'] = now\n",
    "    final_old_assign_new['added_at'] = now\n",
    "    \n",
    "    final_data_to_assign = final_data_to_assign.drop(columns='index', errors='ignore')\n",
    "    final_data_to_assign = ensure_correct_dispatching(final_data_to_assign, task_based_list, final_old_assign_new)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Save data to various destinations\n",
    "    # ----------------------------------------\n",
    "    print(\"Starting data export process...\")\n",
    "    \n",
    "    # Save to parquet\n",
    "    final_data_to_assign = final_data_to_assign.astype(str)\n",
    "    sheet_df = final_data_to_assign[[\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"added_at\"]]\n",
    "    sheet_df\n",
    "task_based_assignment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
