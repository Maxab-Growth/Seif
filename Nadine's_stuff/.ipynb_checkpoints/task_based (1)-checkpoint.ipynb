{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30efde7d-d7ec-4737-bf08-18e4447e92ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install gspread\n",
    "!pip install oauth2client\n",
    "!pip install slackclient\n",
    "!pip install -U snowflake-connector-python\n",
    "!pip install -U snowflake-snowpark-python\n",
    "!pip install --upgrade psycopg2\n",
    "!pip install -U sqlalchemy\n",
    "!pip install df2gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1744aa0-dece-437d-8004-9c6a1a728270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from snowflake.snowpark import Session \n",
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "import json\n",
    "from requests import get\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date\n",
    "import time\n",
    "import zipfile\n",
    "import io\n",
    "import sqlalchemy\n",
    "from pathlib import Path\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import sys\n",
    "import requests\n",
    "from botocore.exceptions import ClientError\n",
    "import importlib\n",
    "from io import StringIO\n",
    "import snowflake.connector\n",
    "from sqlalchemy import create_engine\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import re\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import snowflake.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d247d4-f857-4f74-b6b9-86fdabf468d6",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b568060-f3f8-4ccb-823d-6b521e1b5cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "from requests import get\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def imports():\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import psycopg2\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException':\n",
    "            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n",
    "            # An error occurred on the server side.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException':\n",
    "            # You provided an invalid value for a parameter.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException':\n",
    "            # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            # We can't find the resource that you asked for.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            return get_secret_value_response['SecretString']\n",
    "        else:\n",
    "            return base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "        \n",
    "def initialize_env():\n",
    "    snowflake_sg_secret = json.loads(get_secret(\"Snowflake-sagemaker\"))\n",
    "    slack_secret = json.loads(get_secret(\"prod/slack/reports\"))\n",
    "    rowaa_metabase_access = json.loads(get_secret(\"prod/metabase/rowaa/user\"))\n",
    "    dwh_writer_secret = json.loads(get_secret(\"prod/db/datawarehouse/sagemaker\"))\n",
    "\n",
    "    os.environ[\"SNOWFLAKE_USERNAME\"] = snowflake_sg_secret[\"username\"]\n",
    "    os.environ[\"SNOWFLAKE_PASSWORD\"] = snowflake_sg_secret[\"password\"]\n",
    "    os.environ[\"SNOWFLAKE_ACCOUNT\"] = snowflake_sg_secret[\"account\"]\n",
    "    os.environ[\"SNOWFLAKE_DATABASE\"] = snowflake_sg_secret[\"database\"]\n",
    "\n",
    "    os.environ[\"SLACK_TOKEN\"] = slack_secret[\"token\"]\n",
    "\n",
    "    os.environ[\"METABASE_USERNAME_ROWAA\"] = rowaa_metabase_access[\"username\"]\n",
    "    os.environ[\"METABASE_PASSWORD_ROWAA\"] = rowaa_metabase_access[\"password\"]\n",
    "\n",
    "    os.environ[\"DWH_WRITER_HOST_NEW\"] = dwh_writer_secret[\"host\"]\n",
    "    os.environ[\"DWH_WRITER_NAME_NEW\"] = dwh_writer_secret[\"dbname\"]\n",
    "    os.environ[\"DWH_WRITER_USER_NAME_NEW\"] = dwh_writer_secret[\"username\"]\n",
    "    os.environ[\"DWH_WRITER_PASSWORD_NEW\"] = dwh_writer_secret[\"password\"] \n",
    "\n",
    "    json_path_sheets = str(Path.home()) + \"/service_account_key_sheets.json\"\n",
    "    sheets_key = get_secret(\"prod/maxab-sheets\")\n",
    "    f = open(json_path_sheets, \"w\")\n",
    "    f.write(sheets_key)\n",
    "    f.close()\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"] = json_path_sheets\n",
    "    \n",
    "\n",
    "def get_from_gsheet(workbook, sheet):\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "         'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\",\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "    initialize_env()\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"], scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    try:\n",
    "        wks = client.open(workbook).worksheet(sheet)\n",
    "        sheet = pd.DataFrame(wks.get_all_records())\n",
    "    except:\n",
    "        print(sheet,'failed to fetch data')\n",
    "    \n",
    "    return sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ce550-bb77-446c-9205-c8fbc184cba4",
   "metadata": {},
   "source": [
    "#### Defining Functions used in AM Data Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59dd2c-5223-432a-a76b-58f30d5185bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## assigning data in any project\n",
    "def assign_data(df, list, assigns):\n",
    "\n",
    "    actual_length = len(df) % len(list)\n",
    "    rows_needed = len(list) - actual_length\n",
    "    columns_needed = len(df.axes[1])\n",
    "    df = df.sample(frac=1)\n",
    "    empty_df = pd.DataFrame(index=np.arange(rows_needed), columns=np.arange(columns_needed))\n",
    "    assgn = pd.concat([df, empty_df])\n",
    "    assgn = assgn.dropna(axis=1, how='all')\n",
    "    mplr = len(assgn) / len(list) \n",
    "    assgn = assgn.assign(agent_assigned=[val for val in list for _ in range(int(mplr))])\n",
    "    assgn = assgn.dropna()\n",
    "\n",
    "    if assgn.shape[1] != 1:\n",
    "\n",
    "            assgn[\"main_system_id\"] = assgn[\"main_system_id\"].fillna('').astype(str).str.replace(\".0\",\"\",regex=False)\n",
    "            assgn[\"retailer_mobile_number\"] = assgn[\"retailer_mobile_number\"].fillna('').astype(str).str.replace(\".0\",\"\",regex=False)\n",
    "            df = assgn.groupby('agent_assigned').head(assigns)\n",
    "            return df\n",
    "    else:\n",
    "            assgn.drop(['agent_assigned'], axis=1)\n",
    "            assgn = assgn.iloc[0:0]\n",
    "            assgn = assgn.assign(index=\"\", main_system_id = \"\",retailer_name = \"\", retailer_mobile_number=\"\",description=\"\",offer=\"\",reward=\"\",balance=\"\",priority=\"\",agent_assigned=\"\")\n",
    "            assgn = assgn[[\"index\",\"main_system_id\",\"retailer_name\",\"retailer_mobile_number\",\"description\",\"offer\",\"reward\",\"balance\",\"priority\",\"agent_assigned\"]]\n",
    "            df = assgn.groupby('agent_assigned').head(assigns)\n",
    "            return df\n",
    "\n",
    "\n",
    "## removing previously assigned - recharge\n",
    "def remove_assign(previously_assigned, df, assigns):\n",
    "    if  previously_assigned.shape[0] != 1:\n",
    "        previously_assigned[0] = previously_assigned[0].fillna('').astype(str).str.replace(\" \",\"\",regex=False)\n",
    "        previously_assigned = previously_assigned.dropna()\n",
    "        previously_assigned[0] = previously_assigned[0].astype('float')\n",
    "        previously_assigned[0] = previously_assigned[0].astype('int')\n",
    "        df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "        main_data_to_assign = pd.DataFrame(df.loc[~df['main_system_id'].isin(previously_assigned[0].astype(int).values)])\n",
    "        main_data_to_assign['main_system_id'] = main_data_to_assign['main_system_id'].astype('int')\n",
    "        main_data_to_assign = main_data_to_assign.groupby('agent_assigned').head(assigns)\n",
    "        return main_data_to_assign\n",
    "    else:\n",
    "        df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "        main_data_to_assign = df.groupby('agent_assigned').head(assigns)\n",
    "        return main_data_to_assign\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978e9c9-769b-46f7-8218-78a15d06cecb",
   "metadata": {},
   "source": [
    "#### Initializing GSheet Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df134b-434f-4ff1-8604-c86be4ef3885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import oauth2client\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from pprint import pprint\n",
    "from df2gspread import df2gspread as d2g\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "import importlib\n",
    "\n",
    "import os\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "import datetime\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "initialize_env()\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "# define the scope\n",
    "scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "# add credentials to the account\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"], scope)\n",
    "# authorize the clientsheet\n",
    "client = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22afcacb-af97-4ea1-a85d-cc9901e4324e",
   "metadata": {},
   "source": [
    "#### Defining ret_metabase Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b0fe1-79c5-46a9-bdd1-dcd778e303a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "def ret_metabase(question):\n",
    "    question_id = str(question)\n",
    "    file_name = 'meta_results.csv'\n",
    "    # Do not modify below this\n",
    "    base_url = 'https://bi.maxab.info/api'\n",
    "    base_headers = {'Content-Type' : 'application/json'}\n",
    "\n",
    "    try:\n",
    "        s_response = requests.post(base_url + '/session',\n",
    "                                data = json.dumps({'username': os.environ[\"METABASE_USERNAME_ROWAA\"], 'password': os.environ[\"METABASE_PASSWORD_ROWAA\"]}),\n",
    "                                headers=base_headers)\n",
    "        s_response.raise_for_status()\n",
    "\n",
    "        session_token = s_response.json()['id']\n",
    "        base_headers['X-Metabase-Session'] = session_token\n",
    "\n",
    "        p_response = requests.post(base_url + '/card/' + question_id + '/query/csv', headers=base_headers)\n",
    "        p_response.raise_for_status()\n",
    "\n",
    "        my_dict = p_response.content\n",
    "        s = str(my_dict,'utf-8')\n",
    "        my_dict = StringIO(s)\n",
    "        df = pd.read_csv(my_dict)\n",
    "        return(df)\n",
    "\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        return(errh)\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        return(errc)\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        return(errt)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        return(err)\n",
    "    \n",
    "#run query save on metabse with ID, \n",
    "#for example here query 1606 is save in my personal collection and I can run it like line below\n",
    "#ret_metabase(1606)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9cc02-6990-4c25-9b76-04fa4b96807f",
   "metadata": {},
   "source": [
    "### AM Attendance [Table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5116305-115e-4d51-89d2-14946b5b645d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# database connection:\n",
    "import os\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import os\n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "\n",
    "## setup the environment:\n",
    "initialize_env()\n",
    "\n",
    "## get secrets:\n",
    "host=os.environ[\"DWH_WRITER_HOST_NEW\"]\n",
    "database=os.environ[\"DWH_WRITER_NAME_NEW\"]\n",
    "user=os.environ[\"DWH_WRITER_USER_NAME_NEW\"]\n",
    "password=os.environ[\"DWH_WRITER_PASSWORD_NEW\"]\n",
    "\n",
    "conn = psycopg2.connect(host=host, database=database, user=user, password=password)\n",
    "print(\"Successfully connected to DB\")\n",
    "\n",
    "try:\n",
    "        ## get the df:\n",
    "    df_3 = get_from_gsheet(\"daily_automatic_assignment\",\"attendance_table\")\n",
    "        \n",
    "        ## db connection:\n",
    "    engine = sqlalchemy.create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}/{database}\")\n",
    "    print(bool(engine))\n",
    "        \n",
    "        \n",
    "        # data cleaning in am_attendance\n",
    "    for col in ['id','agent_id']:\n",
    "        df_3.loc[df_3[col]!='',col] = df_3.loc[df_3[col]!='',col].astype(\"int\")  \n",
    "            \n",
    "    for col in ['date']:\n",
    "        df_3.loc[df_3[col]!='',col] = df_3.loc[df_3[col]!='',col].astype(\"datetime64[ns]\")\n",
    "        \n",
    "        # add last_updated_at: \n",
    "    df_3[\"last_updated_at\"] = datetime.now() \n",
    "       \n",
    "        ## update the table on db:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"start\")\n",
    "        df_3.to_sql(name='am_attendance',schema='fintech', con=engine, if_exists='replace',chunksize=1000,method='multi')\n",
    "        print(\"end\")\n",
    "        conn.close()\n",
    "            \n",
    "        ## close the connection:\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f'attendance table issue: {e}')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ae30b-e8de-4a4b-85ec-5573cc06431d",
   "metadata": {},
   "source": [
    "### Shift Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c63034-c02f-41c1-a071-87578aa3bc56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shift Schedule\n",
    "from datetime import datetime, timedelta\n",
    "now = datetime.now() + timedelta(hours=3)\n",
    "hour = int(str(now.time())[0:2])\n",
    "attendance = ret_metabase(13502)\n",
    "\n",
    "attendance_copy = attendance.copy()\n",
    "\n",
    "attendance_copy['start_time'] = attendance_copy['start_time'].astype(int)\n",
    "attendance_copy['end_time'] = attendance_copy['end_time'].astype(int)\n",
    "\n",
    "attendance_copy['assignment_start_time'] = attendance_copy['start_time'] - 1\n",
    "attendance_copy['assignment_end_time'] = attendance_copy['end_time'] - 2\n",
    "\n",
    "attendance_copy['assign_data'] =  np.where((hour >= attendance_copy['assignment_start_time']\n",
    "                                )\n",
    "                                &\n",
    "                                (hour <= attendance_copy['assignment_end_time'])    \n",
    "                                , 'yes','no' )\n",
    "\n",
    "\n",
    "task_based_agents = attendance_copy.loc[(attendance_copy['project'] == 'task_based') \n",
    "                 &\n",
    "                 (attendance_copy['assign_data'] =='yes')]\n",
    "\n",
    "task_based_list = task_based_agents['agent_id'].values.tolist()\n",
    "number_of_task_based_agents = len(task_based_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100fff8-7db4-4e74-b166-d56117182f3d",
   "metadata": {},
   "source": [
    "### Dispatching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5b979-640b-4e26-b12e-9b1208accaca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if number_of_task_based_agents != 0:\n",
    "    # setup the environment:\n",
    "    initialize_env()\n",
    "\n",
    "    df_2 = ret_metabase(18915)\n",
    "    df_2.columns = map(str.lower, df_2.columns)\n",
    "    df_4 = ret_metabase(36299)\n",
    "    df_4.columns = map(str.lower, df_4.columns)\n",
    "    df_5 = ret_metabase(35981)\n",
    "    df_5.columns = map(str.lower, df_5.columns)\n",
    "    df_7 = ret_metabase(23764)\n",
    "    df_7.columns = map(str.lower, df_7.columns)\n",
    "    df_11 = ret_metabase(38188)\n",
    "    df_11.columns = map(str.lower, df_11.columns)\n",
    "    df_15 = ret_metabase(40154)\n",
    "    df_15.columns = map(str.lower, df_15.columns)\n",
    "    df_16 = ret_metabase(40541)\n",
    "    df_16.columns = map(str.lower, df_16.columns)\n",
    "    df_17 = ret_metabase(35135)\n",
    "    df_17.columns = map(str.lower, df_17.columns)\n",
    "\n",
    "    # Create a list of the dataframes\n",
    "    dataframes = [df_2, df_4, df_5, df_7, df_11, df_15, df_16, df_17]\n",
    "\n",
    "    # Concatenate the dataframes along the rows (union all)\n",
    "    df_unfiltered = pd.concat(dataframes, ignore_index=True)\n",
    "    # Excluding blacklisted retailers\n",
    "    df_raw = df_unfiltered[df_unfiltered['main_system_id'] != 214101]\n",
    "    df_raw['main_system_id'] = df_raw['main_system_id'].str.replace(',', '')\n",
    "    df_raw['main_system_id'] = df_raw['main_system_id'].astype('Int64', errors='ignore')\n",
    "    previous_calls = ret_metabase(35299)\n",
    "    previous_calls['main_system_id'] = previous_calls['main_system_id'].str.replace(',', '')\n",
    "    previous_calls['main_system_id'] = previous_calls['main_system_id'].astype('Int64', errors='ignore')\n",
    "\n",
    "    df = df_raw.merge(previous_calls, on='main_system_id', how='left')\n",
    "    exclude_same_assigns = previous_calls[['main_system_id']].astype('Int64')\n",
    "    task_based = pd.DataFrame(df_raw.loc[~df_raw['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "    old_assgns = pd.DataFrame(df_raw.loc[df_raw['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "    old_assgns['main_system_id'] = old_assgns['main_system_id'].astype('Int64', errors='ignore')\n",
    "    df_2 = old_assgns.merge(previous_calls, on='main_system_id', how='left')\n",
    "    final_old_assign = df_2[[\n",
    "        'main_system_id',\n",
    "        'retailer_mobile_number',\n",
    "        'retailer_name',\n",
    "        'description',\n",
    "        'balance',\n",
    "        'offer',\n",
    "        'reward',\n",
    "        'agent_assigned',\n",
    "        'priority'\n",
    "        ]]\n",
    "\n",
    "    priority_1 = pd.DataFrame(task_based[task_based[\"priority\"]==1].reset_index())\n",
    "    main_data_1 = assign_data(priority_1, task_based_list, 2000)\n",
    "    priority_2 = pd.DataFrame(task_based[task_based[\"priority\"]==2].reset_index())\n",
    "    main_data_2 = assign_data(priority_2, task_based_list, 2000)\n",
    "    priority_3 = pd.DataFrame(task_based[task_based[\"priority\"]==3].reset_index())\n",
    "    main_data_3 = assign_data(priority_3, task_based_list, 2000)\n",
    "    priority_4 = pd.DataFrame(task_based[task_based[\"priority\"]==4].reset_index())\n",
    "    main_data_4 = assign_data(priority_4, task_based_list, 2000)\n",
    "    priority_5 = pd.DataFrame(task_based[task_based[\"priority\"]==5].reset_index())\n",
    "    main_data_5 = assign_data(priority_5, task_based_list, 2000)\n",
    "    priority_6 = pd.DataFrame(task_based[task_based[\"priority\"]==6].reset_index())\n",
    "    main_data_6 = assign_data(priority_6, task_based_list, 2000)\n",
    "    priority_7 = pd.DataFrame(task_based[task_based[\"priority\"]==7].reset_index())\n",
    "    main_data_7 = assign_data(priority_7, task_based_list, 2000)\n",
    "    priority_8 = pd.DataFrame(task_based[task_based[\"priority\"]==8].reset_index())\n",
    "    main_data_8 = assign_data(priority_8, task_based_list, 2000)\n",
    "    priority_9 = pd.DataFrame(task_based[task_based[\"priority\"]==9].reset_index())\n",
    "    main_data_9 = assign_data(priority_9, task_based_list, 2000)\n",
    "    priority_10 = pd.DataFrame(task_based[task_based[\"priority\"]==10].reset_index())\n",
    "    main_data_10 = assign_data(priority_10, task_based_list, 2000)\n",
    "    priority_11 = pd.DataFrame(task_based[task_based[\"priority\"]==11].reset_index())\n",
    "    main_data_11 = assign_data(priority_11, task_based_list, 2000)\n",
    "    priority_12 = pd.DataFrame(task_based[task_based[\"priority\"]==12].reset_index())\n",
    "    main_data_12 = assign_data(priority_12, task_based_list, 2000)\n",
    "    priority_13 = pd.DataFrame(task_based[task_based[\"priority\"]==13].reset_index())\n",
    "    main_data_13 = assign_data(priority_13, task_based_list, 2000)\n",
    "    priority_14 = pd.DataFrame(task_based[task_based[\"priority\"]==14].reset_index())\n",
    "    main_data_14 = assign_data(priority_14, task_based_list, 2000)\n",
    "    priority_15 = pd.DataFrame(task_based[task_based[\"priority\"]==15].reset_index())\n",
    "    main_data_15 = assign_data(priority_15, task_based_list, 2000)\n",
    "\n",
    "    # concatenating priorities\n",
    "    main_list = [\n",
    "        final_old_assign,\n",
    "        main_data_1,\n",
    "        main_data_2,\n",
    "        main_data_3,\n",
    "        main_data_4,\n",
    "        main_data_5,\n",
    "        main_data_6,\n",
    "        main_data_7,\n",
    "        main_data_8,\n",
    "        main_data_9,\n",
    "        main_data_10,\n",
    "        main_data_11,\n",
    "        main_data_12,\n",
    "        main_data_13,\n",
    "        main_data_14,\n",
    "        main_data_15\n",
    "    ]\n",
    "\n",
    "    main_data_total = pd.concat(main_list, axis=0, ignore_index=True)\n",
    "    main_data = main_data_total[[\"main_system_id\", \"retailer_mobile_number\",\"retailer_name\",\"description\",\"reward\",\"balance\",\"offer\",\"agent_assigned\"]]\n",
    "    main_data = main_data.drop_duplicates(subset=['main_system_id'])\n",
    "\n",
    "    # removing previously assigned\n",
    "    scope = [\n",
    "        \"https://spreadsheets.google.com/feeds\",\n",
    "        'https://www.googleapis.com/auth/spreadsheets',\n",
    "        \"https://www.googleapis.com/auth/drive.file\",\n",
    "        \"https://www.googleapis.com/auth/drive\"\n",
    "    ]\n",
    "\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"], scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    sheet = client.open('[HOURLY] TASK-BASED Data')\n",
    "    sheet_instance = sheet.worksheet('Data')\n",
    "    assignments = sheet_instance.get('G5:G')\n",
    "    assignments_df = pd.DataFrame.from_dict(assignments)\n",
    "\n",
    "    df = main_data.copy()\n",
    "\n",
    "    if hour == 18:\n",
    "        main_data_to_assign = remove_assign(assignments_df,df,70)\n",
    "        filtered_df = main_data_to_assign[main_data_to_assign['agent_assigned'].isin(task_based_list)]\n",
    "    else:\n",
    "        main_data_to_assign = remove_assign(assignments_df,df,50)\n",
    "        filtered_df = main_data_to_assign[main_data_to_assign['agent_assigned'].isin(task_based_list)]\n",
    "\n",
    "    # importing data in agents' sheet\n",
    "    final_data_to_assign = filtered_df.copy()\n",
    "    final_data_to_assign = final_data_to_assign.drop_duplicates(subset='main_system_id', keep=\"first\")\n",
    "\n",
    "    final_data_to_assign['added_at'] = dt.datetime.now()  + timedelta(hours=3)\n",
    "    if final_data_to_assign.columns[0] == 'index':\n",
    "        final_data_to_assign = final_data_to_assign.drop(labels='index', axis=1)\n",
    "    pass\n",
    "\n",
    "    final_data_to_assign = final_data_to_assign.astype(str)\n",
    "    google_sh = client.open('[HOURLY] TASK-BASED Data')\n",
    "    sheet = google_sh.worksheet('raw_data')\n",
    "    # sheet.clear()\n",
    "    sheet.append_rows([final_data_to_assign.columns.values.tolist()] + final_data_to_assign.values.tolist(), value_input_option=\"USER_ENTERED\")\n",
    "\n",
    "    # Writing historical data on the dwh\n",
    "    # get secrets:\n",
    "    host = os.environ[\"DWH_WRITER_HOST_NEW\"]\n",
    "    database = os.environ[\"DWH_WRITER_NAME_NEW\"]\n",
    "    user = os.environ[\"DWH_WRITER_USER_NAME_NEW\"]\n",
    "    password = os.environ[\"DWH_WRITER_PASSWORD_NEW\"]\n",
    "\n",
    "    conn = psycopg2.connect(host=host, database=database, user=user, password=password)\n",
    "    print(\"Successfully connected to DB\")\n",
    "\n",
    "    df = final_data_to_assign[['main_system_id', 'description','offer', 'reward','agent_assigned']]\n",
    "    df['dispatched_at'] = dt.datetime.now() - timedelta(hours=3)\n",
    "\n",
    "    # db connection:\n",
    "    engine = sqlalchemy.create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}/{database}\")\n",
    "    print(bool(engine))\n",
    "\n",
    "    df['main_system_id'] = df['main_system_id'].str.replace(',', '')\n",
    "    df['agent_assigned'] = df['agent_assigned'].str.replace(',', '')\n",
    "    df['reward'] = df['reward'].str.replace(',', '')\n",
    "\n",
    "    df.main_system_id = pd.to_numeric(df.main_system_id, errors='coerce').astype('Int64')\n",
    "    df['reward'] = pd.to_numeric(df['reward'], errors='coerce').astype(float)\n",
    "    df['agent_assigned'] = df['agent_assigned'].astype('Int64', errors='ignore')\n",
    "\n",
    "    # update the table on db:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"start\")\n",
    "        df.to_sql(name='task_based_am_projects', schema='fintech', con=engine, if_exists='append', chunksize=1000, method='multi', index=False)\n",
    "        print(\"end\")\n",
    "        conn.close()\n",
    "\n",
    "        # close the connection:\n",
    "        conn.close()\n",
    "\n",
    "else:\n",
    "    print(\"Hour is not equal to 9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739401b-fb86-4fd9-a35f-6a08e1914a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assignments_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a8ffe-d4dd-40f3-af51-576931631c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
