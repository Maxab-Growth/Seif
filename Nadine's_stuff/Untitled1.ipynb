{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2116524e-9243-4587-9216-85b9c68ce270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from common_functions import google_sheets, task_fail_slack_alert, get_secret, snowflake_query, ret_metabase\n",
    "from datetime import datetime, timedelta\n",
    "import datetime as dt\n",
    "\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def initialize_env():\n",
    "    snowflake_sg_secret = json.loads(get_secret(\"Snowflake-sagemaker\"))\n",
    "    slack_secret = json.loads(get_secret(\"prod/slack/reports\"))\n",
    "    fintech_service_account = json.loads(get_secret(\"prod/fintechServiceEmail/credentials\"))\n",
    "    dwh_writer_secret = json.loads(get_secret(\"prod/db/datawarehouse/sagemaker\"))\n",
    "\n",
    "    os.environ[\"SNOWFLAKE_USERNAME\"] = snowflake_sg_secret[\"username\"]\n",
    "    os.environ[\"SNOWFLAKE_PASSWORD\"] = snowflake_sg_secret[\"password\"]\n",
    "    os.environ[\"SNOWFLAKE_ACCOUNT\"] = snowflake_sg_secret[\"account\"]\n",
    "    os.environ[\"SNOWFLAKE_DATABASE\"] = snowflake_sg_secret[\"database\"]\n",
    "\n",
    "    os.environ[\"SLACK_TOKEN\"] = slack_secret[\"token\"]\n",
    "\n",
    "    os.environ[\"FINTECH_EMONEY_EMAIL\"] = fintech_service_account[\"email_name\"]\n",
    "    os.environ[\"FINTECH_EMONEY_PASSWORD\"] = fintech_service_account[\"email_password\"]\n",
    "\n",
    "    metabase_secret = json.loads(get_secret(\"prod/metabase/maxab_config\"))\n",
    "    os.environ[\"EGYPT_METABASE_USERNAME\"] = metabase_secret[\"metabase_user\"]\n",
    "    os.environ[\"EGYPT_METABASE_PASSWORD\"] = metabase_secret[\"metabase_password\"]\n",
    "\n",
    "    os.environ[\"DWH_WRITER_HOST_NEW\"] = dwh_writer_secret[\"host\"]\n",
    "    os.environ[\"DWH_WRITER_NAME_NEW\"] = dwh_writer_secret[\"dbname\"]\n",
    "    os.environ[\"DWH_WRITER_USER_NAME_NEW\"] = dwh_writer_secret[\"username\"]\n",
    "    os.environ[\"DWH_WRITER_PASSWORD_NEW\"] = dwh_writer_secret[\"password\"] \n",
    "\n",
    "    json_path_sheets = str(Path.home()) + \"/service_account_key_sheets.json\"\n",
    "    sheets_key = get_secret(\"prod/maxab-sheets\")\n",
    "    f = open(json_path_sheets, \"w\")\n",
    "    f.write(sheets_key)\n",
    "    f.close()\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"] = json_path_sheets\n",
    "\n",
    "def ret_metabase(country, question, filters={}, initialized = False):\n",
    "\n",
    "    \n",
    "    if not initialized: \n",
    "        initialize_env()\n",
    "    \n",
    "    question_id = str(question)\n",
    "    \n",
    "    if country.lower() == 'egypt':\n",
    "        base_url = 'https://bi.maxab.info/api'\n",
    "        username = str(os.environ[\"EGYPT_METABASE_USERNAME\"])\n",
    "        password = str(os.environ[\"EGYPT_METABASE_PASSWORD\"])\n",
    "    else:\n",
    "        base_url = 'https://bi.maxabma.com/api'\n",
    "        username = str(os.environ[\"AFRICA_METABASE_USERNAME\"])\n",
    "        password = str(os.environ[\"AFRICA_METABASE_PASSWORD\"])\n",
    "\n",
    "    base_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    try:\n",
    "        s_response = requests.post(\n",
    "            base_url + '/session',\n",
    "            data=json.dumps({\n",
    "                'username': username,\n",
    "                'password': password\n",
    "            }),\n",
    "            headers=base_headers)\n",
    "        \n",
    "        s_response.raise_for_status()\n",
    "\n",
    "        session_token = s_response.json()['id']\n",
    "        base_headers['X-Metabase-Session'] = session_token\n",
    "        \n",
    "        params = []\n",
    "        \n",
    "        for name, value in filters.items():\n",
    "            filter_type, filter_value = value\n",
    "            param = {'target': ['variable', ['template-tag', name]], 'value': filter_value}\n",
    "            \n",
    "            if filter_type.lower() == 'date':\n",
    "                param['type'] = 'date/range' if isinstance(filter_value, list) else 'date/single'\n",
    "            elif filter_type.lower() == 'category':\n",
    "                param['type'] = 'category'\n",
    "            elif filter_type.lower() == 'text':\n",
    "                param['type'] = 'text'\n",
    "            elif filter_type.lower() == 'number':\n",
    "                param['type'] = 'number'\n",
    "            elif filter_type.lower() == 'field list':\n",
    "                param['type'] = 'id'\n",
    "                param['target'] = ['dimension', ['template-tag', name]]\n",
    "            \n",
    "            params.append(param)\n",
    "\n",
    "        p_response = requests.post(base_url + '/card/' + question_id + '/query/csv', \n",
    "                                   json={'parameters': params}, \n",
    "                                   headers=base_headers)\n",
    "        p_response.raise_for_status()\n",
    "\n",
    "        my_dict = p_response.content\n",
    "        s = str(my_dict, 'utf-8')\n",
    "        my_dict = StringIO(s)\n",
    "        df = pd.read_csv(my_dict)\n",
    "        return(df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def check_distribution(df, agent_list):\n",
    "    \"\"\"\n",
    "    Checks if the distribution of project types across agents is even.\n",
    "    Returns a dictionary with the count of rows per agent for each project type.\n",
    "    \"\"\"\n",
    "    distribution = {}\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        distribution[project] = project_df['agent_assigned'].value_counts().reindex(agent_list, fill_value=0)\n",
    "    \n",
    "    return distribution\n",
    "\n",
    "def redistribute_rows(df, agent_list):\n",
    "    \"\"\"\n",
    "    Redistributes rows among agents if the distribution is uneven.\n",
    "    \"\"\"\n",
    "    project_types = df['project_name'].unique()\n",
    "    redistributed_data = pd.DataFrame()\n",
    "    print(\"Redistributing rows...\")\n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.sample(frac=1).reset_index(drop=True)  # Shuffle data\n",
    "        rows_per_agent = len(project_df) // len(agent_list)\n",
    "        remainder = len(project_df) % len(agent_list)\n",
    "        \n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, agent in enumerate(agent_list):\n",
    "            end_idx = start_idx + rows_per_agent + (1 if i < remainder else 0)\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            redistributed_data = pd.concat([redistributed_data, agent_data])\n",
    "            \n",
    "            start_idx = end_idx\n",
    "    \n",
    "    redistributed_data = redistributed_data.reset_index(drop=True)\n",
    "    \n",
    "    return redistributed_data\n",
    "\n",
    "def ensure_correct_dispatching(df, agent_list, final_old_assign):\n",
    "    \"\"\"\n",
    "    Ensures the dispatching is done correctly by checking and redistributing rows if necessary.\n",
    "    `final_old_assign` rows are excluded from redistribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Exclude final_old_assign from the distribution check, but keep rows with main_system_id == 1\n",
    "    new_assignments = df[~df['main_system_id'].isin(final_old_assign['main_system_id']) | (df['main_system_id'] == 1)]\n",
    "    \n",
    "    distribution = check_distribution(new_assignments, agent_list)\n",
    "    \n",
    "    # Check if any project has an uneven distribution across agents\n",
    "    uneven_distribution = any(distribution[project].nunique() > 1 for project in distribution)\n",
    "    \n",
    "    if uneven_distribution:\n",
    "        print(\"Uneven distribution detected.\")\n",
    "        new_assignments = redistribute_rows(new_assignments, agent_list)\n",
    "    else:\n",
    "        print(\"Distribution is even. No redistribution needed.\")\n",
    "    \n",
    "    # Combine the redistributed new assignments with the final_old_assign\n",
    "    final_data = pd.concat([final_old_assign, new_assignments], ignore_index=True)\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def check_distribution_df(df, agent_list):\n",
    "    \"\"\"\n",
    "    Checks if the distribution of project types across agents is even.\n",
    "    Returns a DataFrame with the count of rows per agent for each project type.\n",
    "    \"\"\"\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    distribution_data = []\n",
    "    current_time = datetime.now()  # Get the current datetime\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        agent_counts = project_df['agent_assigned'].value_counts().reindex(agent_list, fill_value=0)\n",
    "        \n",
    "        for agent, count in agent_counts.items():\n",
    "            distribution_data.append({\n",
    "                'project_type': project,\n",
    "                'agent_assigned': agent,\n",
    "                'count': count,\n",
    "                'datetime': dt.datetime.now()  + timedelta(hours=3)\n",
    "            })\n",
    "    \n",
    "    distribution_df = pd.DataFrame(distribution_data)\n",
    "    \n",
    "    return distribution_df\n",
    "\n",
    "def clean_column_id(df, column_name):\n",
    "    # Ensure the column is treated as a string\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    \n",
    "    # Replace commas in the string\n",
    "    df[column_name] = df[column_name].str.replace(',', '')\n",
    "    \n",
    "    # Convert back to an integer, if appropriate\n",
    "    df[column_name] = df[column_name].astype('Int64', errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ----------------------------------------\n",
    "# Random distribution (Equal assigning)   \n",
    "# ----------------------------------------\n",
    "def assign_data_equal_projects(df, list):\n",
    "    df = df.sample(frac=1)  # Shuffle the data\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    assigned_data = pd.DataFrame()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.reset_index(drop=True)\n",
    "        rows_per_agent = len(project_df) // len(list)\n",
    "        remainder = len(project_df) % len(list)\n",
    "        \n",
    "        # Distribute rows equally\n",
    "        for i, agent in enumerate(list):\n",
    "            start_idx = i * rows_per_agent\n",
    "            end_idx = start_idx + rows_per_agent\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            # Handle remainder\n",
    "            if i < remainder:\n",
    "                extra_row = project_df.iloc[end_idx:end_idx+1].copy()\n",
    "                extra_row['agent_assigned'] = agent\n",
    "                agent_data = pd.concat([agent_data, extra_row])\n",
    "            \n",
    "            assigned_data = pd.concat([assigned_data, agent_data])\n",
    "    \n",
    "    assigned_data = assigned_data.reset_index(drop=True)\n",
    "    \n",
    "    return assigned_data\n",
    "\n",
    "# ----------------------------------------\n",
    "# Mapping distribution (Segment-based)\n",
    "# ----------------------------------------\n",
    "def assign_data_by_mapping(df, mapping_df):\n",
    "    # store retail-agent mapping in a dictionary \n",
    "    mapping_dict = dict(zip(mapping_df['MAIN_SYSTEM_ID'], mapping_df['AGENT_ID']))\n",
    "        \n",
    "    assigned_agents = []\n",
    "\n",
    "    for retailer_id in df['main_system_id']:\n",
    "        if retailer_id in mapping_dict:\n",
    "            assigned_agents.append(mapping_dict[retailer_id])\n",
    "        else:\n",
    "            assigned_agents.append(None)\n",
    "   \n",
    "    df['agent_assigned'] = assigned_agents\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_available_agents(attendance_df, current_hour):\n",
    "    \"\"\"Get list of available task-based agents for the current hour.\"\"\"\n",
    "    attendance_copy = attendance_df.copy()\n",
    "    \n",
    "    attendance_copy['start_time'] = attendance_copy['start_time'].astype(int)\n",
    "    attendance_copy['end_time'] = attendance_copy['end_time'].astype(int)\n",
    "    \n",
    "    attendance_copy['assignment_start_time'] = attendance_copy['start_time'] - 1\n",
    "    attendance_copy['assignment_end_time'] = attendance_copy['end_time'] - 1\n",
    "    \n",
    "    attendance_copy['assign_data'] = np.where(\n",
    "        (current_hour >= attendance_copy['assignment_start_time']) & \n",
    "        (current_hour <= attendance_copy['assignment_end_time']),\n",
    "        'yes', 'no')\n",
    "    \n",
    "    task_based_agents = attendance_copy.loc[\n",
    "        (attendance_copy['project'] == 'task_based') & \n",
    "        (attendance_copy['assign_data'] == 'yes')]\n",
    "    \n",
    "    task_based_list = task_based_agents['agent_id'].values.tolist()\n",
    "    print(f\"Number of available agents: {len(task_based_list)}\")\n",
    "    return task_based_list\n",
    "\n",
    "def fetch_and_process_queries(query_ids, blacklisted_retailers):\n",
    "    \"\"\"Fetch and process data from queries, removing blacklisted retailers.\"\"\"\n",
    "    queries_for_random = query_ids['Query_id_random'].dropna().astype(int).tolist()\n",
    "    queries_for_mapped = query_ids['Query_id_segment_based'].dropna().astype(int).tolist()\n",
    "    print(f\"Fetching data from {len(queries_for_random)+len(queries_for_mapped)} queries...\")\n",
    "    \n",
    "    # Process random queries\n",
    "    dataframes_R = [ret_metabase(\"EGYPT\", query, initialized=True) for query in queries_for_random]\n",
    "    empty_queries = []\n",
    "    for i, df in enumerate(dataframes_R):\n",
    "        if df.empty:\n",
    "            empty_queries.append(queries_for_random[i])\n",
    "        else:\n",
    "            print(f\"Query {queries_for_random[i]} returned {len(df)} records\")\n",
    "        df.columns = map(str.lower, df.columns)\n",
    "    if empty_queries:\n",
    "        print(f\"WARNING: Queries {empty_queries} returned empty dataframe!\")\n",
    "    # ----------------------------------------\n",
    "    # write in google sheet available data\n",
    "    # ----------------------------------------\n",
    "    # check for empty queries in Random tasks\n",
    "    for idx, row in query_ids.iterrows():\n",
    "        query_id = row['Query_id_random']\n",
    "        if pd.notna(query_id):\n",
    "            df = ret_metabase(\"EGYPT\", int(query_id), initialized=True)\n",
    "            query_ids.at[idx, 'Available_data'] = 'Empty' if df.empty else str(len(df))\n",
    "\n",
    "    # Step 3: Overwrite the sheet with updated full data (including manually-entered columns like F)\n",
    "    google_sheets('Query ID Assigning', 'Sheet1', 'overwrite', df=query_ids)\n",
    "    \n",
    "    # check for empty queries in Segment-based\n",
    "    for idx, row in query_ids.iterrows():\n",
    "        query_id = row['Query_id_segment_based']\n",
    "        if pd.notna(query_id):\n",
    "            df = ret_metabase(\"EGYPT\", int(query_id), initialized=True)\n",
    "            query_ids.at[idx, 'Available_data'] = 'Empty' if df.empty else str(len(df))\n",
    "\n",
    "    # Step 3: Overwrite the sheet with updated full data (including manually-entered columns like F)\n",
    "    google_sheets('Query ID Assigning', 'Sheet1', 'overwrite', df=query_ids)\n",
    "    \n",
    "    # Process mapped queries\n",
    "    dataframes_M = [ret_metabase(\"EGYPT\", query, initialized=True) for query in queries_for_mapped]\n",
    "    for i, df in enumerate(dataframes_M):\n",
    "        if df.empty:\n",
    "            print(f\"WARNING: Query {queries_for_mapped[i]} returned empty dataframe!\")\n",
    "        else:\n",
    "            print(f\"Query {queries_for_mapped[i]} returned {len(df)} records\")\n",
    "        df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "    # Combine and clean dataframes\n",
    "    df_unfiltered_R = pd.concat(dataframes_R, ignore_index=True)\n",
    "    df_unfiltered_M = pd.concat(dataframes_M, ignore_index=True)\n",
    "    print(f\"Mapping tasks available: {df_unfiltered_M.shape[0]}\")\n",
    "    print(f\"Random tasks available: {df_unfiltered_R.shape[0]}\")\n",
    "    \n",
    "    # Remove blacklisted retailers\n",
    "    df_raw_R = df_unfiltered_R[~df_unfiltered_R['main_system_id'].isin(blacklisted_retailers)]\n",
    "    df_raw_M = df_unfiltered_M[~df_unfiltered_M['main_system_id'].isin(blacklisted_retailers)]\n",
    "    \n",
    "    df_raw_R = clean_column_id(df_raw_R, 'main_system_id')\n",
    "    df_raw_M = clean_column_id(df_raw_M, 'main_system_id')\n",
    "    print(f\"Removed {(len(df_unfiltered_R)+len(df_unfiltered_M)) - (len(df_raw_R)+len(df_raw_M))} blacklisted retailers\")\n",
    "    \n",
    "    return df_raw_R, df_raw_M\n",
    "\n",
    "def process_previous_assignments(df_raw_R, previous_calls):\n",
    "    \"\"\"Process previous assignments and separate new and old assignments.\"\"\"\n",
    "    exclude_same_assigns = clean_column_id(previous_calls, 'main_system_id')\n",
    "    \n",
    "    # Get new assignments\n",
    "    task_based = pd.DataFrame(df_raw_R.loc[~df_raw_R['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "    \n",
    "    # Get old assignments\n",
    "    old_assgns = pd.DataFrame(df_raw_R.loc[df_raw_R['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "    df_1 = old_assgns.merge(previous_calls, on='main_system_id', how='left')\n",
    "    final_old_assign = df_1[[\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"project_name\"]]\n",
    "    \n",
    "    return task_based, final_old_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a27cc36-6803-4ebc-9da1-214d5765bfed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_assign(previously_assigned, df, assigns, print_it = False):\n",
    "    print(\"Starting remove_assign function...\")\n",
    "\n",
    "    try:\n",
    "        if not previously_assigned.empty:\n",
    "            previously_assigned[0] = previously_assigned[0].fillna('').astype(str).str.replace(\" \", \"\", regex=False)\n",
    "            previously_assigned = previously_assigned.dropna()\n",
    "            previously_assigned[0] = previously_assigned[0].astype('float').astype('int')\n",
    "            df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "            if 'agent_assigned' in df.columns:\n",
    "                if df['agent_assigned'].dtype == 'object':\n",
    "                    df.loc[df['agent_assigned'].notna(), 'agent_assigned'] = df.loc[df['agent_assigned'].notna(), 'agent_assigned'].astype(int)\n",
    "\n",
    "                    print(\"Converted agent_assigned column to int.\")\n",
    "\n",
    "            filtered_ids = previously_assigned[0].astype(int).values\n",
    "            main_data_to_assign = df.loc[~df['main_system_id'].isin(filtered_ids) | (df['main_system_id'] == 1)\n",
    "            ]\n",
    "            if print_it:\n",
    "                print(f\"Available tasks for this batch after filtering. Rows remaining: {len(main_data_to_assign)}\")\n",
    "                print(f\" Retailers that have no aggents to assign to {main_data_to_assign['agent_assigned'].isna().sum()}\")\n",
    "\n",
    "            main_data_to_assign['main_system_id'] = main_data_to_assign['main_system_id'].astype('int')\n",
    "            main_data_to_assign = main_data_to_assign.groupby('agent_assigned').head(assigns)\n",
    "\n",
    "            return main_data_to_assign\n",
    "\n",
    "        else:\n",
    "            print(\"Only one previously assigned entry, skipping filter.\")\n",
    "            df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "            main_data_to_assign = df.groupby('agent_assigned').head(assigns)\n",
    "            return main_data_to_assign\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] remove_assign failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7677631-809f-4a3b-8502-6e68c0cb9c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process at hour: 15\n",
      "Totale agents: 4\n",
      "Number of available agents: 4\n",
      "/home/ec2-user/service_account_key.json\n",
      "Fetching data from 18 queries...\n",
      "Query 35981 returned 63 records\n",
      "Query 38188 returned 61 records\n",
      "Query 59874 returned 62 records\n",
      "WARNING: Queries [59703, 36299, 49423, 49557, 49556, 39940, 49566, 55124, 55045, 56316, 59170, 59170, 59188, 60326] returned empty dataframe!\n",
      "/home/ec2-user/service_account_key.json\n",
      "/home/ec2-user/service_account_key.json\n",
      "Query 59585 returned 2053 records\n",
      "Mapping tasks available: 2053\n",
      "Random tasks available: 186\n",
      "Removed 1 blacklisted retailers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10528/550612972.py:221: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(str)\n",
      "/tmp/ipykernel_10528/550612972.py:224: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].str.replace(',', '')\n",
      "/tmp/ipykernel_10528/550612972.py:227: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype('Int64', errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2053 mapping records\n",
      "Successfully mapped 1028 records to agents\n",
      "Retailers unassigned: 1025\n",
      "Total assignments available (including ones already assigned): 2173\n",
      "/home/ec2-user/service_account_key.json\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'remove_assign' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 133\u001b[0m\n\u001b[1;32m    131\u001b[0m     sheet_df \u001b[38;5;241m=\u001b[39m final_data_to_assign[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_system_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretailer_mobile_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretailer_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_assigned\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded_at\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    132\u001b[0m     sheet_df\n\u001b[0;32m--> 133\u001b[0m \u001b[43mtask_based_assignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 100\u001b[0m, in \u001b[0;36mtask_based_assignment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m df \u001b[38;5;241m=\u001b[39m main_data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Remove previous assignments and limit retailers per agent\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m main_data_to_assign \u001b[38;5;241m=\u001b[39m \u001b[43mremove_assign\u001b[49m(already_assigned_df, df, \u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m    101\u001b[0m final_old_assign_new \u001b[38;5;241m=\u001b[39m remove_assign(already_assigned_df, final_old_assign, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Filter for available agents\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_assign' is not defined"
     ]
    }
   ],
   "source": [
    "def task_based_assignment():\n",
    "    # ----------------------------------------\n",
    "    # Initialize and get current time\n",
    "    # ----------------------------------------\n",
    "    initialize_env()\n",
    "    now = datetime.now() + timedelta(hours=3)\n",
    "    hour = int(str(now.time())[0:2])\n",
    "    print(f\"Starting process at hour: {hour}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Get available agents\n",
    "    # ----------------------------------------\n",
    "    attendance = ret_metabase(\"EGYPT\", 13502, initialized=True)\n",
    "    print(f\"Totale agents: {len(attendance)}\")\n",
    "    task_based_list = get_available_agents(attendance, hour)\n",
    "    time.sleep(15)\n",
    "    \n",
    "    if not task_based_list:\n",
    "        print(\"No agents available for current hour!\")\n",
    "        return\n",
    "        \n",
    "    # ----------------------------------------\n",
    "    # Get query IDs and blacklisted retailers\n",
    "    # ----------------------------------------\n",
    "    query_ids = google_sheets('Query ID Assigning', 'Sheet1', 'get')\n",
    "    blacklisted_retailers = query_ids['Blacklisted_retailers'].dropna().astype(int).tolist()\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Fetch and process query data\n",
    "    # ----------------------------------------\n",
    "    df_raw_R, df_raw_M = fetch_and_process_queries(query_ids, blacklisted_retailers)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Process previous assignments / For follow up calls\n",
    "    # ----------------------------------------\n",
    "    previous_calls = ret_metabase(\"EGYPT\", 35299, initialized=True)\n",
    "    previous_calls = clean_column_id(previous_calls, 'main_system_id')\n",
    "    task_based, final_old_assign = process_previous_assignments(df_raw_R, previous_calls)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Assign agents to tasks\n",
    "    # ----------------------------------------\n",
    "    main_data_list = []\n",
    "    \n",
    "    # Get and process mapping data\n",
    "    mapping_df = ret_metabase('Egypt',59587)\n",
    "    mapping_df = clean_column_id(mapping_df, 'MAIN_SYSTEM_ID')\n",
    "    mapping_df = clean_column_id(mapping_df, 'AGENT_ID')\n",
    "    \n",
    "    if mapping_df.empty:\n",
    "        print(\"WARNING: No mapping data found!\")\n",
    "    else:\n",
    "        print(f\"Found {len(mapping_df)} mapping records\")\n",
    "        mapping_df = mapping_df[mapping_df['AGENT_ID'].isin(task_based_list)]\n",
    "        mapped_data = assign_data_by_mapping(df_raw_M, mapping_df)\n",
    "        \n",
    "        if mapped_data.empty:\n",
    "            print(\"WARNING: No data mapped to agents!\")\n",
    "        else:\n",
    "            notna = mapped_data['agent_assigned'].notna().sum()\n",
    "            na = mapped_data['agent_assigned'].isna().sum()\n",
    "            print(f\"Successfully mapped {notna} records to agents\")\n",
    "            print(f\"Retailers unassigned: {na}\")\n",
    "        main_data_list.append(mapped_data)\n",
    "    \n",
    "    # Process tasks by priority\n",
    "    priority_range = range(1, 16)  # 1 to 15 inclusive\n",
    "    for priority in priority_range:\n",
    "        priority_df = task_based[task_based[\"priority\"] == priority].reset_index(drop=True)\n",
    "        main_data = assign_data_equal_projects(priority_df, task_based_list)\n",
    "        main_data_list.append(main_data)\n",
    "    \n",
    "    main_data_total = pd.concat(main_data_list, ignore_index=True)\n",
    "    print(f\"Total assignments available (including ones already assigned): {main_data_total.shape[0]}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Clean and filter data\n",
    "    # ----------------------------------------\n",
    "    # Select required columns\n",
    "    main_data = main_data_total[\n",
    "        [\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"project_name\"]]\n",
    "    \n",
    "    # Handle special case for main_system_id == 1\n",
    "    main_system_id_1 = main_data[main_data['main_system_id'] == 1]\n",
    "    other_main_system_ids = main_data[main_data['main_system_id'] != 1]\n",
    "    other_main_system_ids = other_main_system_ids.drop_duplicates(subset=['main_system_id'])\n",
    "    main_data = pd.concat([main_system_id_1, other_main_system_ids], ignore_index=True)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Process previous assignments\n",
    "    # ----------------------------------------\n",
    "    sheet = google_sheets('[HOURLY] TASK-BASED Data', 'Data', 'get', [6])\n",
    "    already_assigned = [[str(val)] for val in sheet.iloc[1:, 0].dropna()]\n",
    "    already_assigned_df = pd.DataFrame.from_dict(already_assigned)\n",
    "    \n",
    "    df = main_data.copy()\n",
    "    \n",
    "    # Remove previous assignments and limit retailers per agent\n",
    "    \n",
    "    main_data_to_assign = remove_assign(already_assigned_df, df, 40)\n",
    "    final_old_assign_new = remove_assign(already_assigned_df, final_old_assign, 5)\n",
    "    \n",
    "    # Filter for available agents\n",
    "    filtered_df = main_data_to_assign[main_data_to_assign['agent_assigned'].isin(task_based_list)]\n",
    "    filtered_df_old = final_old_assign_new[final_old_assign_new['agent_assigned'].isin(task_based_list)]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        print(\"WARNING: No assignments after filtering!\")\n",
    "    else:\n",
    "        print(f\"Number of filtered assignments: {len(filtered_df)}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Prepare final data\n",
    "    # ----------------------------------------\n",
    "    final_data_to_assign = filtered_df.drop_duplicates(subset='main_system_id', keep='first').copy()\n",
    "    final_old_assign_new = filtered_df_old.drop_duplicates(subset='main_system_id', keep='first').copy()\n",
    "    \n",
    "    final_data_to_assign['added_at'] = now\n",
    "    final_old_assign_new['added_at'] = now\n",
    "    \n",
    "    final_data_to_assign = final_data_to_assign.drop(columns='index', errors='ignore')\n",
    "    final_data_to_assign = ensure_correct_dispatching(final_data_to_assign, task_based_list, final_old_assign_new)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Save data to various destinations\n",
    "    # ----------------------------------------\n",
    "    print(\"Starting data export process...\")\n",
    "    \n",
    "    # Save to parquet\n",
    "    final_data_to_assign = final_data_to_assign.astype(str)\n",
    "    sheet_df = final_data_to_assign[[\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"added_at\"]]\n",
    "    sheet_df\n",
    "task_based_assignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f692134-b355-46dc-8ede-69ce8b555f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2053 mapping records\n"
     ]
    }
   ],
   "source": [
    "# data = ret_metabase('MOROCCO', 10728)\n",
    "\n",
    "mapping_df = ret_metabase('Egypt',59587)\n",
    "# mapping_df = google_sheets('EG Telesales Cycle Assignment', 'Sheet1', 'get')\n",
    "mapping_df = clean_column_id(mapping_df, 'MAIN_SYSTEM_ID')\n",
    "mapping_df = clean_column_id(mapping_df, 'AGENT_ID')\n",
    "\n",
    "if mapping_df.empty:\n",
    "    print(\"WARNING: No mapping data found!\")\n",
    "else:\n",
    "    print(f\"Found {len(mapping_df)} mapping records\")\n",
    "    # mapping_df = mapping_df[mapping_df['agent_assigned'].isin(task_based_list)]\n",
    "    # mapped_data = assign_data_by_mapping(df_raw_M, mapping_df)\n",
    "\n",
    "    # if mapped_data.empty:\n",
    "    #     print(\"WARNING: No data mapped to agents!\")\n",
    "    # else:\n",
    "    #     notna = mapped_data['agent_assigned'].notna().sum()\n",
    "    #     na = mapped_data['agent_assigned'].isna().sum()\n",
    "    #     print(f\"Successfully mapped {notna} records to agents\")\n",
    "    #     print(f\"Retailers unassigned: {na}\")\n",
    "    # main_data_list.append(mapped_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
