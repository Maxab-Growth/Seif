{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e5a364-d408-4a25-a886-e5d40af86ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gspread in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (6.2.1)\n",
      "Requirement already satisfied: google-auth>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gspread) (2.40.2)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gspread) (1.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth>=1.12.0->gspread) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.1.31)\n",
      "Requirement already satisfied: oauth2client in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.1.3)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client) (0.22.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client) (0.4.2)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client) (4.7.2)\n",
      "Requirement already satisfied: six>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client) (1.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httplib2>=0.9.1->oauth2client) (3.2.3)\n",
      "Requirement already satisfied: slackclient in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.9.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>3.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from slackclient) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>3.5.2->slackclient) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>3.5.2->slackclient) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>3.5.2->slackclient) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>3.5.2->slackclient) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>3.5.2->slackclient) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>3.5.2->slackclient) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>3.5.2->slackclient) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>3.5.2->slackclient) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp<4.0.0,>3.5.2->slackclient) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4.0.0,>3.5.2->slackclient) (3.10)\n",
      "Requirement already satisfied: snowflake-connector-python in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.15.0)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: boto3>=1.24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (1.38.1)\n",
      "Requirement already satisfied: botocore>=1.24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (1.38.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (44.0.2)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (25.0.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (2.10.1)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (2025.2)\n",
      "Requirement already satisfied: requests<3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (2.32.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (24.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (4.13.2)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (3.16.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (4.3.7)\n",
      "Requirement already satisfied: tomlkit in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python) (0.13.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.24->snowflake-connector-python) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.24->snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.17.0)\n",
      "Requirement already satisfied: snowflake-snowpark-python in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.32.0)\n",
      "Requirement already satisfied: setuptools>=40.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python) (79.0.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python) (0.45.1)\n",
      "Requirement already satisfied: snowflake-connector-python<4.0.0,>=3.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python) (4.13.2)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python) (6.0.2)\n",
      "Requirement already satisfied: cloudpickle!=2.1.0,!=2.2.0,<=3.0.0,>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python) (3.0.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python) (5.29.4)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python) (2.9.0.post0)\n",
      "Requirement already satisfied: tzlocal in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-snowpark-python) (5.3.1)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (1.5.1)\n",
      "Requirement already satisfied: boto3>=1.24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (1.38.1)\n",
      "Requirement already satisfied: botocore>=1.24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (1.38.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (44.0.2)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (25.0.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2.10.1)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2025.2)\n",
      "Requirement already satisfied: requests<3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2.32.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (24.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2025.1.31)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (3.16.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (4.3.7)\n",
      "Requirement already satisfied: tomlkit in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (0.13.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil->snowflake-snowpark-python) (1.17.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.24->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.24->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (0.12.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.24->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2.4.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2.22)\n",
      "Requirement already satisfied: psycopg2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.9.10)\n",
      "Requirement already satisfied: sqlalchemy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.0.41)\n",
      "Requirement already satisfied: greenlet>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy) (4.13.2)\n",
      "Requirement already satisfied: df2gspread in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.0.4)\n",
      "Collecting argparse>=1.3.0 (from df2gspread)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: google-api-python-client==1.6.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from df2gspread) (1.6.7)\n",
      "Requirement already satisfied: gspread>=2.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from df2gspread) (6.2.1)\n",
      "Requirement already satisfied: oauth2client<5.0.0dev,>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from df2gspread) (4.1.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from df2gspread) (1.5.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-api-python-client==1.6.7->df2gspread) (0.22.0)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-api-python-client==1.6.7->df2gspread) (1.17.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-api-python-client==1.6.7->df2gspread) (3.0.1)\n",
      "Requirement already satisfied: google-auth>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gspread>=2.1.1->df2gspread) (2.40.2)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gspread>=2.1.1->df2gspread) (1.2.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client<5.0.0dev,>=1.5.0->df2gspread) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client<5.0.0dev,>=1.5.0->df2gspread) (0.4.2)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from oauth2client<5.0.0dev,>=1.5.0->df2gspread) (4.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->df2gspread) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->df2gspread) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->df2gspread) (1.26.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth>=1.12.0->gspread>=2.1.1->df2gspread) (5.5.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth-oauthlib>=0.4.1->gspread>=2.1.1->df2gspread) (2.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.6.7->df2gspread) (3.2.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=2.1.1->df2gspread) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=2.1.1->df2gspread) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=2.1.1->df2gspread) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=2.1.1->df2gspread) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=2.1.1->df2gspread) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=2.1.1->df2gspread) (2025.1.31)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install gspread\n",
    "!pip install oauth2client\n",
    "!pip install slackclient\n",
    "!pip install -U snowflake-connector-python\n",
    "!pip install -U snowflake-snowpark-python\n",
    "!pip install --upgrade psycopg2\n",
    "!pip install -U sqlalchemy\n",
    "!pip install df2gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41e7cf80-ac47-4c1e-b4c5-84fa231f193a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from snowflake.snowpark import Session \n",
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "import json\n",
    "from requests import get\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date\n",
    "import time\n",
    "import zipfile\n",
    "import io\n",
    "import sqlalchemy\n",
    "from pathlib import Path\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import sys\n",
    "import requests\n",
    "from botocore.exceptions import ClientError\n",
    "import importlib\n",
    "from io import StringIO\n",
    "import snowflake.connector\n",
    "from sqlalchemy import create_engine\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import re\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d2c17f-d181-41f1-863b-61c4bfa9df56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import base64\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "from requests import get\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def imports():\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import psycopg2\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException':\n",
    "            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException':\n",
    "            # An error occurred on the server side.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException':\n",
    "            # You provided an invalid value for a parameter.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException':\n",
    "            # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            # We can't find the resource that you asked for.\n",
    "            # Deal with the exception here, and/or rethrow at your discretion.\n",
    "            raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            return get_secret_value_response['SecretString']\n",
    "        else:\n",
    "            return base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "        \n",
    "def initialize_env():\n",
    "    snowflake_sg_secret = json.loads(get_secret(\"Snowflake-sagemaker\"))\n",
    "    slack_secret = json.loads(get_secret(\"prod/slack/reports\"))\n",
    "    rowaa_metabase_access = json.loads(get_secret(\"prod/metabase/rowaa/user\"))\n",
    "    dwh_writer_secret = json.loads(get_secret(\"prod/db/datawarehouse/sagemaker\"))\n",
    "\n",
    "    os.environ[\"SNOWFLAKE_USERNAME\"] = snowflake_sg_secret[\"username\"]\n",
    "    os.environ[\"SNOWFLAKE_PASSWORD\"] = snowflake_sg_secret[\"password\"]\n",
    "    os.environ[\"SNOWFLAKE_ACCOUNT\"] = snowflake_sg_secret[\"account\"]\n",
    "    os.environ[\"SNOWFLAKE_DATABASE\"] = snowflake_sg_secret[\"database\"]\n",
    "\n",
    "    os.environ[\"SLACK_TOKEN\"] = slack_secret[\"token\"]\n",
    "\n",
    "    os.environ[\"METABASE_USERNAME_ROWAA\"] = rowaa_metabase_access[\"username\"]\n",
    "    os.environ[\"METABASE_PASSWORD_ROWAA\"] = rowaa_metabase_access[\"password\"]\n",
    "\n",
    "    os.environ[\"DWH_WRITER_HOST_NEW\"] = dwh_writer_secret[\"host\"]\n",
    "    os.environ[\"DWH_WRITER_NAME_NEW\"] = dwh_writer_secret[\"dbname\"]\n",
    "    os.environ[\"DWH_WRITER_USER_NAME_NEW\"] = dwh_writer_secret[\"username\"]\n",
    "    os.environ[\"DWH_WRITER_PASSWORD_NEW\"] = dwh_writer_secret[\"password\"] \n",
    "\n",
    "    json_path_sheets = str(Path.home()) + \"/service_account_key_sheets.json\"\n",
    "    sheets_key = get_secret(\"prod/maxab-sheets\")\n",
    "    f = open(json_path_sheets, \"w\")\n",
    "    f.write(sheets_key)\n",
    "    f.close()\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"] = json_path_sheets\n",
    "    \n",
    "\n",
    "def get_from_gsheet(workbook, sheet):\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "         'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\",\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "    initialize_env()\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"], scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    try:\n",
    "        wks = client.open(workbook).worksheet(sheet)\n",
    "        sheet = pd.DataFrame(wks.get_all_records())\n",
    "    except:\n",
    "        print(sheet,'failed to fetch data')\n",
    "    \n",
    "    return sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdde1c8-3228-445e-a845-4342aba93cd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19a379a-8074-449c-acfc-25038495e129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "def ret_metabase(username, password, question):\n",
    "    base_url = 'https://bi.maxab.info/api'\n",
    "    base_headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    try:\n",
    "        s_response = requests.post(\n",
    "            base_url + '/session',\n",
    "            data=json.dumps({\n",
    "                'username': username,\n",
    "                'password': password\n",
    "            }),\n",
    "            headers=base_headers)\n",
    "        s_response.raise_for_status()\n",
    "\n",
    "        session_token = s_response.json()['id']\n",
    "        base_headers['X-Metabase-Session'] = session_token\n",
    "\n",
    "        p_response = requests.post(base_url + '/card/' + str(question) + '/query/csv', headers=base_headers)\n",
    "        p_response.raise_for_status()\n",
    "\n",
    "        my_dict = p_response.content\n",
    "        s = str(my_dict, 'utf-8')\n",
    "        my_dict = StringIO(s)\n",
    "        df = pd.read_csv(my_dict)\n",
    "        return(df)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(e)\n",
    "\n",
    "    \n",
    "#run query save on metabse with ID, \n",
    "#for example here query 1606 is save in my personal collection and I can run it like line below\n",
    "# ret_metabase(1606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8de4cc31-c812-440f-8527-5fdaa04fd61d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_data_equal_projects(df, list, assigns):\n",
    "    df = df.sample(frac=1)  # Shuffle the data\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    assigned_data = pd.DataFrame()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.reset_index(drop=True)\n",
    "        rows_per_agent = len(project_df) // len(list)\n",
    "        remainder = len(project_df) % len(list)\n",
    "        \n",
    "        # Distribute rows equally\n",
    "        for i, agent in enumerate(list):\n",
    "            start_idx = i * rows_per_agent\n",
    "            end_idx = start_idx + rows_per_agent\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            # Handle remainder\n",
    "            if i < remainder:\n",
    "                extra_row = project_df.iloc[end_idx:end_idx+1].copy()\n",
    "                extra_row['agent_assigned'] = agent\n",
    "                agent_data = pd.concat([agent_data, extra_row])\n",
    "            \n",
    "            assigned_data = pd.concat([assigned_data, agent_data])\n",
    "    \n",
    "    assigned_data = assigned_data.reset_index(drop=True)\n",
    "    \n",
    "    return assigned_data\n",
    "\n",
    "\n",
    "def clean_column_id(df, column_name):\n",
    "    # Ensure the column is treated as a string\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    \n",
    "    # Replace commas in the string\n",
    "    df[column_name] = df[column_name].str.replace(',', '')\n",
    "    \n",
    "    # Convert back to an integer, if appropriate\n",
    "    df[column_name] = df[column_name].astype('Int64', errors='ignore')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d6a966-f8e0-49cb-9324-31a467356834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assign tasks by mapping \n",
    "def assign_data_by_mapping(df, mapping_df):\n",
    "    # store retail-agent mapping in a dictionary \n",
    "    mapping_dict = dict(zip(mapping_df['main_system_id'], mapping_df['agent_assigned']))\n",
    "    \n",
    "    \n",
    "    assigned_agents = []\n",
    "    # missing_retailers = [] could use it later for viewing errors\n",
    "\n",
    "    for retailer_id in df['main_system_id']:\n",
    "        if retailer_id in mapping_dict:\n",
    "            assigned_agents.append(mapping_dict[retailer_id])\n",
    "        else:\n",
    "            assigned_agents.append(None)\n",
    "            \n",
    "\n",
    "    \n",
    "    df['agent_assigned'] = assigned_agents\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20837716-4453-4b08-8791-e0b7a2b31c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_env():\n",
    "    snowflake_sg_secret = json.loads(get_secret(\"Snowflake-sagemaker\"))\n",
    "    slack_secret = json.loads(get_secret(\"prod/slack/reports\"))\n",
    "    fintech_service_account = json.loads(get_secret(\"prod/fintechServiceEmail/credentials\"))\n",
    "    dwh_writer_secret = json.loads(get_secret(\"prod/db/datawarehouse/sagemaker\"))\n",
    "\n",
    "    os.environ[\"SNOWFLAKE_USERNAME\"] = snowflake_sg_secret[\"username\"]\n",
    "    os.environ[\"SNOWFLAKE_PASSWORD\"] = snowflake_sg_secret[\"password\"]\n",
    "    os.environ[\"SNOWFLAKE_ACCOUNT\"] = snowflake_sg_secret[\"account\"]\n",
    "    os.environ[\"SNOWFLAKE_DATABASE\"] = snowflake_sg_secret[\"database\"]\n",
    "\n",
    "    os.environ[\"SLACK_TOKEN\"] = slack_secret[\"token\"]\n",
    "\n",
    "    os.environ[\"FINTECH_EMONEY_EMAIL\"] = fintech_service_account[\"email_name\"]\n",
    "    os.environ[\"FINTECH_EMONEY_PASSWORD\"] = fintech_service_account[\"email_password\"]\n",
    "\n",
    "    metabase_secret = json.loads(get_secret(\"prod/metabase/maxab_config\"))\n",
    "    os.environ[\"EGYPT_METABASE_USERNAME\"] = metabase_secret[\"metabase_user\"]\n",
    "    os.environ[\"EGYPT_METABASE_PASSWORD\"] = metabase_secret[\"metabase_password\"]\n",
    "\n",
    "    os.environ[\"DWH_WRITER_HOST_NEW\"] = dwh_writer_secret[\"host\"]\n",
    "    os.environ[\"DWH_WRITER_NAME_NEW\"] = dwh_writer_secret[\"dbname\"]\n",
    "    os.environ[\"DWH_WRITER_USER_NAME_NEW\"] = dwh_writer_secret[\"username\"]\n",
    "    os.environ[\"DWH_WRITER_PASSWORD_NEW\"] = dwh_writer_secret[\"password\"] \n",
    "\n",
    "    json_path_sheets = str(Path.home()) + \"/service_account_key_sheets.json\"\n",
    "    sheets_key = get_secret(\"prod/maxab-sheets\")\n",
    "    f = open(json_path_sheets, \"w\")\n",
    "    f.write(sheets_key)\n",
    "    f.close()\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"] = json_path_sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e8934e5-d82b-4be4-9664-5a15be05c3df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_assign(previously_assigned, df, assigns):\n",
    "    if  previously_assigned.shape[0] != 1:\n",
    "        previously_assigned[0] = previously_assigned[0].fillna('').astype(str).str.replace(\" \",\"\",regex=False)\n",
    "        previously_assigned = previously_assigned.dropna()\n",
    "        previously_assigned[0] = previously_assigned[0].astype('float')\n",
    "        previously_assigned[0] = previously_assigned[0].astype('int')\n",
    "        df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "        if df['agent_assigned'].dtype == 'object':\n",
    "            df['agent_assigned'] = df['agent_assigned'].astype('int')\n",
    "\n",
    "        # Filter out previously assigned IDs but keep main_system_id == 1\n",
    "        main_data_to_assign = pd.DataFrame(\n",
    "            df.loc[~df['main_system_id'].isin(previously_assigned[0].astype(int).values) | (df['main_system_id'] == 1)]\n",
    "        )    \n",
    "\n",
    "        main_data_to_assign['main_system_id'] = main_data_to_assign['main_system_id'].astype('int')\n",
    "        main_data_to_assign = main_data_to_assign.groupby('agent_assigned').head(assigns)\n",
    "        return main_data_to_assign\n",
    "    else:\n",
    "        df['main_system_id'] = df['main_system_id'].astype('int')\n",
    "        main_data_to_assign = df.groupby('agent_assigned').head(assigns)\n",
    "        return main_data_to_assign\n",
    "\n",
    "def assign_data_equal_projects(df, list, assigns):\n",
    "    df = df.sample(frac=1)  # Shuffle the data\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    assigned_data = pd.DataFrame()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.reset_index(drop=True)\n",
    "        rows_per_agent = len(project_df) // len(list)\n",
    "        remainder = len(project_df) % len(list)\n",
    "        \n",
    "        # Distribute rows equally\n",
    "        for i, agent in enumerate(list):\n",
    "            start_idx = i * rows_per_agent\n",
    "            end_idx = start_idx + rows_per_agent\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            # Handle remainder\n",
    "            if i < remainder:\n",
    "                extra_row = project_df.iloc[end_idx:end_idx+1].copy()\n",
    "                extra_row['agent_assigned'] = agent\n",
    "                agent_data = pd.concat([agent_data, extra_row])\n",
    "            \n",
    "            assigned_data = pd.concat([assigned_data, agent_data])\n",
    "    \n",
    "    assigned_data = assigned_data.reset_index(drop=True)\n",
    "    \n",
    "    return assigned_data\n",
    "\n",
    "def check_distribution(df, agent_list):\n",
    "    \"\"\"\n",
    "    Checks if the distribution of project types across agents is even.\n",
    "    Returns a dictionary with the count of rows per agent for each project type.\n",
    "    \"\"\"\n",
    "    distribution = {}\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        distribution[project] = project_df['agent_assigned'].value_counts().reindex(agent_list, fill_value=0)\n",
    "    \n",
    "    return distribution\n",
    "\n",
    "def redistribute_rows(df, agent_list):\n",
    "    \"\"\"\n",
    "    Redistributes rows among agents if the distribution is uneven.\n",
    "    \"\"\"\n",
    "    project_types = df['project_name'].unique()\n",
    "    redistributed_data = pd.DataFrame()\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        project_df = project_df.sample(frac=1).reset_index(drop=True)  # Shuffle data\n",
    "        rows_per_agent = len(project_df) // len(agent_list)\n",
    "        remainder = len(project_df) % len(agent_list)\n",
    "        \n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, agent in enumerate(agent_list):\n",
    "            end_idx = start_idx + rows_per_agent + (1 if i < remainder else 0)\n",
    "            agent_data = project_df.iloc[start_idx:end_idx].copy()\n",
    "            agent_data['agent_assigned'] = agent\n",
    "            \n",
    "            redistributed_data = pd.concat([redistributed_data, agent_data])\n",
    "            \n",
    "            start_idx = end_idx\n",
    "    \n",
    "    redistributed_data = redistributed_data.reset_index(drop=True)\n",
    "    \n",
    "    return redistributed_data\n",
    "\n",
    "def ensure_correct_dispatching(df, agent_list, final_old_assign):\n",
    "    \"\"\"\n",
    "    Ensures the dispatching is done correctly by checking and redistributing rows if necessary.\n",
    "    `final_old_assign` rows are excluded from redistribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Exclude final_old_assign from the distribution check, but keep rows with main_system_id == 1\n",
    "    new_assignments = df[~df['main_system_id'].isin(final_old_assign['main_system_id']) | (df['main_system_id'] == 1)]\n",
    "    \n",
    "    distribution = check_distribution(new_assignments, agent_list)\n",
    "    \n",
    "    # Check if any project has an uneven distribution across agents\n",
    "    uneven_distribution = any(distribution[project].nunique() > 1 for project in distribution)\n",
    "    \n",
    "    if uneven_distribution:\n",
    "        print(\"Uneven distribution detected. Redistributing rows...\")\n",
    "        new_assignments = redistribute_rows(new_assignments, agent_list)\n",
    "    else:\n",
    "        print(\"Distribution is even. No redistribution needed.\")\n",
    "    \n",
    "    # Combine the redistributed new assignments with the final_old_assign\n",
    "    final_data = pd.concat([final_old_assign, new_assignments], ignore_index=True)\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def check_distribution_df(df, agent_list):\n",
    "    \"\"\"\n",
    "    Checks if the distribution of project types across agents is even.\n",
    "    Returns a DataFrame with the count of rows per agent for each project type.\n",
    "    \"\"\"\n",
    "    project_types = df['project_name'].unique()\n",
    "    \n",
    "    distribution_data = []\n",
    "    current_time = datetime.now()  # Get the current datetime\n",
    "    \n",
    "    for project in project_types:\n",
    "        project_df = df[df['project_name'] == project]\n",
    "        agent_counts = project_df['agent_assigned'].value_counts().reindex(agent_list, fill_value=0)\n",
    "        \n",
    "        for agent, count in agent_counts.items():\n",
    "            distribution_data.append({\n",
    "                'project_type': project,\n",
    "                'agent_assigned': agent,\n",
    "                'count': count,\n",
    "                'datetime': dt.datetime.now()  + timedelta(hours=3)\n",
    "            })\n",
    "    \n",
    "    distribution_df = pd.DataFrame(distribution_data)\n",
    "    \n",
    "    return distribution_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f59641f-771a-489e-ae65-7d7e834b62df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13779/4143388731.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(str)\n",
      "/tmp/ipykernel_13779/4143388731.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].str.replace(',', '')\n",
      "/tmp/ipykernel_13779/4143388731.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype('Int64', errors='ignore')\n",
      "/tmp/ipykernel_13779/84675118.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['main_system_id'] = df['main_system_id'].astype('int')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uneven distribution detected. Redistributing rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13779/84675118.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['main_system_id'] = df['main_system_id'].astype('int')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uneven distribution detected. Redistributing rows...\n",
      "    main_system_id                             retailer_mobile_number  \\\n",
      "0           869000  B40B37323944B913E208D8-3B237F7DA755BFCF33A4654...   \n",
      "1           868486  B40B37313841BE13E60BDE-15054073F3812CD90D38D7F...   \n",
      "2           869473  B40B35323E41B91BE606DD-59FE7BD74FBB132B70E66A2...   \n",
      "3           868815  B40B36343C4AB812E20CDD-4FED25BAE91E52A4620E07F...   \n",
      "4           868963  B40B37373D41B015E10EDE-5AB1DCBB57229C5912C92AB...   \n",
      "..             ...                                                ...   \n",
      "129         872258  B40B37313B41B815E20CDD-A835ACEADF7BDA45234E4FD...   \n",
      "130         873374  B40B36303C44B015E00FD8-FC757FB33D0C659A2C5007D...   \n",
      "131          79148  B40B37303C4BBC17E60FDF-E33B7EE22882E7BDA26FD01...   \n",
      "132          70284  B40B37323F41B916EC07DD-452AFF9812B233BDFAC6CFE...   \n",
      "133         680873  B40B36313C47BF12E40DDF-53FD68EE1D27C25011CDE17...   \n",
      "\n",
      "                retailer_name  \\\n",
      "0              احمد مصلح شحات   \n",
      "1           Ahmed AbdElhakeem   \n",
      "2                       selim   \n",
      "3            سعودي احمد محمد    \n",
      "4                    أحمد رجب   \n",
      "..                        ...   \n",
      "129  احمد عبدالديان عبدالستار   \n",
      "130      اسلام محمد حسين صالح   \n",
      "131                     محمود   \n",
      "132                 احمد مشعل   \n",
      "133                 احمد نجيب   \n",
      "\n",
      "                                           description reward balance offer  \\\n",
      "0    project:app acquisition هينزلك 10 جنيه كاشباك ...      0       0     0   \n",
      "1    project:app acquisition هينزلك 10 جنيه كاشباك ...      0       0     0   \n",
      "2    project:app acquisition هينزلك 10 جنيه كاشباك ...      0       0     0   \n",
      "3    project:app acquisition هينزلك 10 جنيه كاشباك ...      0       0     0   \n",
      "4    project:app acquisition هينزلك 10 جنيه كاشباك ...      0       0     0   \n",
      "..                                                 ...    ...     ...   ...   \n",
      "129  Project:POS Onboarding+ لحظي +High Value Distr...      0       0     0   \n",
      "130  Project:POS Onboarding+ لحظي +High Value Distr...      0       0     0   \n",
      "131  Project:POS Onboarding+ عادي +High Value District      0       0     0   \n",
      "132                       Project:POS Onboarding+ لحظي      0       0     0   \n",
      "133  Project:POS Onboarding+ لحظي +High Value Distr...      0       0     0   \n",
      "\n",
      "    agent_assigned                    added_at  \n",
      "0             2648  2025-05-29 15:52:11.594719  \n",
      "1             2648  2025-05-29 15:52:11.594719  \n",
      "2             2648  2025-05-29 15:52:11.594719  \n",
      "3             2648  2025-05-29 15:52:11.594719  \n",
      "4             2648  2025-05-29 15:52:11.594719  \n",
      "..             ...                         ...  \n",
      "129           5280  2025-05-29 15:52:11.594719  \n",
      "130           5280  2025-05-29 15:52:11.594719  \n",
      "131           5565  2025-05-29 15:52:11.594719  \n",
      "132           5565  2025-05-29 15:52:11.594719  \n",
      "133           5565  2025-05-29 15:52:11.594719  \n",
      "\n",
      "[134 rows x 9 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1hOdDpkJrLoUCxkY22H5xWatkziDoVT00btULtJkahNU',\n",
       " 'tableRange': 'Sheet1!A1:D115',\n",
       " 'updates': {'spreadsheetId': '1hOdDpkJrLoUCxkY22H5xWatkziDoVT00btULtJkahNU',\n",
       "  'updatedRange': 'Sheet1!A116:D139',\n",
       "  'updatedRows': 24,\n",
       "  'updatedColumns': 4,\n",
       "  'updatedCells': 96}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_env()\n",
    "now = datetime.now() + timedelta(hours=3)\n",
    "hour = int(str(now.time())[0:2])\n",
    "\n",
    "attendance = ret_metabase(os.environ[\"EGYPT_METABASE_USERNAME\"], os.environ[\"EGYPT_METABASE_PASSWORD\"], 13502)\n",
    "attendance_copy = attendance.copy()\n",
    "\n",
    "attendance_copy['start_time'] = attendance_copy['start_time'].astype(int)\n",
    "attendance_copy['end_time'] = attendance_copy['end_time'].astype(int)\n",
    "\n",
    "attendance_copy['assignment_start_time'] = attendance_copy['start_time'] - 1\n",
    "attendance_copy['assignment_end_time'] = attendance_copy['end_time'] - 1\n",
    "\n",
    "attendance_copy['assign_data'] = np.where(\n",
    "    (hour >= attendance_copy['assignment_start_time']) & (hour <= attendance_copy['assignment_end_time']),\n",
    "    'yes',\n",
    "    'no')\n",
    "\n",
    "task_based_agents = attendance_copy.loc[\n",
    "    (attendance_copy['project'] == 'task_based') & (attendance_copy['assign_data'] == 'yes')\n",
    "]\n",
    "\n",
    "task_based_list = task_based_agents['agent_id'].values.tolist()\n",
    "number_of_task_based_agents = len(task_based_list)\n",
    "\n",
    "time.sleep(15)\n",
    "\n",
    "if number_of_task_based_agents != 0:\n",
    "    # setup the environment:\n",
    "    initialize_env()\n",
    "\n",
    "    scope = [\n",
    "        \"https://spreadsheets.google.com/feeds\",\n",
    "        'https://www.googleapis.com/auth/spreadsheets',\n",
    "        \"https://www.googleapis.com/auth/drive.file\",\n",
    "        \"https://www.googleapis.com/auth/drive\"\n",
    "    ]\n",
    "\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_SHEETS\"], scope)\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    query_ids = [59874, 36299, 35981, 38188, 49423, 49557, 49556, 39940, 49566, 55124, 55045, 56316, 59170, 59188, 59874 ]\n",
    "\n",
    "    queries_for_mapped = ret_metabase(os.environ[\"EGYPT_METABASE_USERNAME\"], os.environ[\"EGYPT_METABASE_PASSWORD\"], 59585)\n",
    "    queries_for_mapped.columns = queries_for_mapped.columns.str.lower()\n",
    "\n",
    "    dataframes = [ret_metabase(os.environ[\"EGYPT_METABASE_USERNAME\"], os.environ[\"EGYPT_METABASE_PASSWORD\"], qid) for qid in query_ids]\n",
    "    for df in dataframes:\n",
    "        df.columns = map(str.lower, df.columns)\n",
    "\n",
    "    # Concatenate the DataFrames along the rows (union all)\n",
    "    df_unfiltered = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Excluding blacklisted retailers\n",
    "    blacklisted_retailers = [214101,663943,667569,523168,604291,525506,671719,266692,26115,373055]\n",
    "\n",
    "    df_raw = df_unfiltered[~df_unfiltered['main_system_id'].isin(blacklisted_retailers)]\n",
    "    df_raw = clean_column_id(df_raw, 'main_system_id')\n",
    "    queries_for_mapped = queries_for_mapped[~queries_for_mapped['main_system_id'].isin(blacklisted_retailers)]\n",
    "\n",
    "    # Fetching old assigns data\n",
    "\n",
    "    previous_calls = ret_metabase(os.environ[\"EGYPT_METABASE_USERNAME\"], os.environ[\"EGYPT_METABASE_PASSWORD\"], 35299)\n",
    "    previous_calls = clean_column_id(previous_calls, 'main_system_id')\n",
    "\n",
    "    df = df_raw.merge(previous_calls, on='main_system_id', how='left')\n",
    "    exclude_same_assigns = clean_column_id(previous_calls,'main_system_id')\n",
    "    task_based = pd.DataFrame(df_raw.loc[~df_raw['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "\n",
    "    old_assgns = pd.DataFrame(df_raw.loc[df_raw['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "    old_assgns = clean_column_id(old_assgns, 'main_system_id')\n",
    "    df_1 = old_assgns.merge(previous_calls, on='main_system_id', how='left')\n",
    "    final_old_assign = df_1[[\"main_system_id\", \"retailer_mobile_number\",\"retailer_name\",\"description\",\"reward\",\"balance\",\"offer\",\"agent_assigned\",\"project_name\"]]\n",
    "\n",
    "    # Define the range of priorities\n",
    "    priority_range = range(1, 16)  # 1 to 15 inclusive\n",
    "\n",
    "    # Initialize a list to store the results\n",
    "    main_data_list = []\n",
    "\n",
    "    mapping_sheet = client.open('EG Telesales Cycle Assignment')\n",
    "    mapping_worksheet = mapping_sheet.worksheet('Sheet1')  \n",
    "    mapping_data = mapping_worksheet.get_all_values()\n",
    "    mapping_df = pd.DataFrame(mapping_data[1:], columns=mapping_data[0])  # Convert to DataFrame first\n",
    "    mapping_df = clean_column_id(mapping_df, 'main_system_id')\n",
    "    mapping_df = clean_column_id(mapping_df, 'agent_assigned')\n",
    "\n",
    "    mapping_df = mapping_df[mapping_df['agent_assigned'].isin(task_based_list)]\n",
    "    mapped_data = assign_data_by_mapping(queries_for_mapped, mapping_df) \n",
    "    main_data_list.append(mapped_data)\n",
    "\n",
    "    # Iterate over each priority level\n",
    "    for priority in priority_range:\n",
    "        # Filter the DataFrame for the current priority\n",
    "        priority_df = task_based[task_based[\"priority\"] == priority].reset_index(drop=True)\n",
    "\n",
    "        # Assign data to projects\n",
    "        main_data = assign_data_equal_projects(priority_df, task_based_list, 2000)\n",
    "\n",
    "        # Append the result to the list\n",
    "        main_data_list.append(main_data)\n",
    "\n",
    "    # Concatenate all the main_data DataFrames into one\n",
    "    main_data_total = pd.concat(main_data_list, ignore_index=True)\n",
    "\n",
    "    # Filter the data to include the required columns\n",
    "    main_data = main_data_total[\n",
    "        [\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"project_name\"] ]\n",
    "\n",
    "    # Separate rows where main_system_id == 1\n",
    "    main_system_id_1 = main_data[main_data['main_system_id'] == 1]\n",
    "\n",
    "    # Remove duplicates for rows where main_system_id is NOT 1\n",
    "    other_main_system_ids = main_data[main_data['main_system_id'] != 1]\n",
    "    other_main_system_ids = other_main_system_ids.drop_duplicates(subset=['main_system_id'])\n",
    "\n",
    "    # Combine both parts back together\n",
    "    main_data = pd.concat([main_system_id_1, other_main_system_ids], ignore_index=True)\n",
    "\n",
    "\n",
    "    sheet = client.open('[HOURLY] TASK-BASED Data')\n",
    "    sheet_instance = sheet.worksheet('Data')\n",
    "    assignments = sheet_instance.get('G5:G')\n",
    "    assignments_df = pd.DataFrame.from_dict(assignments)\n",
    "\n",
    "    df = main_data.copy()\n",
    "\n",
    "    # Remove previous assignments and filter \n",
    "    main_data_to_assign = remove_assign(assignments_df, df, 40)\n",
    "    final_old_assign_new = remove_assign(assignments_df, final_old_assign, 5)\n",
    "\n",
    "    # Filter the dataframes based on 'agent_assigned' being in task_based_list\n",
    "    filtered_df = main_data_to_assign[main_data_to_assign['agent_assigned'].isin(task_based_list)]\n",
    "    filtered_df_old = final_old_assign_new[final_old_assign_new['agent_assigned'].isin(task_based_list)]\n",
    "\n",
    "    # Importing data into agents' sheet\n",
    "    final_data_to_assign = filtered_df.drop_duplicates(subset='main_system_id', keep='first').copy()\n",
    "    final_old_assign_new = filtered_df_old.drop_duplicates(subset='main_system_id', keep='first').copy()\n",
    "\n",
    "    # Add 'added_at' timestamp\n",
    "    final_data_to_assign['added_at'] = now\n",
    "    final_old_assign_new['added_at'] = now\n",
    "\n",
    "    # Remove 'index' column if it exists\n",
    "    final_data_to_assign = final_data_to_assign.drop(columns='index', errors='ignore')\n",
    "\n",
    "    final_data_to_assign = ensure_correct_dispatching(final_data_to_assign, task_based_list, final_old_assign_new)\n",
    "\n",
    "    final_data_to_assign = final_data_to_assign.astype(str)\n",
    "    sheet_df = final_data_to_assign[[\"main_system_id\", \"retailer_mobile_number\",\"retailer_name\",\"description\",\"reward\",\"balance\",\"offer\",\"agent_assigned\",\"added_at\"]]\n",
    "\n",
    "query_ids = [59703, 36299, 35981, 38188, 49423, 49557, 49556, 39940, 49566, 55124, 55045, 56316, 59170, 59188 ]\n",
    "\n",
    "queries_for_mapped = ret_metabase(os.environ[\"EGYPT_METABASE_USERNAME\"], os.environ[\"EGYPT_METABASE_PASSWORD\"], 59585)\n",
    "queries_for_mapped.columns = queries_for_mapped.columns.str.lower()\n",
    "\n",
    "dataframes = [ret_metabase(os.environ[\"EGYPT_METABASE_USERNAME\"], os.environ[\"EGYPT_METABASE_PASSWORD\"], qid) for qid in query_ids]\n",
    "for df in dataframes:\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "\n",
    "# Concatenate the DataFrames along the rows (union all)\n",
    "df_unfiltered = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Excluding blacklisted retailers\n",
    "blacklisted_retailers = [214101,663943,667569,523168,604291,525506,671719,266692,26115,373055]\n",
    "\n",
    "df_raw = df_unfiltered[~df_unfiltered['main_system_id'].isin(blacklisted_retailers)]\n",
    "df_raw = clean_column_id(df_raw, 'main_system_id')\n",
    "queries_for_mapped = queries_for_mapped[~queries_for_mapped['main_system_id'].isin(blacklisted_retailers)]\n",
    "\n",
    "# Fetching old assigns data\n",
    "\n",
    "previous_calls = ret_metabase(os.environ[\"EGYPT_METABASE_USERNAME\"], os.environ[\"EGYPT_METABASE_PASSWORD\"], 35299)\n",
    "previous_calls = clean_column_id(previous_calls, 'main_system_id')\n",
    "\n",
    "df = df_raw.merge(previous_calls, on='main_system_id', how='left')\n",
    "exclude_same_assigns = clean_column_id(previous_calls,'main_system_id')\n",
    "task_based = pd.DataFrame(df_raw.loc[~df_raw['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "\n",
    "old_assgns = pd.DataFrame(df_raw.loc[df_raw['main_system_id'].isin(exclude_same_assigns['main_system_id'].astype(int).values)])\n",
    "old_assgns = clean_column_id(old_assgns, 'main_system_id')\n",
    "df_1 = old_assgns.merge(previous_calls, on='main_system_id', how='left')\n",
    "final_old_assign = df_1[[\"main_system_id\", \"retailer_mobile_number\",\"retailer_name\",\"description\",\"reward\",\"balance\",\"offer\",\"agent_assigned\",\"project_name\"]]\n",
    "\n",
    "# Define the range of priorities\n",
    "priority_range = range(1, 16)  # 1 to 15 inclusive\n",
    "\n",
    "# Initialize a list to store the results\n",
    "main_data_list = []\n",
    "\n",
    "mapping_sheet = client.open('EG Telesales Cycle Assignment')\n",
    "mapping_worksheet = mapping_sheet.worksheet('Sheet1')  \n",
    "mapping_data = mapping_worksheet.get_all_values()\n",
    "mapping_df = pd.DataFrame(mapping_data[1:], columns=mapping_data[0])  # Convert to DataFrame first\n",
    "mapping_df = clean_column_id(mapping_df, 'main_system_id')\n",
    "mapping_df = clean_column_id(mapping_df, 'agent_assigned')\n",
    "\n",
    "mapping_df = mapping_df[mapping_df['agent_assigned'].isin(task_based_list)]\n",
    "mapped_data = assign_data_by_mapping(queries_for_mapped, mapping_df) \n",
    "main_data_list.append(mapped_data)\n",
    "\n",
    "# Iterate over each priority level\n",
    "for priority in priority_range:\n",
    "    # Filter the DataFrame for the current priority\n",
    "    priority_df = task_based[task_based[\"priority\"] == priority].reset_index(drop=True)\n",
    "\n",
    "    # Assign data to projects\n",
    "    main_data = assign_data_equal_projects(priority_df, task_based_list, 2000)\n",
    "\n",
    "    # Append the result to the list\n",
    "    main_data_list.append(main_data)\n",
    "\n",
    "# Concatenate all the main_data DataFrames into one\n",
    "main_data_total = pd.concat(main_data_list, ignore_index=True)\n",
    "\n",
    "# Filter the data to include the required columns\n",
    "main_data = main_data_total[\n",
    "    [\"main_system_id\", \"retailer_mobile_number\", \"retailer_name\", \"description\", \"reward\", \"balance\", \"offer\", \"agent_assigned\", \"project_name\"] ]\n",
    "\n",
    "# Separate rows where main_system_id == 1\n",
    "main_system_id_1 = main_data[main_data['main_system_id'] == 1]\n",
    "\n",
    "# Remove duplicates for rows where main_system_id is NOT 1\n",
    "other_main_system_ids = main_data[main_data['main_system_id'] != 1]\n",
    "other_main_system_ids = other_main_system_ids.drop_duplicates(subset=['main_system_id'])\n",
    "\n",
    "# Combine both parts back together\n",
    "main_data = pd.concat([main_system_id_1, other_main_system_ids], ignore_index=True)\n",
    "\n",
    "\n",
    "sheet = client.open('[HOURLY] TASK-BASED Data')\n",
    "sheet_instance = sheet.worksheet('Data')\n",
    "assignments = sheet_instance.get('G5:G')\n",
    "assignments_df = pd.DataFrame.from_dict(assignments)\n",
    "# print(assignments_df)\n",
    "df = main_data.copy()\n",
    "\n",
    "# Remove previous assignments and filter \n",
    "main_data_to_assign = remove_assign(assignments_df, df, 40)\n",
    "final_old_assign_new = remove_assign(assignments_df, final_old_assign, 5)\n",
    "\n",
    "# Filter the dataframes based on 'agent_assigned' being in task_based_list\n",
    "filtered_df = main_data_to_assign[main_data_to_assign['agent_assigned'].isin(task_based_list)]\n",
    "filtered_df_old = final_old_assign_new[final_old_assign_new['agent_assigned'].isin(task_based_list)]\n",
    "\n",
    "# Importing data into agents' sheet\n",
    "final_data_to_assign = filtered_df.drop_duplicates(subset='main_system_id', keep='first').copy()\n",
    "final_old_assign_new = filtered_df_old.drop_duplicates(subset='main_system_id', keep='first').copy()\n",
    "\n",
    "# Add 'added_at' timestamp\n",
    "final_data_to_assign['added_at'] = now\n",
    "final_old_assign_new['added_at'] = now\n",
    "\n",
    "# Remove 'index' column if it exists\n",
    "final_data_to_assign = final_data_to_assign.drop(columns='index', errors='ignore')\n",
    "\n",
    "final_data_to_assign = ensure_correct_dispatching(final_data_to_assign, task_based_list, final_old_assign_new)\n",
    "\n",
    "final_data_to_assign = final_data_to_assign.astype(str)\n",
    "sheet_df = final_data_to_assign[[\"main_system_id\", \"retailer_mobile_number\",\"retailer_name\",\"description\",\"reward\",\"balance\",\"offer\",\"agent_assigned\",\"added_at\"]]\n",
    "print(sheet_df)\n",
    "\n",
    "google_sh = client.open('one_more_test')\n",
    "sheet = google_sh.worksheet('Sheet1')\n",
    "sheet.append_rows([sheet_df.columns.values.tolist()] + sheet_df.values.tolist(), value_input_option=\"USER_ENTERED\")\n",
    "\n",
    "distribution_df = check_distribution_df(final_data_to_assign, task_based_list)\n",
    "distribution_df = distribution_df.astype(str)\n",
    "\n",
    "google_sh = client.open('testing_second')\n",
    "sheet = google_sh.worksheet('Sheet1')\n",
    "data_to_import = distribution_df.values.tolist()\n",
    "sheet.append_rows(data_to_import, value_input_option=\"USER_ENTERED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
